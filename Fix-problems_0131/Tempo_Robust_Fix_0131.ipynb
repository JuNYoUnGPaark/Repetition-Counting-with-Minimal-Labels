{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 1) Strict Seeding\n",
        "# ---------------------------------------------------------------------\n",
        "def set_strict_seed(seed: int):\n",
        "    random.seed(seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 2) Data Loading (RAW)\n",
        "# ---------------------------------------------------------------------\n",
        "def load_mhealth_dataset_raw(data_dir, target_activities_map, column_names):\n",
        "    \"\"\"\n",
        "    Returns:\n",
        "      full_dataset[subj_key][act_name] = raw_df (all columns except activity_id)\n",
        "    \"\"\"\n",
        "    full_dataset = {}\n",
        "    file_list = sorted(glob.glob(os.path.join(data_dir, \"mHealth_subject*.log\")))\n",
        "    if not file_list:\n",
        "        print(f\"[Warning] No mHealth logs found in {data_dir}\")\n",
        "        return {}\n",
        "\n",
        "    for file_path in file_list:\n",
        "        file_name = os.path.basename(file_path)\n",
        "        subj_part = file_name.split('.')[0]\n",
        "        try:\n",
        "            subj_id_num = int(''.join(filter(str.isdigit, subj_part)))\n",
        "            subj_key = f\"subject{subj_id_num}\"\n",
        "        except:\n",
        "            subj_key = subj_part\n",
        "\n",
        "        try:\n",
        "            df = pd.read_csv(file_path, sep=\"\\t\", header=None)\n",
        "            df = df.iloc[:, :len(column_names)]\n",
        "            df.columns = column_names\n",
        "\n",
        "            subj_data = {}\n",
        "            for label_code, activity_name in target_activities_map.items():\n",
        "                activity_df = df[df['activity_id'] == label_code].copy()\n",
        "                if not activity_df.empty:\n",
        "                    subj_data[activity_name] = activity_df.drop(columns=['activity_id'])\n",
        "            full_dataset[subj_key] = subj_data\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {file_name}: {e}\")\n",
        "            pass\n",
        "\n",
        "    return full_dataset\n",
        "\n",
        "def prepare_trial_list_raw(label_config, full_data, target_map, feature_map):\n",
        "    \"\"\"\n",
        "    label_config: list of (subject, act_id, gt_count)\n",
        "    Returns trials with RAW signal:\n",
        "      trial[\"raw\"] = (T,C) float32\n",
        "    \"\"\"\n",
        "    trial_list = []\n",
        "    for subj, act_id, gt_count in label_config:\n",
        "        act_name = target_map.get(act_id)\n",
        "        feats = feature_map.get(act_id)\n",
        "\n",
        "        if subj in full_data and act_name in full_data[subj]:\n",
        "            raw_df = full_data[subj][act_name][feats]\n",
        "            raw_np = raw_df.values.astype(np.float32)\n",
        "\n",
        "            trial_list.append({\n",
        "                \"raw\": raw_np,               # (T,C)\n",
        "                \"count\": float(gt_count),    # GT total count\n",
        "                \"subj\": subj,\n",
        "                \"act_id\": int(act_id),\n",
        "                \"meta\": f\"{subj}_{act_name}\",\n",
        "            })\n",
        "        else:\n",
        "            # silently skip\n",
        "            pass\n",
        "    return trial_list\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 3) Integrator Baseline (alpha-only, beta=0)\n",
        "# ---------------------------------------------------------------------\n",
        "def compute_energy_integral_raw(x_raw: np.ndarray, fs: int, eps=1e-12) -> float:\n",
        "    \"\"\"\n",
        "    x_raw: (T,C) raw float32\n",
        "    - DC removal per-channel\n",
        "    - energy integral: (1/fs) * sum_t sum_c (x_dc^2)\n",
        "    \"\"\"\n",
        "    x = np.asarray(x_raw, dtype=np.float32)\n",
        "    x = x - x.mean(axis=0, keepdims=True)\n",
        "    E = float(np.sum(x * x) / max(float(fs), eps))\n",
        "    return E\n",
        "\n",
        "def fit_linear_energy_calibrator_alpha_only(train_trials, fs: int, ridge: float = 1e-8):\n",
        "    \"\"\"\n",
        "    Fit alpha with beta=0: y ≈ alpha*E\n",
        "      alpha = (E^T y) / (E^T E + ridge)\n",
        "    \"\"\"\n",
        "    if len(train_trials) == 0:\n",
        "        return 0.0\n",
        "\n",
        "    Es, ys = [], []\n",
        "    for tr in train_trials:\n",
        "        E = compute_energy_integral_raw(tr[\"raw\"], fs=fs)\n",
        "        Es.append(E)\n",
        "        ys.append(float(tr[\"count\"]))\n",
        "\n",
        "    Es = np.asarray(Es, dtype=np.float64)\n",
        "    ys = np.asarray(ys, dtype=np.float64)\n",
        "\n",
        "    num = float(np.sum(Es * ys))\n",
        "    den = float(np.sum(Es * Es) + ridge)\n",
        "    return float(num / den)\n",
        "\n",
        "def baselineE2_predict_count(alpha_only: float, x_raw: np.ndarray, fs: int) -> float:\n",
        "    E = compute_energy_integral_raw(x_raw, fs=fs)\n",
        "    return float(alpha_only * E)\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 4) Metrics\n",
        "# ---------------------------------------------------------------------\n",
        "def summarize_errors(preds, gts):\n",
        "    preds = np.asarray(preds, dtype=np.float64)\n",
        "    gts   = np.asarray(gts, dtype=np.float64)\n",
        "\n",
        "    diffs = preds - gts\n",
        "    maes  = np.abs(diffs)\n",
        "    mapes = maes / (np.abs(gts) + 1e-6) * 100.0\n",
        "\n",
        "    out = {\n",
        "        \"MAE_mean\": float(maes.mean()),\n",
        "        \"MAE_std\":  float(maes.std(ddof=1)) if len(maes) > 1 else 0.0,\n",
        "        \"MAPE_mean\": float(mapes.mean()),\n",
        "        \"MAPE_std\":  float(mapes.std(ddof=1)) if len(mapes) > 1 else 0.0,\n",
        "        \"Bias_mean\": float(diffs.mean()),\n",
        "        \"Bias_std\":  float(diffs.std(ddof=1)) if len(diffs) > 1 else 0.0,\n",
        "        \"n\": int(len(preds)),\n",
        "    }\n",
        "    return out\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 5) Main: Table VI 3 scenarios only\n",
        "# ---------------------------------------------------------------------\n",
        "def main():\n",
        "    CONFIG = {\n",
        "        \"seed\": 42,\n",
        "        \"data_dir\": \"/content/drive/MyDrive/Colab Notebooks/HAR_data/MHEALTHDATASET\",\n",
        "        \"fs\": 50,\n",
        "\n",
        "        \"COLUMN_NAMES\": [\n",
        "            'acc_chest_x', 'acc_chest_y', 'acc_chest_z',\n",
        "            'ecg_1', 'ecg_2',\n",
        "            'acc_ankle_x', 'acc_ankle_y', 'acc_ankle_z',\n",
        "            'gyro_ankle_x', 'gyro_ankle_y', 'gyro_ankle_z',\n",
        "            'mag_ankle_x', 'mag_ankle_y', 'mag_ankle_z',\n",
        "            'acc_arm_x', 'acc_arm_y', 'acc_arm_z',\n",
        "            'gyro_arm_x', 'gyro_arm_y', 'gyro_arm_z',\n",
        "            'mag_arm_x', 'mag_arm_y', 'mag_arm_z',\n",
        "            'activity_id'\n",
        "        ],\n",
        "\n",
        "        \"TARGET_ACTIVITIES_MAP\": {\n",
        "            6: 'Waist bends forward',\n",
        "            7: 'Frontal elevation of arms',\n",
        "            8: 'Knees bending',\n",
        "            10: 'Jogging',\n",
        "            12: 'Jump front & back'\n",
        "        },\n",
        "\n",
        "        # 동일 입력 차원 유지용 (원 코드 그대로)\n",
        "        \"ACT_FEATURE_MAP\": {\n",
        "            6: ['acc_chest_x', 'acc_chest_y', 'acc_chest_z',\n",
        "                'acc_ankle_x', 'acc_ankle_y', 'acc_ankle_z',\n",
        "                'gyro_ankle_x', 'gyro_ankle_y', 'gyro_ankle_z',\n",
        "                'acc_arm_x', 'acc_arm_y', 'acc_arm_z',\n",
        "                'gyro_arm_x', 'gyro_arm_y', 'gyro_arm_z'],\n",
        "            7: ['acc_chest_x', 'acc_chest_y', 'acc_chest_z',\n",
        "                'acc_ankle_x', 'acc_ankle_y', 'acc_ankle_z',\n",
        "                'gyro_ankle_x', 'gyro_ankle_y', 'gyro_ankle_z',\n",
        "                'acc_arm_x', 'acc_arm_y', 'acc_arm_z',\n",
        "                'gyro_arm_x', 'gyro_arm_y', 'gyro_arm_z'],\n",
        "            8: ['acc_chest_x', 'acc_chest_y', 'acc_chest_z',\n",
        "                'acc_ankle_x', 'acc_ankle_y', 'acc_ankle_z',\n",
        "                'gyro_ankle_x', 'gyro_ankle_y', 'gyro_ankle_z',\n",
        "                'acc_arm_x', 'acc_arm_y', 'acc_arm_z',\n",
        "                'gyro_arm_x', 'gyro_arm_y', 'gyro_arm_z'],\n",
        "            10: ['acc_chest_x', 'acc_chest_y', 'acc_chest_z',\n",
        "                 'acc_ankle_x', 'acc_ankle_y', 'acc_ankle_z',\n",
        "                 'gyro_ankle_x', 'gyro_ankle_y', 'gyro_ankle_z',\n",
        "                 'acc_arm_x', 'acc_arm_y', 'acc_arm_z',\n",
        "                 'gyro_arm_x', 'gyro_arm_y', 'gyro_arm_z'],\n",
        "            12: ['acc_chest_x', 'acc_chest_y', 'acc_chest_z',\n",
        "                 'acc_ankle_x', 'acc_ankle_y', 'acc_ankle_z',\n",
        "                 'gyro_ankle_x', 'gyro_ankle_y', 'gyro_ankle_z',\n",
        "                 'acc_arm_x', 'acc_arm_y', 'acc_arm_z',\n",
        "                 'gyro_arm_x', 'gyro_arm_y', 'gyro_arm_z'],\n",
        "        },\n",
        "\n",
        "        # COUNT_TABLE[act_id][subject] = GT count\n",
        "        \"COUNT_TABLE\": {\n",
        "            6: {\n",
        "                \"subject1\": 21, \"subject2\": 19, \"subject3\": 21, \"subject4\": 20, \"subject5\": 20,\n",
        "                \"subject6\": 20, \"subject7\": 20, \"subject8\": 21, \"subject9\": 21, \"subject10\": 20,\n",
        "            },\n",
        "            7: {\n",
        "                \"subject1\": 20, \"subject2\": 20, \"subject3\": 20, \"subject4\": 20, \"subject5\": 20,\n",
        "                \"subject6\": 20, \"subject7\": 20, \"subject8\": 19, \"subject9\": 19, \"subject10\": 20,\n",
        "            },\n",
        "            8: {\n",
        "                \"subject1\": 20, \"subject2\": 21, \"subject3\": 21, \"subject4\": 19, \"subject5\": 20,\n",
        "                \"subject6\": 20, \"subject7\": 21, \"subject8\": 21, \"subject9\": 21, \"subject10\": 21,\n",
        "            },\n",
        "            10: {\n",
        "                \"subject1\": 157, \"subject2\": 161, \"subject3\": 154, \"subject4\": 154, \"subject5\": 160,\n",
        "                \"subject6\": 156, \"subject7\": 153, \"subject8\": 160, \"subject9\": 166, \"subject10\": 156,\n",
        "            },\n",
        "            12: {\n",
        "                \"subject1\": 20, \"subject2\": 22, \"subject3\": 21, \"subject4\": 21, \"subject5\": 20,\n",
        "                \"subject6\": 21, \"subject7\": 19, \"subject8\": 20, \"subject9\": 20, \"subject10\": 20,\n",
        "            },\n",
        "        },\n",
        "\n",
        "        # ✅ Table VI scenarios only (Train -> Test)\n",
        "        \"TABLE_VI_SCENARIOS\": [\n",
        "            (\"Success case\",        7,  8),   # Frontal elevation -> Knees bending\n",
        "            (\"Partial failure\",     6, 12),   # Waist bends -> Jump front & back\n",
        "            (\"Complete breakdown\",  8, 10),   # Knees bending -> Jogging\n",
        "        ],\n",
        "    }\n",
        "\n",
        "    set_strict_seed(CONFIG[\"seed\"])\n",
        "\n",
        "    # load data\n",
        "    full_data = load_mhealth_dataset_raw(\n",
        "        CONFIG[\"data_dir\"],\n",
        "        CONFIG[\"TARGET_ACTIVITIES_MAP\"],\n",
        "        CONFIG[\"COLUMN_NAMES\"]\n",
        "    )\n",
        "    if not full_data:\n",
        "        print(\"[ERROR] dataset load failed\")\n",
        "        return\n",
        "\n",
        "    subjects = [f\"subject{i}\" for i in range(1, 11)]\n",
        "    fs = CONFIG[\"fs\"]\n",
        "\n",
        "    # helper: build labels for an act_id\n",
        "    def build_labels(act_id):\n",
        "        labels = []\n",
        "        for s in subjects:\n",
        "            if act_id not in CONFIG[\"COUNT_TABLE\"]:\n",
        "                continue\n",
        "            if s not in CONFIG[\"COUNT_TABLE\"][act_id]:\n",
        "                continue\n",
        "            labels.append((s, act_id, CONFIG[\"COUNT_TABLE\"][act_id][s]))\n",
        "        return labels\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 110)\n",
        "    print(\"Integrator baseline (Energy integral + alpha-only calibration), evaluated on Table VI scenarios\")\n",
        "    print(\"=\" * 110)\n",
        "\n",
        "    # run 3 scenarios\n",
        "    for case_type, train_act, test_act in CONFIG[\"TABLE_VI_SCENARIOS\"]:\n",
        "        train_name = CONFIG[\"TARGET_ACTIVITIES_MAP\"][train_act]\n",
        "        test_name  = CONFIG[\"TARGET_ACTIVITIES_MAP\"][test_act]\n",
        "\n",
        "        # prepare trials\n",
        "        train_trials = prepare_trial_list_raw(\n",
        "            build_labels(train_act),\n",
        "            full_data,\n",
        "            CONFIG[\"TARGET_ACTIVITIES_MAP\"],\n",
        "            CONFIG[\"ACT_FEATURE_MAP\"]\n",
        "        )\n",
        "        test_trials = prepare_trial_list_raw(\n",
        "            build_labels(test_act),\n",
        "            full_data,\n",
        "            CONFIG[\"TARGET_ACTIVITIES_MAP\"],\n",
        "            CONFIG[\"ACT_FEATURE_MAP\"]\n",
        "        )\n",
        "\n",
        "        if len(train_trials) == 0 or len(test_trials) == 0:\n",
        "            print(f\"[Skip] {train_name} -> {test_name} (missing trials)\")\n",
        "            continue\n",
        "\n",
        "        # fit alpha on TRAIN (beta=0)\n",
        "        alpha = fit_linear_energy_calibrator_alpha_only(train_trials, fs=fs, ridge=1e-8)\n",
        "\n",
        "        # predict on TEST (all subjects)\n",
        "        preds, gts = [], []\n",
        "        for tr in test_trials:\n",
        "            pred = baselineE2_predict_count(alpha, tr[\"raw\"], fs=fs)\n",
        "            gt = float(tr[\"count\"])\n",
        "            preds.append(pred)\n",
        "            gts.append(gt)\n",
        "\n",
        "        summary = summarize_errors(preds, gts)\n",
        "\n",
        "        print(f\"\\n[{case_type}] {train_name} -> {test_name}\")\n",
        "        print(f\"  alpha={alpha:.6e} | n={summary['n']}\")\n",
        "        print(f\"  MAE (mean±std):  {summary['MAE_mean']:.3f} ± {summary['MAE_std']:.3f}\")\n",
        "        print(f\"  MAPE% (mean±std): {summary['MAPE_mean']:.2f} ± {summary['MAPE_std']:.2f}\")\n",
        "        print(f\"  Bias (mean±std): {summary['Bias_mean']:.3f} ± {summary['Bias_std']:.3f}\")\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 110)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AUwyz9KJDoxC",
        "outputId": "f2e52554-81c3-4e51-b3ae-f44a54e787e0"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==============================================================================================================\n",
            "Integrator baseline (Energy integral + alpha-only calibration), evaluated on Table VI scenarios\n",
            "==============================================================================================================\n",
            "\n",
            "[Success case] Frontal elevation of arms -> Knees bending\n",
            "  alpha=4.348327e-03 | n=10\n",
            "  MAE (mean±std):  9.991 ± 2.786\n",
            "  MAPE% (mean±std): 48.56 ± 12.64\n",
            "  Bias (mean±std): -9.991 ± 2.786\n",
            "\n",
            "[Partial failure] Waist bends forward -> Jump front & back\n",
            "  alpha=6.295517e-03 | n=10\n",
            "  MAE (mean±std):  54.727 ± 10.184\n",
            "  MAPE% (mean±std): 268.63 ± 49.40\n",
            "  Bias (mean±std): 54.727 ± 10.184\n",
            "\n",
            "[Complete breakdown] Knees bending -> Jogging\n",
            "  alpha=8.060321e-03 | n=10\n",
            "  MAE (mean±std):  64.715 ± 25.632\n",
            "  MAPE% (mean±std): 41.18 ± 16.62\n",
            "  Bias (mean±std): 64.715 ± 25.632\n",
            "\n",
            "==============================================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 1) Strict Seeding\n",
        "# ---------------------------------------------------------------------\n",
        "def set_strict_seed(seed: int):\n",
        "    random.seed(seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 2) Data Loading (RAW)\n",
        "# ---------------------------------------------------------------------\n",
        "def load_mhealth_dataset_raw(data_dir, target_activities_map, column_names):\n",
        "    \"\"\"\n",
        "    Returns:\n",
        "      full_dataset[subj_key][act_name] = raw_df (all columns except activity_id)\n",
        "    \"\"\"\n",
        "    full_dataset = {}\n",
        "    file_list = sorted(glob.glob(os.path.join(data_dir, \"mHealth_subject*.log\")))\n",
        "    if not file_list:\n",
        "        print(f\"[Warning] No mHealth logs found in {data_dir}\")\n",
        "        return {}\n",
        "\n",
        "    for file_path in file_list:\n",
        "        file_name = os.path.basename(file_path)\n",
        "        subj_part = file_name.split('.')[0]\n",
        "        try:\n",
        "            subj_id_num = int(''.join(filter(str.isdigit, subj_part)))\n",
        "            subj_key = f\"subject{subj_id_num}\"\n",
        "        except:\n",
        "            subj_key = subj_part\n",
        "\n",
        "        try:\n",
        "            df = pd.read_csv(file_path, sep=\"\\t\", header=None)\n",
        "            df = df.iloc[:, :len(column_names)]\n",
        "            df.columns = column_names\n",
        "\n",
        "            subj_data = {}\n",
        "            for label_code, activity_name in target_activities_map.items():\n",
        "                activity_df = df[df['activity_id'] == label_code].copy()\n",
        "                if not activity_df.empty:\n",
        "                    subj_data[activity_name] = activity_df.drop(columns=['activity_id'])\n",
        "            full_dataset[subj_key] = subj_data\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {file_name}: {e}\")\n",
        "            pass\n",
        "\n",
        "    return full_dataset\n",
        "\n",
        "\n",
        "def prepare_trial_list_raw(label_config, full_data, target_map, feature_map):\n",
        "    \"\"\"\n",
        "    label_config: list of (subject, act_id, gt_count)\n",
        "    Returns trials with RAW signal:\n",
        "      trial[\"raw\"] = (T,C) float32\n",
        "    \"\"\"\n",
        "    trial_list = []\n",
        "    for subj, act_id, gt_count in label_config:\n",
        "        act_name = target_map.get(act_id)\n",
        "        feats = feature_map.get(act_id)\n",
        "\n",
        "        if subj in full_data and act_name in full_data[subj]:\n",
        "            raw_df = full_data[subj][act_name][feats]\n",
        "            raw_np = raw_df.values.astype(np.float32)\n",
        "\n",
        "            trial_list.append({\n",
        "                \"raw\": raw_np,               # (T,C)\n",
        "                \"count\": float(gt_count),    # GT total count\n",
        "                \"subj\": subj,\n",
        "                \"act_id\": int(act_id),\n",
        "                \"meta\": f\"{subj}_{act_name}\",\n",
        "            })\n",
        "        else:\n",
        "            pass\n",
        "\n",
        "    return trial_list\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 3) Integrator Baseline (alpha+beta)\n",
        "# ---------------------------------------------------------------------\n",
        "def compute_energy_integral_raw(x_raw: np.ndarray, fs: int, eps=1e-12) -> float:\n",
        "    \"\"\"\n",
        "    x_raw: (T,C) raw float32\n",
        "    - DC removal per-channel\n",
        "    - energy integral: (1/fs) * sum_t sum_c (x_dc^2)\n",
        "    \"\"\"\n",
        "    x = np.asarray(x_raw, dtype=np.float32)\n",
        "    x = x - x.mean(axis=0, keepdims=True)\n",
        "    E = float(np.sum(x * x) / max(float(fs), eps))\n",
        "    return E\n",
        "\n",
        "\n",
        "def fit_linear_energy_calibrator_alpha_beta(train_trials, fs: int, ridge: float = 1e-8):\n",
        "    \"\"\"\n",
        "    Fit (alpha, beta) in: y ≈ alpha*E + beta\n",
        "    Closed-form ridge on normal eq:\n",
        "      theta = (X^T X + ridge*I)^-1 X^T y\n",
        "      where X = [E, 1]\n",
        "    Returns: (alpha, beta)\n",
        "    \"\"\"\n",
        "    if len(train_trials) == 0:\n",
        "        return 0.0, 0.0\n",
        "\n",
        "    Es, ys = [], []\n",
        "    for tr in train_trials:\n",
        "        E = compute_energy_integral_raw(tr[\"raw\"], fs=fs)\n",
        "        Es.append(E)\n",
        "        ys.append(float(tr[\"count\"]))\n",
        "\n",
        "    Es = np.asarray(Es, dtype=np.float64)  # (N,)\n",
        "    ys = np.asarray(ys, dtype=np.float64)  # (N,)\n",
        "\n",
        "    # X: (N,2) = [E, 1]\n",
        "    X = np.stack([Es, np.ones_like(Es)], axis=1)\n",
        "\n",
        "    # ridge\n",
        "    XtX = X.T @ X\n",
        "    XtX = XtX + ridge * np.eye(2, dtype=np.float64)\n",
        "    Xty = X.T @ ys\n",
        "\n",
        "    theta = np.linalg.solve(XtX, Xty)  # (2,)\n",
        "    alpha = float(theta[0])\n",
        "    beta  = float(theta[1])\n",
        "    return alpha, beta\n",
        "\n",
        "\n",
        "def baseline_predict_count(alpha: float, beta: float, x_raw: np.ndarray, fs: int) -> float:\n",
        "    E = compute_energy_integral_raw(x_raw, fs=fs)\n",
        "    return float(alpha * E + beta)\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 4) Metrics\n",
        "# ---------------------------------------------------------------------\n",
        "def summarize_errors(preds, gts):\n",
        "    preds = np.asarray(preds, dtype=np.float64)\n",
        "    gts   = np.asarray(gts, dtype=np.float64)\n",
        "\n",
        "    diffs = preds - gts\n",
        "    maes  = np.abs(diffs)\n",
        "    mapes = maes / (np.abs(gts) + 1e-6) * 100.0\n",
        "\n",
        "    out = {\n",
        "        \"MAE_mean\": float(maes.mean()),\n",
        "        \"MAE_std\":  float(maes.std(ddof=1)) if len(maes) > 1 else 0.0,\n",
        "        \"MAPE_mean\": float(mapes.mean()),\n",
        "        \"MAPE_std\":  float(mapes.std(ddof=1)) if len(mapes) > 1 else 0.0,\n",
        "        \"Bias_mean\": float(diffs.mean()),\n",
        "        \"Bias_std\":  float(diffs.std(ddof=1)) if len(diffs) > 1 else 0.0,\n",
        "        \"n\": int(len(preds)),\n",
        "    }\n",
        "    return out\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 5) Main: Table VI 3 scenarios only\n",
        "# ---------------------------------------------------------------------\n",
        "def main():\n",
        "    CONFIG = {\n",
        "        \"seed\": 42,\n",
        "        \"data_dir\": \"/content/drive/MyDrive/Colab Notebooks/HAR_data/MHEALTHDATASET\",\n",
        "        \"fs\": 50,\n",
        "\n",
        "        \"COLUMN_NAMES\": [\n",
        "            'acc_chest_x', 'acc_chest_y', 'acc_chest_z',\n",
        "            'ecg_1', 'ecg_2',\n",
        "            'acc_ankle_x', 'acc_ankle_y', 'acc_ankle_z',\n",
        "            'gyro_ankle_x', 'gyro_ankle_y', 'gyro_ankle_z',\n",
        "            'mag_ankle_x', 'mag_ankle_y', 'mag_ankle_z',\n",
        "            'acc_arm_x', 'acc_arm_y', 'acc_arm_z',\n",
        "            'gyro_arm_x', 'gyro_arm_y', 'gyro_arm_z',\n",
        "            'mag_arm_x', 'mag_arm_y', 'mag_arm_z',\n",
        "            'activity_id'\n",
        "        ],\n",
        "\n",
        "        \"TARGET_ACTIVITIES_MAP\": {\n",
        "            6: 'Waist bends forward',\n",
        "            7: 'Frontal elevation of arms',\n",
        "            8: 'Knees bending',\n",
        "            10: 'Jogging',\n",
        "            12: 'Jump front & back'\n",
        "        },\n",
        "\n",
        "        # 동일 입력 차원 유지용\n",
        "        \"ACT_FEATURE_MAP\": {\n",
        "            6: ['acc_chest_x', 'acc_chest_y', 'acc_chest_z',\n",
        "                'acc_ankle_x', 'acc_ankle_y', 'acc_ankle_z',\n",
        "                'gyro_ankle_x', 'gyro_ankle_y', 'gyro_ankle_z',\n",
        "                'acc_arm_x', 'acc_arm_y', 'acc_arm_z',\n",
        "                'gyro_arm_x', 'gyro_arm_y', 'gyro_arm_z'],\n",
        "            7: ['acc_chest_x', 'acc_chest_y', 'acc_chest_z',\n",
        "                'acc_ankle_x', 'acc_ankle_y', 'acc_ankle_z',\n",
        "                'gyro_ankle_x', 'gyro_ankle_y', 'gyro_ankle_z',\n",
        "                'acc_arm_x', 'acc_arm_y', 'acc_arm_z',\n",
        "                'gyro_arm_x', 'gyro_arm_y', 'gyro_arm_z'],\n",
        "            8: ['acc_chest_x', 'acc_chest_y', 'acc_chest_z',\n",
        "                'acc_ankle_x', 'acc_ankle_y', 'acc_ankle_z',\n",
        "                'gyro_ankle_x', 'gyro_ankle_y', 'gyro_ankle_z',\n",
        "                'acc_arm_x', 'acc_arm_y', 'acc_arm_z',\n",
        "                'gyro_arm_x', 'gyro_arm_y', 'gyro_arm_z'],\n",
        "            10: ['acc_chest_x', 'acc_chest_y', 'acc_chest_z',\n",
        "                 'acc_ankle_x', 'acc_ankle_y', 'acc_ankle_z',\n",
        "                 'gyro_ankle_x', 'gyro_ankle_y', 'gyro_ankle_z',\n",
        "                 'acc_arm_x', 'acc_arm_y', 'acc_arm_z',\n",
        "                 'gyro_arm_x', 'gyro_arm_y', 'gyro_arm_z'],\n",
        "            12: ['acc_chest_x', 'acc_chest_y', 'acc_chest_z',\n",
        "                 'acc_ankle_x', 'acc_ankle_y', 'acc_ankle_z',\n",
        "                 'gyro_ankle_x', 'gyro_ankle_y', 'gyro_ankle_z',\n",
        "                 'acc_arm_x', 'acc_arm_y', 'acc_arm_z',\n",
        "                 'gyro_arm_x', 'gyro_arm_y', 'gyro_arm_z'],\n",
        "        },\n",
        "\n",
        "        \"COUNT_TABLE\": {\n",
        "            6: {\n",
        "                \"subject1\": 21, \"subject2\": 19, \"subject3\": 21, \"subject4\": 20, \"subject5\": 20,\n",
        "                \"subject6\": 20, \"subject7\": 20, \"subject8\": 21, \"subject9\": 21, \"subject10\": 20,\n",
        "            },\n",
        "            7: {\n",
        "                \"subject1\": 20, \"subject2\": 20, \"subject3\": 20, \"subject4\": 20, \"subject5\": 20,\n",
        "                \"subject6\": 20, \"subject7\": 20, \"subject8\": 19, \"subject9\": 19, \"subject10\": 20,\n",
        "            },\n",
        "            8: {\n",
        "                \"subject1\": 20, \"subject2\": 21, \"subject3\": 21, \"subject4\": 19, \"subject5\": 20,\n",
        "                \"subject6\": 20, \"subject7\": 21, \"subject8\": 21, \"subject9\": 21, \"subject10\": 21,\n",
        "            },\n",
        "            10: {\n",
        "                \"subject1\": 157, \"subject2\": 161, \"subject3\": 154, \"subject4\": 154, \"subject5\": 160,\n",
        "                \"subject6\": 156, \"subject7\": 153, \"subject8\": 160, \"subject9\": 166, \"subject10\": 156,\n",
        "            },\n",
        "            12: {\n",
        "                \"subject1\": 20, \"subject2\": 22, \"subject3\": 21, \"subject4\": 21, \"subject5\": 20,\n",
        "                \"subject6\": 21, \"subject7\": 19, \"subject8\": 20, \"subject9\": 20, \"subject10\": 20,\n",
        "            },\n",
        "        },\n",
        "\n",
        "        # Table VI scenarios only (Train -> Test)\n",
        "        \"TABLE_VI_SCENARIOS\": [\n",
        "            (\"Success case\",        7,  8),   # Frontal elevation -> Knees bending\n",
        "            (\"Partial failure\",     6, 12),   # Waist bends -> Jump front & back\n",
        "            (\"Complete breakdown\",  8, 10),   # Knees bending -> Jogging\n",
        "        ],\n",
        "    }\n",
        "\n",
        "    set_strict_seed(CONFIG[\"seed\"])\n",
        "\n",
        "    full_data = load_mhealth_dataset_raw(\n",
        "        CONFIG[\"data_dir\"],\n",
        "        CONFIG[\"TARGET_ACTIVITIES_MAP\"],\n",
        "        CONFIG[\"COLUMN_NAMES\"]\n",
        "    )\n",
        "    if not full_data:\n",
        "        print(\"[ERROR] dataset load failed\")\n",
        "        return\n",
        "\n",
        "    subjects = [f\"subject{i}\" for i in range(1, 11)]\n",
        "    fs = CONFIG[\"fs\"]\n",
        "\n",
        "    def build_labels(act_id):\n",
        "        labels = []\n",
        "        for s in subjects:\n",
        "            if act_id not in CONFIG[\"COUNT_TABLE\"]:\n",
        "                continue\n",
        "            if s not in CONFIG[\"COUNT_TABLE\"][act_id]:\n",
        "                continue\n",
        "            labels.append((s, act_id, CONFIG[\"COUNT_TABLE\"][act_id][s]))\n",
        "        return labels\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 110)\n",
        "    print(\"Integrator baseline (Energy integral + alpha+beta calibration), evaluated on Table VI scenarios\")\n",
        "    print(\"=\" * 110)\n",
        "\n",
        "    for case_type, train_act, test_act in CONFIG[\"TABLE_VI_SCENARIOS\"]:\n",
        "        train_name = CONFIG[\"TARGET_ACTIVITIES_MAP\"][train_act]\n",
        "        test_name  = CONFIG[\"TARGET_ACTIVITIES_MAP\"][test_act]\n",
        "\n",
        "        train_trials = prepare_trial_list_raw(\n",
        "            build_labels(train_act),\n",
        "            full_data,\n",
        "            CONFIG[\"TARGET_ACTIVITIES_MAP\"],\n",
        "            CONFIG[\"ACT_FEATURE_MAP\"]\n",
        "        )\n",
        "        test_trials = prepare_trial_list_raw(\n",
        "            build_labels(test_act),\n",
        "            full_data,\n",
        "            CONFIG[\"TARGET_ACTIVITIES_MAP\"],\n",
        "            CONFIG[\"ACT_FEATURE_MAP\"]\n",
        "        )\n",
        "\n",
        "        if len(train_trials) == 0 or len(test_trials) == 0:\n",
        "            print(f\"[Skip] {train_name} -> {test_name} (missing trials)\")\n",
        "            continue\n",
        "\n",
        "        alpha, beta = fit_linear_energy_calibrator_alpha_beta(train_trials, fs=fs, ridge=1e-8)\n",
        "\n",
        "        preds, gts = [], []\n",
        "        for tr in test_trials:\n",
        "            pred = baseline_predict_count(alpha, beta, tr[\"raw\"], fs=fs)\n",
        "            gt = float(tr[\"count\"])\n",
        "            preds.append(pred)\n",
        "            gts.append(gt)\n",
        "\n",
        "        summary = summarize_errors(preds, gts)\n",
        "\n",
        "        print(f\"\\n[{case_type}] {train_name} -> {test_name}\")\n",
        "        print(f\"  alpha={alpha:.6e} | beta={beta:.6e} | n={summary['n']}\")\n",
        "        print(f\"  MAE (mean±std):   {summary['MAE_mean']:.3f} ± {summary['MAE_std']:.3f}\")\n",
        "        print(f\"  MAPE% (mean±std): {summary['MAPE_mean']:.2f} ± {summary['MAPE_std']:.2f}\")\n",
        "        print(f\"  Bias (mean±std):  {summary['Bias_mean']:.3f} ± {summary['Bias_std']:.3f}\")\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 110)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V3_JXC1wHh5u",
        "outputId": "1fd018f4-5151-48e5-fdb3-8bf8594763c7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==============================================================================================================\n",
            "Integrator baseline (Energy integral + alpha+beta calibration), evaluated on Table VI scenarios\n",
            "==============================================================================================================\n",
            "\n",
            "[Success case] Frontal elevation of arms -> Knees bending\n",
            "  alpha=-5.992391e-05 | beta=2.005494e+01 | n=10\n",
            "  MAE (mean±std):   0.767 ± 0.466\n",
            "  MAPE% (mean±std): 3.71 ± 2.23\n",
            "  Bias (mean±std):  -0.590 ± 0.697\n",
            "\n",
            "[Partial failure] Waist bends forward -> Jump front & back\n",
            "  alpha=1.490611e-04 | beta=1.984672e+01 | n=10\n",
            "  MAE (mean±std):   1.376 ± 0.552\n",
            "  MAPE% (mean±std): 6.85 ± 2.99\n",
            "  Bias (mean±std):  1.226 ± 0.860\n",
            "\n",
            "[Complete breakdown] Knees bending -> Jogging\n",
            "  alpha=-4.056960e-04 | beta=2.148045e+01 | n=10\n",
            "  MAE (mean±std):   147.414 ± 4.025\n",
            "  MAPE% (mean±std): 93.48 ± 0.78\n",
            "  Bias (mean±std):  -147.414 ± 4.025\n",
            "\n",
            "==============================================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from scipy.signal import find_peaks\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 1) Strict Seeding\n",
        "# ---------------------------------------------------------------------\n",
        "def set_strict_seed(seed: int):\n",
        "    random.seed(seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 2) Data Loading (RAW)\n",
        "# ---------------------------------------------------------------------\n",
        "def load_mhealth_dataset_raw(data_dir, target_activities_map, column_names):\n",
        "    \"\"\"\n",
        "    Returns:\n",
        "      full_dataset[subj_key][act_name] = raw_df (all columns except activity_id)\n",
        "    \"\"\"\n",
        "    full_dataset = {}\n",
        "    file_list = sorted(glob.glob(os.path.join(data_dir, \"mHealth_subject*.log\")))\n",
        "    if not file_list:\n",
        "        print(f\"[Warning] No mHealth logs found in {data_dir}\")\n",
        "        return {}\n",
        "\n",
        "    for file_path in file_list:\n",
        "        file_name = os.path.basename(file_path)\n",
        "        subj_part = file_name.split('.')[0]\n",
        "        try:\n",
        "            subj_id_num = int(''.join(filter(str.isdigit, subj_part)))\n",
        "            subj_key = f\"subject{subj_id_num}\"\n",
        "        except:\n",
        "            subj_key = subj_part\n",
        "\n",
        "        try:\n",
        "            df = pd.read_csv(file_path, sep=\"\\t\", header=None)\n",
        "            df = df.iloc[:, :len(column_names)]\n",
        "            df.columns = column_names\n",
        "\n",
        "            subj_data = {}\n",
        "            for label_code, activity_name in target_activities_map.items():\n",
        "                activity_df = df[df['activity_id'] == label_code].copy()\n",
        "                if not activity_df.empty:\n",
        "                    subj_data[activity_name] = activity_df.drop(columns=['activity_id'])\n",
        "            full_dataset[subj_key] = subj_data\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {file_name}: {e}\")\n",
        "            pass\n",
        "\n",
        "    return full_dataset\n",
        "\n",
        "\n",
        "def prepare_trial_list_raw(label_config, full_data, target_map, feature_map):\n",
        "    \"\"\"\n",
        "    label_config: list of (subject, act_id, gt_count)\n",
        "    Returns trials with RAW signal:\n",
        "      trial[\"raw\"] = (T,C) float32\n",
        "    \"\"\"\n",
        "    trial_list = []\n",
        "    for subj, act_id, gt_count in label_config:\n",
        "        act_name = target_map.get(act_id)\n",
        "        feats = feature_map.get(act_id)\n",
        "\n",
        "        if subj in full_data and act_name in full_data[subj]:\n",
        "            raw_df = full_data[subj][act_name][feats]\n",
        "            raw_np = raw_df.values.astype(np.float32)\n",
        "\n",
        "            trial_list.append({\n",
        "                \"raw\": raw_np,               # (T,C)\n",
        "                \"count\": float(gt_count),    # GT total count\n",
        "                \"subj\": subj,\n",
        "                \"act_id\": int(act_id),\n",
        "                \"meta\": f\"{subj}_{act_name}\",\n",
        "            })\n",
        "        else:\n",
        "            pass\n",
        "    return trial_list\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 3) Integrator Baseline (alpha-only, beta=0) + (alpha+beta)\n",
        "# ---------------------------------------------------------------------\n",
        "def compute_energy_integral_raw(x_raw: np.ndarray, fs: int, eps=1e-12) -> float:\n",
        "    \"\"\"\n",
        "    x_raw: (T,C) raw float32\n",
        "    - DC removal per-channel\n",
        "    - energy integral: (1/fs) * sum_t sum_c (x_dc^2)\n",
        "    \"\"\"\n",
        "    x = np.asarray(x_raw, dtype=np.float32)\n",
        "    x = x - x.mean(axis=0, keepdims=True)\n",
        "    E = float(np.sum(x * x) / max(float(fs), eps))\n",
        "    return E\n",
        "\n",
        "\n",
        "def fit_linear_energy_calibrator_alpha_only(train_trials, fs: int, ridge: float = 1e-8):\n",
        "    \"\"\"\n",
        "    Fit alpha with beta=0: y ≈ alpha*E\n",
        "      alpha = (E^T y) / (E^T E + ridge)\n",
        "    \"\"\"\n",
        "    if len(train_trials) == 0:\n",
        "        return 0.0\n",
        "\n",
        "    Es, ys = [], []\n",
        "    for tr in train_trials:\n",
        "        E = compute_energy_integral_raw(tr[\"raw\"], fs=fs)\n",
        "        Es.append(E)\n",
        "        ys.append(float(tr[\"count\"]))\n",
        "\n",
        "    Es = np.asarray(Es, dtype=np.float64)\n",
        "    ys = np.asarray(ys, dtype=np.float64)\n",
        "\n",
        "    num = float(np.sum(Es * ys))\n",
        "    den = float(np.sum(Es * Es) + ridge)\n",
        "    return float(num / den)\n",
        "\n",
        "\n",
        "def baselineE2_predict_count_alpha_only(alpha_only: float, x_raw: np.ndarray, fs: int) -> float:\n",
        "    E = compute_energy_integral_raw(x_raw, fs=fs)\n",
        "    return float(alpha_only * E)\n",
        "\n",
        "\n",
        "def fit_linear_energy_calibrator_alpha_beta(train_trials, fs: int, ridge: float = 1e-8):\n",
        "    \"\"\"\n",
        "    Fit alpha,beta: y ≈ alpha*E + beta  (ridge on alpha only)\n",
        "    Closed-form via normal equations for 2 params.\n",
        "    \"\"\"\n",
        "    if len(train_trials) == 0:\n",
        "        return 0.0, 0.0\n",
        "\n",
        "    Es, ys = [], []\n",
        "    for tr in train_trials:\n",
        "        E = compute_energy_integral_raw(tr[\"raw\"], fs=fs)\n",
        "        Es.append(E)\n",
        "        ys.append(float(tr[\"count\"]))\n",
        "\n",
        "    E = np.asarray(Es, dtype=np.float64)\n",
        "    y = np.asarray(ys, dtype=np.float64)\n",
        "\n",
        "    # Solve:\n",
        "    # [sum(E^2)+ridge, sum(E)] [alpha] = [sum(Ey)]\n",
        "    # [sum(E),          n   ] [beta ]   [sum(y)]\n",
        "    sEE = float(np.sum(E * E) + ridge)\n",
        "    sE  = float(np.sum(E))\n",
        "    sEy = float(np.sum(E * y))\n",
        "    sy  = float(np.sum(y))\n",
        "    n   = float(len(y))\n",
        "\n",
        "    A = np.array([[sEE, sE],\n",
        "                  [sE,  n ]], dtype=np.float64)\n",
        "    b = np.array([sEy, sy], dtype=np.float64)\n",
        "\n",
        "    sol = np.linalg.solve(A, b)\n",
        "    alpha, beta = float(sol[0]), float(sol[1])\n",
        "    return alpha, beta\n",
        "\n",
        "\n",
        "def baselineE2_predict_count_alpha_beta(alpha: float, beta: float, x_raw: np.ndarray, fs: int) -> float:\n",
        "    E = compute_energy_integral_raw(x_raw, fs=fs)\n",
        "    return float(alpha * E + beta)\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 4) Peak Counter Baseline (train-only global threshold)\n",
        "# ---------------------------------------------------------------------\n",
        "def _scalar_magnitude(x_raw: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    x_raw: (T,C) -> scalar magnitude (T,)\n",
        "    Uses L2 norm across channels after per-channel mean removal.\n",
        "    \"\"\"\n",
        "    x = np.asarray(x_raw, dtype=np.float32)\n",
        "    x = x - x.mean(axis=0, keepdims=True)\n",
        "    mag = np.sqrt(np.sum(x * x, axis=1) + 1e-12)\n",
        "    return mag.astype(np.float32)\n",
        "\n",
        "\n",
        "def _smooth_gaussian_1d_np(x: np.ndarray, sigma: float) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Simple gaussian smoothing without scipy.ndimage dependency.\n",
        "    sigma in samples.\n",
        "    \"\"\"\n",
        "    x = np.asarray(x, dtype=np.float32)\n",
        "    if sigma <= 0:\n",
        "        return x\n",
        "\n",
        "    # kernel size ~ 6*sigma\n",
        "    radius = int(max(1, round(3.0 * sigma)))\n",
        "    kx = np.arange(-radius, radius + 1, dtype=np.float32)\n",
        "    kernel = np.exp(-(kx * kx) / (2.0 * float(sigma) * float(sigma)))\n",
        "    kernel /= np.sum(kernel) + 1e-12\n",
        "\n",
        "    # reflect pad conv\n",
        "    xp = np.pad(x, (radius, radius), mode=\"reflect\")\n",
        "    y = np.convolve(xp, kernel, mode=\"valid\")\n",
        "    return y.astype(np.float32)\n",
        "\n",
        "\n",
        "def fit_peak_threshold_global(train_trials, fs: int, smooth_sigma_sec=0.12, thr_frac=0.35):\n",
        "    \"\"\"\n",
        "    Global peak height threshold determined from TRAIN trials:\n",
        "      thr = thr_frac * median( max(smoothed_mag) over trials )\n",
        "    \"\"\"\n",
        "    if len(train_trials) == 0:\n",
        "        return 0.0\n",
        "\n",
        "    sigma = float(smooth_sigma_sec) * float(fs)\n",
        "    max_vals = []\n",
        "    for tr in train_trials:\n",
        "        mag = _scalar_magnitude(tr[\"raw\"])\n",
        "        mag_s = _smooth_gaussian_1d_np(mag, sigma=sigma)\n",
        "        max_vals.append(float(np.max(mag_s)))\n",
        "\n",
        "    med_max = float(np.median(np.asarray(max_vals, dtype=np.float64)))\n",
        "    thr = float(thr_frac * med_max)\n",
        "    return thr\n",
        "\n",
        "\n",
        "def peak_counter_predict_count(x_raw: np.ndarray, fs: int, height_thr: float,\n",
        "                               smooth_sigma_sec=0.12, min_dist_sec=0.25,\n",
        "                               prominence_frac=0.10):\n",
        "    \"\"\"\n",
        "    Count peaks on smoothed magnitude:\n",
        "      - height: global threshold\n",
        "      - distance: min_dist_sec\n",
        "      - prominence: prominence_frac * height_thr (stabilize)\n",
        "    \"\"\"\n",
        "    mag = _scalar_magnitude(x_raw)\n",
        "    sigma = float(smooth_sigma_sec) * float(fs)\n",
        "    mag_s = _smooth_gaussian_1d_np(mag, sigma=sigma)\n",
        "\n",
        "    distance = int(max(1, round(float(min_dist_sec) * float(fs))))\n",
        "    prominence = float(prominence_frac) * float(max(height_thr, 1e-6))\n",
        "\n",
        "    peaks, _ = find_peaks(mag_s, height=height_thr, distance=distance, prominence=prominence)\n",
        "    return float(len(peaks))\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 5) FFT Counter Baseline (dominant frequency)\n",
        "# ---------------------------------------------------------------------\n",
        "def fft_counter_predict_count(x_raw: np.ndarray, fs: int, fmin=0.3, fmax=5.0,\n",
        "                              smooth_sigma_sec=0.12):\n",
        "    \"\"\"\n",
        "    Estimate reps via dominant frequency:\n",
        "      mag -> smooth -> FFT -> pick peak freq in [fmin,fmax]\n",
        "      count_hat = f_peak * duration\n",
        "    \"\"\"\n",
        "    mag = _scalar_magnitude(x_raw)\n",
        "    sigma = float(smooth_sigma_sec) * float(fs)\n",
        "    mag_s = _smooth_gaussian_1d_np(mag, sigma=sigma)\n",
        "\n",
        "    T = int(mag_s.shape[0])\n",
        "    if T < 8:\n",
        "        return 0.0\n",
        "\n",
        "    dur = float(T) / float(fs)\n",
        "\n",
        "    # Hann window\n",
        "    w = np.hanning(T).astype(np.float32)\n",
        "    xw = (mag_s - float(np.mean(mag_s))) * w\n",
        "\n",
        "    # rFFT\n",
        "    X = np.fft.rfft(xw.astype(np.float64))\n",
        "    freqs = np.fft.rfftfreq(T, d=1.0 / float(fs))\n",
        "    power = (X.real * X.real + X.imag * X.imag)\n",
        "\n",
        "    # band mask\n",
        "    mask = (freqs >= float(fmin)) & (freqs <= float(fmax))\n",
        "    if not np.any(mask):\n",
        "        return 0.0\n",
        "\n",
        "    freqs_b = freqs[mask]\n",
        "    power_b = power[mask]\n",
        "    idx = int(np.argmax(power_b))\n",
        "    f_peak = float(freqs_b[idx])\n",
        "\n",
        "    count_hat = f_peak * dur\n",
        "    return float(count_hat)\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 6) Metrics\n",
        "# ---------------------------------------------------------------------\n",
        "def summarize_errors(preds, gts):\n",
        "    preds = np.asarray(preds, dtype=np.float64)\n",
        "    gts   = np.asarray(gts, dtype=np.float64)\n",
        "\n",
        "    diffs = preds - gts\n",
        "    maes  = np.abs(diffs)\n",
        "    mapes = maes / (np.abs(gts) + 1e-6) * 100.0\n",
        "\n",
        "    out = {\n",
        "        \"MAE_mean\": float(maes.mean()),\n",
        "        \"MAE_std\":  float(maes.std(ddof=1)) if len(maes) > 1 else 0.0,\n",
        "        \"MAPE_mean\": float(mapes.mean()),\n",
        "        \"MAPE_std\":  float(mapes.std(ddof=1)) if len(mapes) > 1 else 0.0,\n",
        "        \"Bias_mean\": float(diffs.mean()),\n",
        "        \"Bias_std\":  float(diffs.std(ddof=1)) if len(diffs) > 1 else 0.0,\n",
        "        \"n\": int(len(preds)),\n",
        "    }\n",
        "    return out\n",
        "\n",
        "\n",
        "def print_summary_block(method_name, summary, extra_line=None, indent=\"  \"):\n",
        "    if extra_line is not None:\n",
        "        print(f\"{indent}{extra_line}\")\n",
        "    print(f\"{indent}{method_name} | n={summary['n']}\")\n",
        "    print(f\"{indent}MAE (mean±std):   {summary['MAE_mean']:.3f} ± {summary['MAE_std']:.3f}\")\n",
        "    print(f\"{indent}MAPE% (mean±std): {summary['MAPE_mean']:.2f} ± {summary['MAPE_std']:.2f}\")\n",
        "    print(f\"{indent}Bias (mean±std):  {summary['Bias_mean']:.3f} ± {summary['Bias_std']:.3f}\")\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 7) Main: Table VI 3 scenarios only\n",
        "# ---------------------------------------------------------------------\n",
        "def main():\n",
        "    CONFIG = {\n",
        "        \"seed\": 42,\n",
        "        \"data_dir\": \"/content/drive/MyDrive/Colab Notebooks/HAR_data/MHEALTHDATASET\",\n",
        "        \"fs\": 50,\n",
        "\n",
        "        \"COLUMN_NAMES\": [\n",
        "            'acc_chest_x', 'acc_chest_y', 'acc_chest_z',\n",
        "            'ecg_1', 'ecg_2',\n",
        "            'acc_ankle_x', 'acc_ankle_y', 'acc_ankle_z',\n",
        "            'gyro_ankle_x', 'gyro_ankle_y', 'gyro_ankle_z',\n",
        "            'mag_ankle_x', 'mag_ankle_y', 'mag_ankle_z',\n",
        "            'acc_arm_x', 'acc_arm_y', 'acc_arm_z',\n",
        "            'gyro_arm_x', 'gyro_arm_y', 'gyro_arm_z',\n",
        "            'mag_arm_x', 'mag_arm_y', 'mag_arm_z',\n",
        "            'activity_id'\n",
        "        ],\n",
        "\n",
        "        \"TARGET_ACTIVITIES_MAP\": {\n",
        "            6: 'Waist bends forward',\n",
        "            7: 'Frontal elevation of arms',\n",
        "            8: 'Knees bending',\n",
        "            10: 'Jogging',\n",
        "            12: 'Jump front & back'\n",
        "        },\n",
        "\n",
        "        \"ACT_FEATURE_MAP\": {\n",
        "            6: ['acc_chest_x', 'acc_chest_y', 'acc_chest_z',\n",
        "                'acc_ankle_x', 'acc_ankle_y', 'acc_ankle_z',\n",
        "                'gyro_ankle_x', 'gyro_ankle_y', 'gyro_ankle_z',\n",
        "                'acc_arm_x', 'acc_arm_y', 'acc_arm_z',\n",
        "                'gyro_arm_x', 'gyro_arm_y', 'gyro_arm_z'],\n",
        "            7: ['acc_chest_x', 'acc_chest_y', 'acc_chest_z',\n",
        "                'acc_ankle_x', 'acc_ankle_y', 'acc_ankle_z',\n",
        "                'gyro_ankle_x', 'gyro_ankle_y', 'gyro_ankle_z',\n",
        "                'acc_arm_x', 'acc_arm_y', 'acc_arm_z',\n",
        "                'gyro_arm_x', 'gyro_arm_y', 'gyro_arm_z'],\n",
        "            8: ['acc_chest_x', 'acc_chest_y', 'acc_chest_z',\n",
        "                'acc_ankle_x', 'acc_ankle_y', 'acc_ankle_z',\n",
        "                'gyro_ankle_x', 'gyro_ankle_y', 'gyro_ankle_z',\n",
        "                'acc_arm_x', 'acc_arm_y', 'acc_arm_z',\n",
        "                'gyro_arm_x', 'gyro_arm_y', 'gyro_arm_z'],\n",
        "            10: ['acc_chest_x', 'acc_chest_y', 'acc_chest_z',\n",
        "                 'acc_ankle_x', 'acc_ankle_y', 'acc_ankle_z',\n",
        "                 'gyro_ankle_x', 'gyro_ankle_y', 'gyro_ankle_z',\n",
        "                 'acc_arm_x', 'acc_arm_y', 'acc_arm_z',\n",
        "                 'gyro_arm_x', 'gyro_arm_y', 'gyro_arm_z'],\n",
        "            12: ['acc_chest_x', 'acc_chest_y', 'acc_chest_z',\n",
        "                 'acc_ankle_x', 'acc_ankle_y', 'acc_ankle_z',\n",
        "                 'gyro_ankle_x', 'gyro_ankle_y', 'gyro_ankle_z',\n",
        "                 'acc_arm_x', 'acc_arm_y', 'acc_arm_z',\n",
        "                 'gyro_arm_x', 'gyro_arm_y', 'gyro_arm_z'],\n",
        "        },\n",
        "\n",
        "        \"COUNT_TABLE\": {\n",
        "            6: {\n",
        "                \"subject1\": 21, \"subject2\": 19, \"subject3\": 21, \"subject4\": 20, \"subject5\": 20,\n",
        "                \"subject6\": 20, \"subject7\": 20, \"subject8\": 21, \"subject9\": 21, \"subject10\": 20,\n",
        "            },\n",
        "            7: {\n",
        "                \"subject1\": 20, \"subject2\": 20, \"subject3\": 20, \"subject4\": 20, \"subject5\": 20,\n",
        "                \"subject6\": 20, \"subject7\": 20, \"subject8\": 19, \"subject9\": 19, \"subject10\": 20,\n",
        "            },\n",
        "            8: {\n",
        "                \"subject1\": 20, \"subject2\": 21, \"subject3\": 21, \"subject4\": 19, \"subject5\": 20,\n",
        "                \"subject6\": 20, \"subject7\": 21, \"subject8\": 21, \"subject9\": 21, \"subject10\": 21,\n",
        "            },\n",
        "            10: {\n",
        "                \"subject1\": 157, \"subject2\": 161, \"subject3\": 154, \"subject4\": 154, \"subject5\": 160,\n",
        "                \"subject6\": 156, \"subject7\": 153, \"subject8\": 160, \"subject9\": 166, \"subject10\": 156,\n",
        "            },\n",
        "            12: {\n",
        "                \"subject1\": 20, \"subject2\": 22, \"subject3\": 21, \"subject4\": 21, \"subject5\": 20,\n",
        "                \"subject6\": 21, \"subject7\": 19, \"subject8\": 20, \"subject9\": 20, \"subject10\": 20,\n",
        "            },\n",
        "        },\n",
        "\n",
        "        # Table VI scenarios only (Train -> Test)\n",
        "        \"TABLE_VI_SCENARIOS\": [\n",
        "            (\"Success case\",        7,  8),   # Frontal elevation -> Knees bending\n",
        "            (\"Partial failure\",     6, 12),   # Waist bends -> Jump front & back\n",
        "            (\"Complete breakdown\",  8, 10),   # Knees bending -> Jogging\n",
        "        ],\n",
        "\n",
        "        # Peak / FFT hyperparams (fixed, no per-activity tuning)\n",
        "        \"PEAK_smooth_sigma_sec\": 0.12,\n",
        "        \"PEAK_thr_frac\": 0.35,\n",
        "        \"PEAK_min_dist_sec\": 0.25,\n",
        "        \"PEAK_prom_frac\": 0.10,\n",
        "\n",
        "        \"FFT_fmin\": 0.3,\n",
        "        \"FFT_fmax\": 5.0,\n",
        "        \"FFT_smooth_sigma_sec\": 0.12,\n",
        "    }\n",
        "\n",
        "    set_strict_seed(CONFIG[\"seed\"])\n",
        "\n",
        "    full_data = load_mhealth_dataset_raw(\n",
        "        CONFIG[\"data_dir\"],\n",
        "        CONFIG[\"TARGET_ACTIVITIES_MAP\"],\n",
        "        CONFIG[\"COLUMN_NAMES\"]\n",
        "    )\n",
        "    if not full_data:\n",
        "        print(\"[ERROR] dataset load failed\")\n",
        "        return\n",
        "\n",
        "    subjects = [f\"subject{i}\" for i in range(1, 11)]\n",
        "    fs = CONFIG[\"fs\"]\n",
        "\n",
        "    def build_labels(act_id):\n",
        "        labels = []\n",
        "        for s in subjects:\n",
        "            if act_id not in CONFIG[\"COUNT_TABLE\"]:\n",
        "                continue\n",
        "            if s not in CONFIG[\"COUNT_TABLE\"][act_id]:\n",
        "                continue\n",
        "            labels.append((s, act_id, CONFIG[\"COUNT_TABLE\"][act_id][s]))\n",
        "        return labels\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 110)\n",
        "    print(\"Classic baselines (Integrator / Peak / FFT), evaluated on Table VI 3 scenarios\")\n",
        "    print(\"=\" * 110)\n",
        "\n",
        "    for case_type, train_act, test_act in CONFIG[\"TABLE_VI_SCENARIOS\"]:\n",
        "        train_name = CONFIG[\"TARGET_ACTIVITIES_MAP\"][train_act]\n",
        "        test_name  = CONFIG[\"TARGET_ACTIVITIES_MAP\"][test_act]\n",
        "\n",
        "        train_trials = prepare_trial_list_raw(\n",
        "            build_labels(train_act),\n",
        "            full_data,\n",
        "            CONFIG[\"TARGET_ACTIVITIES_MAP\"],\n",
        "            CONFIG[\"ACT_FEATURE_MAP\"]\n",
        "        )\n",
        "        test_trials = prepare_trial_list_raw(\n",
        "            build_labels(test_act),\n",
        "            full_data,\n",
        "            CONFIG[\"TARGET_ACTIVITIES_MAP\"],\n",
        "            CONFIG[\"ACT_FEATURE_MAP\"]\n",
        "        )\n",
        "\n",
        "        if len(train_trials) == 0 or len(test_trials) == 0:\n",
        "            print(f\"[Skip] {train_name} -> {test_name} (missing trials)\")\n",
        "            continue\n",
        "\n",
        "        # ---- Integrator calibration on TRAIN\n",
        "        alpha_only = fit_linear_energy_calibrator_alpha_only(train_trials, fs=fs, ridge=1e-8)\n",
        "        alpha_ab, beta_ab = fit_linear_energy_calibrator_alpha_beta(train_trials, fs=fs, ridge=1e-8)\n",
        "\n",
        "        # ---- Peak threshold on TRAIN (global)\n",
        "        peak_thr = fit_peak_threshold_global(\n",
        "            train_trials, fs=fs,\n",
        "            smooth_sigma_sec=CONFIG[\"PEAK_smooth_sigma_sec\"],\n",
        "            thr_frac=CONFIG[\"PEAK_thr_frac\"]\n",
        "        )\n",
        "\n",
        "        # ---- Evaluate on TEST (all subjects)\n",
        "        preds_int_a, preds_int_ab, preds_peak, preds_fft, gts = [], [], [], [], []\n",
        "        for tr in test_trials:\n",
        "            x_raw = tr[\"raw\"]\n",
        "            gt = float(tr[\"count\"])\n",
        "\n",
        "            preds_int_a.append(baselineE2_predict_count_alpha_only(alpha_only, x_raw, fs=fs))\n",
        "            preds_int_ab.append(baselineE2_predict_count_alpha_beta(alpha_ab, beta_ab, x_raw, fs=fs))\n",
        "\n",
        "            preds_peak.append(\n",
        "                peak_counter_predict_count(\n",
        "                    x_raw, fs=fs, height_thr=peak_thr,\n",
        "                    smooth_sigma_sec=CONFIG[\"PEAK_smooth_sigma_sec\"],\n",
        "                    min_dist_sec=CONFIG[\"PEAK_min_dist_sec\"],\n",
        "                    prominence_frac=CONFIG[\"PEAK_prom_frac\"]\n",
        "                )\n",
        "            )\n",
        "\n",
        "            preds_fft.append(\n",
        "                fft_counter_predict_count(\n",
        "                    x_raw, fs=fs,\n",
        "                    fmin=CONFIG[\"FFT_fmin\"], fmax=CONFIG[\"FFT_fmax\"],\n",
        "                    smooth_sigma_sec=CONFIG[\"FFT_smooth_sigma_sec\"]\n",
        "                )\n",
        "            )\n",
        "\n",
        "            gts.append(gt)\n",
        "\n",
        "        s_int_a  = summarize_errors(preds_int_a,  gts)\n",
        "        s_int_ab = summarize_errors(preds_int_ab, gts)\n",
        "        s_peak   = summarize_errors(preds_peak,   gts)\n",
        "        s_fft    = summarize_errors(preds_fft,    gts)\n",
        "\n",
        "        print(f\"\\n[{case_type}] {train_name} -> {test_name}\")\n",
        "        print(f\"  Train-calibration stats: alpha_only={alpha_only:.6e} | alpha={alpha_ab:.6e}, beta={beta_ab:.6e} | peak_thr={peak_thr:.6e}\")\n",
        "        print_summary_block(\"Integrator (alpha-only)\", s_int_a,  indent=\"  \")\n",
        "        print_summary_block(\"Integrator (alpha+beta)\", s_int_ab, indent=\"  \")\n",
        "        print_summary_block(\"Peak Counter\",            s_peak,   indent=\"  \")\n",
        "        print_summary_block(\"FFT Counter\",             s_fft,    indent=\"  \")\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 110)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jfTRKMoRJqFY",
        "outputId": "d8d4d4cb-b177-42da-bb81-1edc3fd804ae"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==============================================================================================================\n",
            "Classic baselines (Integrator / Peak / FFT), evaluated on Table VI 3 scenarios\n",
            "==============================================================================================================\n",
            "\n",
            "[Success case] Frontal elevation of arms -> Knees bending\n",
            "  Train-calibration stats: alpha_only=4.348327e-03 | alpha=-5.992398e-05, beta=2.005494e+01 | peak_thr=4.373187e+00\n",
            "  Integrator (alpha-only) | n=10\n",
            "  MAE (mean±std):   9.991 ± 2.786\n",
            "  MAPE% (mean±std): 48.56 ± 12.64\n",
            "  Bias (mean±std):  -9.991 ± 2.786\n",
            "  Integrator (alpha+beta) | n=10\n",
            "  MAE (mean±std):   0.767 ± 0.466\n",
            "  MAPE% (mean±std): 3.71 ± 2.23\n",
            "  Bias (mean±std):  -0.590 ± 0.697\n",
            "  Peak Counter | n=10\n",
            "  MAE (mean±std):   23.900 ± 7.047\n",
            "  MAPE% (mean±std): 117.49 ± 38.35\n",
            "  Bias (mean±std):  23.900 ± 7.047\n",
            "  FFT Counter | n=10\n",
            "  MAE (mean±std):   16.900 ± 5.763\n",
            "  MAPE% (mean±std): 82.24 ± 27.83\n",
            "  Bias (mean±std):  16.700 ± 6.378\n",
            "\n",
            "[Partial failure] Waist bends forward -> Jump front & back\n",
            "  Train-calibration stats: alpha_only=6.295517e-03 | alpha=1.490610e-04, beta=1.984672e+01 | peak_thr=3.656669e+00\n",
            "  Integrator (alpha-only) | n=10\n",
            "  MAE (mean±std):   54.727 ± 10.184\n",
            "  MAPE% (mean±std): 268.63 ± 49.40\n",
            "  Bias (mean±std):  54.727 ± 10.184\n",
            "  Integrator (alpha+beta) | n=10\n",
            "  MAE (mean±std):   1.376 ± 0.552\n",
            "  MAPE% (mean±std): 6.85 ± 2.99\n",
            "  Bias (mean±std):  1.226 ± 0.860\n",
            "  Peak Counter | n=10\n",
            "  MAE (mean±std):   17.800 ± 4.211\n",
            "  MAPE% (mean±std): 87.84 ± 21.68\n",
            "  Bias (mean±std):  17.800 ± 4.211\n",
            "  FFT Counter | n=10\n",
            "  MAE (mean±std):   19.300 ± 1.829\n",
            "  MAPE% (mean±std): 94.82 ± 9.97\n",
            "  Bias (mean±std):  16.300 ± 11.046\n",
            "\n",
            "[Complete breakdown] Knees bending -> Jogging\n",
            "  Train-calibration stats: alpha_only=8.060321e-03 | alpha=-4.056962e-04, beta=2.148045e+01 | peak_thr=3.846061e+00\n",
            "  Integrator (alpha-only) | n=10\n",
            "  MAE (mean±std):   64.715 ± 25.632\n",
            "  MAPE% (mean±std): 41.18 ± 16.62\n",
            "  Bias (mean±std):  64.715 ± 25.632\n",
            "  Integrator (alpha+beta) | n=10\n",
            "  MAE (mean±std):   147.414 ± 4.025\n",
            "  MAPE% (mean±std): 93.48 ± 0.78\n",
            "  Bias (mean±std):  -147.414 ± 4.025\n",
            "  Peak Counter | n=10\n",
            "  MAE (mean±std):   70.000 ± 15.224\n",
            "  MAPE% (mean±std): 44.25 ± 8.93\n",
            "  Bias (mean±std):  -70.000 ± 15.224\n",
            "  FFT Counter | n=10\n",
            "  MAE (mean±std):   78.600 ± 2.119\n",
            "  MAPE% (mean±std): 49.84 ± 0.23\n",
            "  Bias (mean±std):  -78.600 ± 2.119\n",
            "\n",
            "==============================================================================================================\n"
          ]
        }
      ]
    }
  ]
}