{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# MHEALTH `.pt`\n"
      ],
      "metadata": {
        "id": "rM_ruu7I7ApB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "STS8mHA95_Mc",
        "outputId": "f4e926bf-b545-4a48-ec5b-b68bc3e11aea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n",
            "[Load] X shape=(2867, 15), fs=50, dur=57.34s\n",
            "[Train] windows=13 (win_sec=8.0, stride_sec=4.0)\n",
            "Epoch   1 | loss=5.7923 | mae_count=1.054\n",
            "Epoch   5 | loss=0.9982 | mae_count=0.711\n",
            "Epoch  10 | loss=0.6179 | mae_count=0.502\n",
            "Epoch  20 | loss=0.3701 | mae_count=0.375\n",
            "Epoch  50 | loss=0.2177 | mae_count=0.179\n",
            "Epoch 100 | loss=0.1715 | mae_count=0.124\n",
            "Epoch 200 | loss=0.1564 | mae_count=0.102\n",
            "[Check] Pred(win)=20.72 vs GT=21.00 (diff=-0.28)\n",
            "[OK] Saved: /content/drive/MyDrive/Colab Notebooks/rpi_package/models/mhealth_model.pt\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import glob\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 1) Strict Seeding\n",
        "# ---------------------------------------------------------------------\n",
        "def set_strict_seed(seed: int):\n",
        "    random.seed(seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 2) Window-level supervision (TRAIN) + windowing inference (TEST)\n",
        "# ---------------------------------------------------------------------\n",
        "def trial_list_to_windows(trial_list, fs, win_sec=8.0, stride_sec=4.0, drop_last=True):\n",
        "    win_len = int(round(win_sec * fs))\n",
        "    stride = int(round(stride_sec * fs))\n",
        "    assert win_len > 0 and stride > 0\n",
        "\n",
        "    windows = []\n",
        "    for item in trial_list:\n",
        "        x = item[\"data\"]\n",
        "        T = x.shape[0]\n",
        "        total_count = float(item[\"count\"])\n",
        "        meta = item[\"meta\"]\n",
        "\n",
        "        total_dur = max(T / float(fs), 1e-6)\n",
        "        rate_trial = total_count / total_dur\n",
        "\n",
        "        if T < win_len:\n",
        "            win_dur = T / float(fs)\n",
        "            windows.append({\n",
        "                \"data\": x,\n",
        "                \"count\": rate_trial * win_dur,\n",
        "                \"meta\": f\"{meta}__win[0:{T}]\",\n",
        "                \"parent_meta\": meta,\n",
        "                \"parent_T\": T,\n",
        "                \"win_start\": 0,\n",
        "                \"win_end\": T,\n",
        "            })\n",
        "            continue\n",
        "\n",
        "        last_start = T - win_len\n",
        "        starts = list(range(0, last_start + 1, stride))\n",
        "\n",
        "        for st in starts:\n",
        "            ed = st + win_len\n",
        "            win_dur = win_len / float(fs)\n",
        "            windows.append({\n",
        "                \"data\": x[st:ed],\n",
        "                \"count\": rate_trial * win_dur,\n",
        "                \"meta\": f\"{meta}__win[{st}:{ed}]\",\n",
        "                \"parent_meta\": meta,\n",
        "                \"parent_T\": T,\n",
        "                \"win_start\": st,\n",
        "                \"win_end\": ed,\n",
        "            })\n",
        "\n",
        "        if not drop_last:\n",
        "            last_st = starts[-1] + stride\n",
        "            if last_st < T:\n",
        "                ed = T\n",
        "                win_dur = (ed - last_st) / float(fs)\n",
        "                windows.append({\n",
        "                    \"data\": x[last_st:ed],\n",
        "                    \"count\": rate_trial * win_dur,\n",
        "                    \"meta\": f\"{meta}__win[{last_st}:{ed}]\",\n",
        "                    \"parent_meta\": meta,\n",
        "                    \"parent_T\": T,\n",
        "                    \"win_start\": last_st,\n",
        "                    \"win_end\": ed,\n",
        "                })\n",
        "\n",
        "    return windows\n",
        "\n",
        "\n",
        "def predict_count_by_windowing(model, x_np, fs, win_sec, stride_sec, device, tau=1.0, batch_size=64):\n",
        "    win_len = int(round(win_sec * fs))\n",
        "    stride = int(round(stride_sec * fs))\n",
        "    T = x_np.shape[0]\n",
        "    total_dur = T / float(fs)\n",
        "\n",
        "    if T <= win_len:\n",
        "        x_tensor = torch.tensor(x_np, dtype=torch.float32).transpose(0, 1).unsqueeze(0).to(device)\n",
        "        with torch.no_grad():\n",
        "            rate_hat, _, _, _ = model(x_tensor, mask=None, tau=tau)\n",
        "        pred_count = float(rate_hat.item() * total_dur)\n",
        "        return pred_count, np.array([float(rate_hat.item())], dtype=np.float32)\n",
        "\n",
        "    starts = list(range(0, T - win_len + 1, stride))\n",
        "    windows = np.stack([x_np[st:st + win_len] for st in starts], axis=0)\n",
        "\n",
        "    xw = torch.tensor(windows, dtype=torch.float32).permute(0, 2, 1).to(device)\n",
        "\n",
        "    rates = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, xw.shape[0], batch_size):\n",
        "            xb = xw[i:i + batch_size]\n",
        "            r_hat, _, _, _ = model(xb, mask=None, tau=tau)\n",
        "            rates.append(r_hat.detach().cpu().numpy())\n",
        "\n",
        "    rates = np.concatenate(rates, axis=0)\n",
        "    rate_mean = float(rates.mean())\n",
        "    pred_count = rate_mean * total_dur\n",
        "    return float(pred_count), rates\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 3) Dataset / Collate (variable length padding)\n",
        "# ---------------------------------------------------------------------\n",
        "class TrialDataset(Dataset):\n",
        "    def __init__(self, trial_list):\n",
        "        self.trials = trial_list\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.trials)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.trials[idx]\n",
        "        data = torch.tensor(item['data'], dtype=torch.float32).transpose(0, 1)\n",
        "        count = torch.tensor(item['count'], dtype=torch.float32)\n",
        "        return data, count, item['meta']\n",
        "\n",
        "\n",
        "def collate_variable_length(batch):\n",
        "    max_len = max([x[0].shape[1] for x in batch])\n",
        "    C = batch[0][0].shape[0]\n",
        "\n",
        "    padded_data, masks, counts, metas, lengths = [], [], [], [], []\n",
        "    for data, count, meta in batch:\n",
        "        T = data.shape[1]\n",
        "        lengths.append(T)\n",
        "\n",
        "        pad_size = max_len - T\n",
        "        if pad_size > 0:\n",
        "            pad = torch.zeros(C, pad_size)\n",
        "            d_padded = torch.cat([data, pad], dim=1)\n",
        "            mask = torch.cat([torch.ones(T), torch.zeros(pad_size)], dim=0)\n",
        "        else:\n",
        "            d_padded = data\n",
        "            mask = torch.ones(T)\n",
        "\n",
        "        padded_data.append(d_padded)\n",
        "        masks.append(mask)\n",
        "        counts.append(count)\n",
        "        metas.append(meta)\n",
        "\n",
        "    return {\n",
        "        \"data\": torch.stack(padded_data),\n",
        "        \"mask\": torch.stack(masks),\n",
        "        \"count\": torch.stack(counts),\n",
        "        \"length\": torch.tensor(lengths, dtype=torch.float32),\n",
        "        \"meta\": metas\n",
        "    }\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 4) Model\n",
        "# ---------------------------------------------------------------------\n",
        "class ManifoldEncoder(nn.Module):\n",
        "    def __init__(self, input_ch, hidden_dim=128, latent_dim=16):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv1d(input_ch, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, latent_dim, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        z = self.net(x)\n",
        "        z = z.transpose(1, 2)\n",
        "        return z\n",
        "\n",
        "\n",
        "class ManifoldDecoder(nn.Module):\n",
        "    def __init__(self, latent_dim, hidden_dim, out_ch):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv1d(latent_dim, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, out_ch, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        zt = z.transpose(1, 2)\n",
        "        x_hat = self.net(zt)\n",
        "        return x_hat\n",
        "\n",
        "\n",
        "class MultiRateHead(nn.Module):\n",
        "    def __init__(self, latent_dim=16, hidden=64, K_max=6):\n",
        "        super().__init__()\n",
        "        self.K_max = K_max\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(latent_dim, hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden, 1 + K_max)\n",
        "        )\n",
        "\n",
        "    def forward(self, z, tau=1.0):\n",
        "        out = self.net(z)\n",
        "        amp = F.softplus(out[..., 0])\n",
        "        phase_logits = out[..., 1:]\n",
        "        phase = F.softmax(phase_logits / tau, dim=-1)\n",
        "        return amp, phase, phase_logits\n",
        "\n",
        "\n",
        "class KAutoCountModel(nn.Module):\n",
        "    def __init__(self, input_ch, hidden_dim=128, latent_dim=16, K_max=6, k_hidden=64):\n",
        "        super().__init__()\n",
        "        self.encoder = ManifoldEncoder(input_ch, hidden_dim, latent_dim)\n",
        "        self.decoder = ManifoldDecoder(latent_dim, hidden_dim, input_ch)\n",
        "        self.rate_head = MultiRateHead(latent_dim, hidden=hidden_dim, K_max=K_max)\n",
        "\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, (nn.Conv1d, nn.Linear)):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            b = self.rate_head.net[-1].bias\n",
        "            b.zero_()\n",
        "            b[0].fill_(-2.0)\n",
        "\n",
        "    @staticmethod\n",
        "    def _masked_mean_time(x, mask=None, eps=1e-6):\n",
        "        if mask is None:\n",
        "            return x.mean(dim=1)\n",
        "        if x.dim() == 2:\n",
        "            m = mask.to(dtype=x.dtype, device=x.device)\n",
        "            return (x * m).sum(dim=1) / (m.sum(dim=1) + eps)\n",
        "        elif x.dim() == 3:\n",
        "            m = mask.to(dtype=x.dtype, device=x.device).unsqueeze(-1)\n",
        "            return (x * m).sum(dim=1) / (m.sum(dim=1) + eps)\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported dim for masked mean: {x.dim()}\")\n",
        "\n",
        "    def forward(self, x, mask=None, tau=1.0):\n",
        "        z = self.encoder(x)\n",
        "        x_hat = self.decoder(z)\n",
        "\n",
        "        amp_t, phase_p, phase_logits = self.rate_head(z, tau=tau)\n",
        "        rates_k_t = amp_t.unsqueeze(-1) * phase_p\n",
        "\n",
        "        micro_rate_t = amp_t\n",
        "\n",
        "        p_bar = self._masked_mean_time(phase_p, mask)\n",
        "        k_hat = 1.0 / (p_bar.pow(2).sum(dim=1) + 1e-6)\n",
        "\n",
        "        rep_rate_t = micro_rate_t / (k_hat.unsqueeze(1) + 1e-6)\n",
        "        if mask is not None:\n",
        "            rep_rate_t = rep_rate_t * mask\n",
        "\n",
        "        if mask is None:\n",
        "            avg_rep_rate = rep_rate_t.mean(dim=1)\n",
        "        else:\n",
        "            avg_rep_rate = (rep_rate_t * mask).sum(dim=1) / (mask.sum(dim=1) + 1e-6)\n",
        "\n",
        "        aux = {\n",
        "            \"rates_k_t\": rates_k_t,\n",
        "            \"phase_p\": phase_p,\n",
        "            \"phase_logits\": phase_logits,\n",
        "            \"micro_rate_t\": micro_rate_t,\n",
        "            \"rep_rate_t\": rep_rate_t,\n",
        "            \"k_hat\": k_hat,\n",
        "        }\n",
        "        return avg_rep_rate, z, x_hat, aux\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 5) Loss utils\n",
        "# ---------------------------------------------------------------------\n",
        "def masked_recon_mse(x_hat, x, mask, eps=1e-6):\n",
        "    mask = mask.to(dtype=x.dtype, device=x.device)\n",
        "    mask_bc = mask.unsqueeze(1)\n",
        "    se = (x_hat - x) ** 2\n",
        "    se = se * mask_bc\n",
        "    denom = (mask.sum() * x.shape[1]) + eps\n",
        "    return se.sum() / denom\n",
        "\n",
        "\n",
        "def temporal_smoothness(v, mask=None, eps=1e-6):\n",
        "    dv = torch.abs(v[:, 1:] - v[:, :-1])\n",
        "    if mask is None:\n",
        "        return dv.mean()\n",
        "    m = mask[:, 1:] * mask[:, :-1]\n",
        "    m = m.to(dtype=dv.dtype, device=dv.device)\n",
        "    return (dv * m).sum() / (m.sum() + eps)\n",
        "\n",
        "\n",
        "def phase_entropy_loss(phase_p, mask=None, eps=1e-8):\n",
        "    ent = -(phase_p * (phase_p + eps).log()).sum(dim=-1)\n",
        "    if mask is None:\n",
        "        return ent.mean()\n",
        "    ent = ent * mask\n",
        "    return ent.sum() / (mask.sum() + eps)\n",
        "\n",
        "\n",
        "def effK_usage_loss(phase_p, mask=None, eps=1e-6):\n",
        "    if mask is None:\n",
        "        p_bar = phase_p.mean(dim=1)  # (B,K)\n",
        "    else:\n",
        "        m = mask.to(dtype=phase_p.dtype, device=phase_p.device).unsqueeze(-1)\n",
        "        p_bar = (phase_p * m).sum(dim=1) / (m.sum(dim=1) + eps)\n",
        "\n",
        "    effK = 1.0 / (p_bar.pow(2).sum(dim=1) + eps)\n",
        "    return effK.mean(), effK.detach()\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 6) Train\n",
        "# ---------------------------------------------------------------------\n",
        "def train_one_epoch(model, loader, optimizer, config, device):\n",
        "    model.train()\n",
        "    stats = {k: 0.0 for k in [\n",
        "        'loss', 'loss_rate', 'loss_recon', 'loss_smooth', 'loss_phase_ent', 'loss_effk',\n",
        "        'mae_count'\n",
        "    ]}\n",
        "\n",
        "    fs = config[\"fs\"]\n",
        "    tau = config.get(\"tau\", 1.0)\n",
        "    lam_recon = config.get(\"lambda_recon\", 1.0)\n",
        "    lam_smooth = config.get(\"lambda_smooth\", 0.05)\n",
        "    lam_phase_ent = config.get(\"lambda_phase_ent\", 0.01)\n",
        "    lam_effk = config.get(\"lambda_effk\", 0.005)\n",
        "\n",
        "    for batch in loader:\n",
        "        x = batch[\"data\"].to(device)\n",
        "        mask = batch[\"mask\"].to(device)\n",
        "        y_count = batch[\"count\"].to(device)\n",
        "        length = batch[\"length\"].to(device)\n",
        "\n",
        "        duration = torch.clamp(length / fs, min=1e-6)\n",
        "        y_rate = y_count / duration\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        rate_hat, z, x_hat, aux = model(x, mask, tau=tau)\n",
        "\n",
        "        loss_rate = F.mse_loss(rate_hat, y_rate)\n",
        "        loss_recon = masked_recon_mse(x_hat, x, mask)\n",
        "        loss_smooth = temporal_smoothness(aux[\"rep_rate_t\"], mask)\n",
        "        loss_phase_ent = phase_entropy_loss(aux[\"phase_p\"], mask)\n",
        "        loss_effk, _ = effK_usage_loss(aux[\"phase_p\"], mask)\n",
        "\n",
        "        loss = (loss_rate\n",
        "                + lam_recon * loss_recon\n",
        "                + lam_smooth * loss_smooth\n",
        "                + lam_phase_ent * loss_phase_ent\n",
        "                + lam_effk * loss_effk)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        count_hat = rate_hat * duration\n",
        "        stats['loss'] += loss.item()\n",
        "        stats['loss_rate'] += loss_rate.item()\n",
        "        stats['loss_recon'] += loss_recon.item()\n",
        "        stats['loss_smooth'] += loss_smooth.item()\n",
        "        stats['loss_phase_ent'] += loss_phase_ent.item()\n",
        "        stats['loss_effk'] += loss_effk.item()\n",
        "        stats['mae_count'] += torch.abs(count_hat - y_count).mean().item()\n",
        "\n",
        "    n = len(loader)\n",
        "    return {k: v / n for k, v in stats.items()}\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 7) Main\n",
        "# ---------------------------------------------------------------------\n",
        "def main_overfit_mhealth_npy():\n",
        "    CONFIG = {\n",
        "        \"seed\": 42,\n",
        "\n",
        "        # ✅ 라즈베리 패키지에 저장해둔 npy\n",
        "        \"x_npy\": \"/content/drive/MyDrive/Colab Notebooks/rpi_package/data/mhealth_X.npy\",\n",
        "        \"gt_count\": 21.0,   # GT\n",
        "\n",
        "        # Training params\n",
        "        \"epochs\": 200,      # overfit 목적이면 100~300 아무거나 OK\n",
        "        \"lr\": 5e-4,\n",
        "        \"batch_size\": 64,\n",
        "        \"fs\": 50,\n",
        "\n",
        "        # Windowing\n",
        "        \"win_sec\": 8.0,\n",
        "        \"stride_sec\": 4.0,\n",
        "        \"drop_last\": True,\n",
        "\n",
        "        # Model\n",
        "        \"hidden_dim\": 128,\n",
        "        \"latent_dim\": 16,\n",
        "        \"K_max\": 6,\n",
        "\n",
        "        # Temperature\n",
        "        \"tau\": 1.0,\n",
        "\n",
        "        # Loss weights (너 기존 그대로)\n",
        "        \"lambda_recon\": 1.0,\n",
        "        \"lambda_smooth\": 0.05,\n",
        "        \"lambda_phase_ent\": 0.01,\n",
        "        \"lambda_effk\": 0.0075,\n",
        "    }\n",
        "\n",
        "    set_strict_seed(CONFIG[\"seed\"])\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(\"Device:\", device)\n",
        "\n",
        "    # 1) Load X\n",
        "    X = np.load(CONFIG[\"x_npy\"]).astype(np.float32)  # (T,C)\n",
        "    T, C = X.shape\n",
        "    print(f\"[Load] X shape={X.shape}, fs={CONFIG['fs']}, dur={T/CONFIG['fs']:.2f}s\")\n",
        "\n",
        "    # 2) Build single trial list\n",
        "    trial_list = [{\n",
        "        \"data\": X,\n",
        "        \"count\": float(CONFIG[\"gt_count\"]),\n",
        "        \"meta\": \"mhealth_single_trial\"\n",
        "    }]\n",
        "\n",
        "    # 3) Window-proxy training set\n",
        "    train_windows = trial_list_to_windows(\n",
        "        trial_list,\n",
        "        fs=CONFIG[\"fs\"],\n",
        "        win_sec=CONFIG[\"win_sec\"],\n",
        "        stride_sec=CONFIG[\"stride_sec\"],\n",
        "        drop_last=CONFIG[\"drop_last\"]\n",
        "    )\n",
        "    print(f\"[Train] windows={len(train_windows)} (win_sec={CONFIG['win_sec']}, stride_sec={CONFIG['stride_sec']})\")\n",
        "    assert len(train_windows) > 0\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        TrialDataset(train_windows),\n",
        "        batch_size=CONFIG[\"batch_size\"],\n",
        "        shuffle=True,\n",
        "        collate_fn=collate_variable_length,\n",
        "        num_workers=0\n",
        "    )\n",
        "\n",
        "    # 4) Model init\n",
        "    model = KAutoCountModel(\n",
        "        input_ch=C,\n",
        "        hidden_dim=CONFIG[\"hidden_dim\"],\n",
        "        latent_dim=CONFIG[\"latent_dim\"],\n",
        "        K_max=CONFIG[\"K_max\"]\n",
        "    ).to(device)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=CONFIG[\"lr\"])\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.5)\n",
        "\n",
        "    # 5) Train (overfit)\n",
        "    for epoch in range(1, CONFIG[\"epochs\"] + 1):\n",
        "        stats = train_one_epoch(model, train_loader, optimizer, CONFIG, device)\n",
        "        scheduler.step()\n",
        "\n",
        "        if epoch in [1, 5, 10, 20, 50, 100, CONFIG[\"epochs\"]]:\n",
        "            print(f\"Epoch {epoch:3d} | loss={stats['loss']:.4f} | mae_count={stats['mae_count']:.3f}\")\n",
        "\n",
        "    # 6) Quick self-check: predict count on the SAME trial (windowing inference)\n",
        "    model.eval()\n",
        "    pred_count, _ = predict_count_by_windowing(\n",
        "        model,\n",
        "        x_np=X,\n",
        "        fs=CONFIG[\"fs\"],\n",
        "        win_sec=CONFIG[\"win_sec\"],\n",
        "        stride_sec=CONFIG[\"stride_sec\"],\n",
        "        device=device,\n",
        "        tau=CONFIG[\"tau\"],\n",
        "        batch_size=CONFIG[\"batch_size\"]\n",
        "    )\n",
        "    print(f\"[Check] Pred(win)={pred_count:.2f} vs GT={CONFIG['gt_count']:.2f} (diff={pred_count-CONFIG['gt_count']:+.2f})\")\n",
        "\n",
        "    # 7) Save checkpoint (.pt)\n",
        "    out_dir = \"/content/drive/MyDrive/Colab Notebooks/rpi_package/models\"\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "    ckpt_path = os.path.join(out_dir, \"mhealth_model.pt\")\n",
        "\n",
        "    ckpt = {\n",
        "        \"model_state_dict\": model.state_dict(),\n",
        "        \"model_args\": {\n",
        "            \"input_ch\": C,\n",
        "            \"hidden_dim\": CONFIG[\"hidden_dim\"],\n",
        "            \"latent_dim\": CONFIG[\"latent_dim\"],\n",
        "            \"K_max\": CONFIG[\"K_max\"],\n",
        "        },\n",
        "        \"data_info\": {\n",
        "            \"fs\": CONFIG[\"fs\"],\n",
        "            \"win_sec\": CONFIG[\"win_sec\"],\n",
        "            \"stride_sec\": CONFIG[\"stride_sec\"],\n",
        "            \"gt_count\": float(CONFIG[\"gt_count\"]),\n",
        "            \"x_shape\": [int(T), int(C)],\n",
        "            \"x_npy\": CONFIG[\"x_npy\"],\n",
        "        },\n",
        "        \"train_info\": {\n",
        "            \"seed\": CONFIG[\"seed\"],\n",
        "            \"epochs\": CONFIG[\"epochs\"],\n",
        "            \"lr\": CONFIG[\"lr\"],\n",
        "            \"batch_size\": CONFIG[\"batch_size\"],\n",
        "            \"tau\": CONFIG[\"tau\"],\n",
        "            \"lambda_recon\": CONFIG[\"lambda_recon\"],\n",
        "            \"lambda_smooth\": CONFIG[\"lambda_smooth\"],\n",
        "            \"lambda_phase_ent\": CONFIG[\"lambda_phase_ent\"],\n",
        "            \"lambda_effk\": CONFIG[\"lambda_effk\"],\n",
        "        }\n",
        "    }\n",
        "\n",
        "    torch.save(ckpt, ckpt_path)\n",
        "    print(\"[OK] Saved:\", ckpt_path)\n",
        "\n",
        "main_overfit_mhealth_npy()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MM-Fit `.pt`"
      ],
      "metadata": {
        "id": "uktXh0hR7FlQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 1) Strict Seeding\n",
        "# ---------------------------------------------------------------------\n",
        "def set_strict_seed(seed: int):\n",
        "    random.seed(seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 2) Windowing\n",
        "# ---------------------------------------------------------------------\n",
        "def trial_list_to_windows(trial_list, fs, win_sec=8.0, stride_sec=4.0, drop_last=True):\n",
        "    win_len = int(round(win_sec * fs))\n",
        "    stride = int(round(stride_sec * fs))\n",
        "    assert win_len > 0 and stride > 0\n",
        "\n",
        "    windows = []\n",
        "    for item in trial_list:\n",
        "        x = item[\"data\"]  # (T,C)\n",
        "        T = x.shape[0]\n",
        "        total_count = float(item[\"count\"])\n",
        "        meta = item[\"meta\"]\n",
        "\n",
        "        total_dur = max(T / float(fs), 1e-6)\n",
        "        rate_trial = total_count / total_dur  # reps/s\n",
        "\n",
        "        if T < win_len:\n",
        "            win_dur = T / float(fs)\n",
        "            windows.append({\n",
        "                \"data\": x,\n",
        "                \"count\": rate_trial * win_dur,\n",
        "                \"meta\": f\"{meta}__win[0:{T}]\",\n",
        "                \"parent_meta\": meta,\n",
        "                \"parent_T\": T,\n",
        "                \"win_start\": 0,\n",
        "                \"win_end\": T,\n",
        "            })\n",
        "            continue\n",
        "\n",
        "        last_start = T - win_len\n",
        "        starts = list(range(0, last_start + 1, stride))\n",
        "\n",
        "        for st in starts:\n",
        "            ed = st + win_len\n",
        "            win_dur = win_len / float(fs)\n",
        "            windows.append({\n",
        "                \"data\": x[st:ed],\n",
        "                \"count\": rate_trial * win_dur,\n",
        "                \"meta\": f\"{meta}__win[{st}:{ed}]\",\n",
        "                \"parent_meta\": meta,\n",
        "                \"parent_T\": T,\n",
        "                \"win_start\": st,\n",
        "                \"win_end\": ed,\n",
        "            })\n",
        "\n",
        "        if not drop_last:\n",
        "            last_st = starts[-1] + stride\n",
        "            if last_st < T:\n",
        "                ed = T\n",
        "                win_dur = (ed - last_st) / float(fs)\n",
        "                windows.append({\n",
        "                    \"data\": x[last_st:ed],\n",
        "                    \"count\": rate_trial * win_dur,\n",
        "                    \"meta\": f\"{meta}__win[{last_st}:{ed}]\",\n",
        "                    \"parent_meta\": meta,\n",
        "                    \"parent_T\": T,\n",
        "                    \"win_start\": last_st,\n",
        "                    \"win_end\": ed,\n",
        "                })\n",
        "\n",
        "    return windows\n",
        "\n",
        "\n",
        "def predict_count_by_windowing(model, x_np, fs, win_sec, stride_sec, device, tau=1.0, batch_size=64):\n",
        "    win_len = int(round(win_sec * fs))\n",
        "    stride = int(round(stride_sec * fs))\n",
        "    T = x_np.shape[0]\n",
        "    total_dur = T / float(fs)\n",
        "\n",
        "    if T <= win_len:\n",
        "        x_tensor = torch.tensor(x_np, dtype=torch.float32).transpose(0, 1).unsqueeze(0).to(device)  # (1,C,T)\n",
        "        with torch.no_grad():\n",
        "            rate_hat, _, _, _ = model(x_tensor, mask=None, tau=tau)\n",
        "        pred_count = float(rate_hat.item() * total_dur)\n",
        "        return pred_count, np.array([float(rate_hat.item())], dtype=np.float32)\n",
        "\n",
        "    starts = list(range(0, T - win_len + 1, stride))\n",
        "    windows = np.stack([x_np[st:st + win_len] for st in starts], axis=0)  # (N, win_len, C)\n",
        "\n",
        "    xw = torch.tensor(windows, dtype=torch.float32).permute(0, 2, 1).to(device)  # (N, C, win_len)\n",
        "\n",
        "    rates = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, xw.shape[0], batch_size):\n",
        "            xb = xw[i:i + batch_size]\n",
        "            r_hat, _, _, _ = model(xb, mask=None, tau=tau)  # (B,)\n",
        "            rates.append(r_hat.detach().cpu().numpy())\n",
        "\n",
        "    rates = np.concatenate(rates, axis=0)  # (N,)\n",
        "    rate_mean = float(rates.mean())\n",
        "    pred_count = rate_mean * total_dur\n",
        "    return float(pred_count), rates\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 2.5) Dataset / Collate (UNCHANGED)\n",
        "# ---------------------------------------------------------------------\n",
        "class TrialDataset(Dataset):\n",
        "    def __init__(self, trial_list):\n",
        "        self.trials = trial_list\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.trials)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.trials[idx]\n",
        "        data = torch.tensor(item['data'], dtype=torch.float32).transpose(0, 1)  # (C, T)\n",
        "        count = torch.tensor(item['count'], dtype=torch.float32)\n",
        "        return data, count, item['meta']\n",
        "\n",
        "\n",
        "def collate_variable_length(batch):\n",
        "    max_len = max([x[0].shape[1] for x in batch])\n",
        "    C = batch[0][0].shape[0]\n",
        "\n",
        "    padded_data, masks, counts, metas, lengths = [], [], [], [], []\n",
        "    for data, count, meta in batch:\n",
        "        T = data.shape[1]\n",
        "        lengths.append(T)\n",
        "\n",
        "        pad_size = max_len - T\n",
        "        if pad_size > 0:\n",
        "            pad = torch.zeros(C, pad_size)\n",
        "            d_padded = torch.cat([data, pad], dim=1)\n",
        "            mask = torch.cat([torch.ones(T), torch.zeros(pad_size)], dim=0)\n",
        "        else:\n",
        "            d_padded = data\n",
        "            mask = torch.ones(T)\n",
        "\n",
        "        padded_data.append(d_padded)\n",
        "        masks.append(mask)\n",
        "        counts.append(count)\n",
        "        metas.append(meta)\n",
        "\n",
        "    return {\n",
        "        \"data\": torch.stack(padded_data),         # (B, C, T_max)\n",
        "        \"mask\": torch.stack(masks),               # (B, T_max)\n",
        "        \"count\": torch.stack(counts),             # (B,)\n",
        "        \"length\": torch.tensor(lengths, dtype=torch.float32),  # (B,)\n",
        "        \"meta\": metas\n",
        "    }\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 3) Model (UNCHANGED)\n",
        "# ---------------------------------------------------------------------\n",
        "class ManifoldEncoder(nn.Module):\n",
        "    def __init__(self, input_ch, hidden_dim=128, latent_dim=16):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv1d(input_ch, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, latent_dim, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        z = self.net(x)            # (B, D, T)\n",
        "        z = z.transpose(1, 2)      # (B, T, D)\n",
        "        return z\n",
        "\n",
        "\n",
        "class ManifoldDecoder(nn.Module):\n",
        "    def __init__(self, latent_dim, hidden_dim, out_ch):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv1d(latent_dim, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, out_ch, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        zt = z.transpose(1, 2)     # (B, D, T)\n",
        "        x_hat = self.net(zt)       # (B, C, T)\n",
        "        return x_hat\n",
        "\n",
        "\n",
        "class MultiRateHead(nn.Module):\n",
        "    def __init__(self, latent_dim=16, hidden=64, K_max=6):\n",
        "        super().__init__()\n",
        "        self.K_max = K_max\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(latent_dim, hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden, 1 + K_max)  # [amp_logit | phase_logits...]\n",
        "        )\n",
        "\n",
        "    def forward(self, z, tau=1.0):\n",
        "        out = self.net(z)                     # (B,T,1+K)\n",
        "        amp = F.softplus(out[..., 0])         # (B,T) >=0\n",
        "        phase_logits = out[..., 1:]           # (B,T,K)\n",
        "        phase = F.softmax(phase_logits / tau, dim=-1)  # (B,T,K), sum=1\n",
        "        return amp, phase, phase_logits\n",
        "\n",
        "\n",
        "class KAutoCountModel(nn.Module):\n",
        "    def __init__(self, input_ch, hidden_dim=128, latent_dim=16, K_max=6, k_hidden=64):\n",
        "        super().__init__()\n",
        "        self.encoder = ManifoldEncoder(input_ch, hidden_dim, latent_dim)\n",
        "        self.decoder = ManifoldDecoder(latent_dim, hidden_dim, input_ch)\n",
        "        self.rate_head = MultiRateHead(latent_dim, hidden=hidden_dim, K_max=K_max)\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, (nn.Conv1d, nn.Linear)):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            b = self.rate_head.net[-1].bias\n",
        "            b.zero_()\n",
        "            b[0].fill_(-2.0)  # amp logit bias만 -2\n",
        "\n",
        "    @staticmethod\n",
        "    def _masked_mean_time(x, mask=None, eps=1e-6):\n",
        "        if mask is None:\n",
        "            return x.mean(dim=1)\n",
        "        if x.dim() == 2:\n",
        "            m = mask.to(dtype=x.dtype, device=x.device)\n",
        "            return (x * m).sum(dim=1) / (m.sum(dim=1) + eps)\n",
        "        elif x.dim() == 3:\n",
        "            m = mask.to(dtype=x.dtype, device=x.device).unsqueeze(-1)\n",
        "            return (x * m).sum(dim=1) / (m.sum(dim=1) + eps)\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported dim for masked mean: {x.dim()}\")\n",
        "\n",
        "    def forward(self, x, mask=None, tau=1.0):\n",
        "        z = self.encoder(x)              # (B,T,D)\n",
        "        x_hat = self.decoder(z)          # (B,C,T)\n",
        "\n",
        "        amp_t, phase_p, phase_logits = self.rate_head(z, tau=tau)\n",
        "        rates_k_t = amp_t.unsqueeze(-1) * phase_p  # (B,T,K)\n",
        "\n",
        "        micro_rate_t = amp_t  # (B,T)\n",
        "\n",
        "        p_bar = self._masked_mean_time(phase_p, mask)           # (B,K)\n",
        "        k_hat = 1.0 / (p_bar.pow(2).sum(dim=1) + 1e-6)          # (B,) in [1,K]\n",
        "\n",
        "        rep_rate_t = micro_rate_t / (k_hat.unsqueeze(1) + 1e-6) # (B,T)\n",
        "        if mask is not None:\n",
        "            rep_rate_t = rep_rate_t * mask\n",
        "\n",
        "        if mask is None:\n",
        "            avg_rep_rate = rep_rate_t.mean(dim=1)\n",
        "        else:\n",
        "            avg_rep_rate = (rep_rate_t * mask).sum(dim=1) / (mask.sum(dim=1) + 1e-6)\n",
        "\n",
        "        aux = {\n",
        "            \"rates_k_t\": rates_k_t,\n",
        "            \"phase_p\": phase_p,\n",
        "            \"phase_logits\": phase_logits,\n",
        "            \"micro_rate_t\": micro_rate_t,\n",
        "            \"rep_rate_t\": rep_rate_t,\n",
        "            \"k_hat\": k_hat,\n",
        "        }\n",
        "        return avg_rep_rate, z, x_hat, aux\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 4) Loss utils (UNCHANGED)\n",
        "# ---------------------------------------------------------------------\n",
        "def masked_recon_mse(x_hat, x, mask, eps=1e-6):\n",
        "    mask = mask.to(dtype=x.dtype, device=x.device)\n",
        "    mask_bc = mask.unsqueeze(1)              # (B,1,T)\n",
        "    se = (x_hat - x) ** 2                    # (B,C,T)\n",
        "    se = se * mask_bc\n",
        "    denom = (mask.sum() * x.shape[1]) + eps  # valid(B*T)*C\n",
        "    return se.sum() / denom\n",
        "\n",
        "\n",
        "def temporal_smoothness(v, mask=None, eps=1e-6):\n",
        "    dv = torch.abs(v[:, 1:] - v[:, :-1])  # (B,T-1)\n",
        "    if mask is None:\n",
        "        return dv.mean()\n",
        "    m = mask[:, 1:] * mask[:, :-1]\n",
        "    m = m.to(dtype=dv.dtype, device=dv.device)\n",
        "    return (dv * m).sum() / (m.sum() + eps)\n",
        "\n",
        "\n",
        "def phase_entropy_loss(phase_p, mask=None, eps=1e-8):\n",
        "    ent = -(phase_p * (phase_p + eps).log()).sum(dim=-1)  # (B,T)\n",
        "    if mask is None:\n",
        "        return ent.mean()\n",
        "    ent = ent * mask\n",
        "    return ent.sum() / (mask.sum() + eps)\n",
        "\n",
        "\n",
        "def effK_usage_loss(phase_p, mask=None, eps=1e-6):\n",
        "    if mask is None:\n",
        "        p_bar = phase_p.mean(dim=1)  # (B,K)\n",
        "    else:\n",
        "        m = mask.to(dtype=phase_p.dtype, device=phase_p.device).unsqueeze(-1)  # (B,T,1)\n",
        "        p_bar = (phase_p * m).sum(dim=1) / (m.sum(dim=1) + eps)\n",
        "\n",
        "    effK = 1.0 / (p_bar.pow(2).sum(dim=1) + eps)\n",
        "    return effK.mean(), effK.detach()\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 5) Train\n",
        "# ---------------------------------------------------------------------\n",
        "def train_one_epoch(model, loader, optimizer, config, device):\n",
        "    model.train()\n",
        "    stats = {k: 0.0 for k in [\n",
        "        'loss', 'loss_rate', 'loss_recon', 'loss_smooth', 'loss_phase_ent', 'loss_effk',\n",
        "        'mae_count'\n",
        "    ]}\n",
        "\n",
        "    fs = config[\"fs\"]\n",
        "    tau = config.get(\"tau\", 1.0)\n",
        "\n",
        "    lam_recon = config.get(\"lambda_recon\", 1.0)\n",
        "    lam_smooth = config.get(\"lambda_smooth\", 0.05)\n",
        "    lam_phase_ent = config.get(\"lambda_phase_ent\", 0.01)\n",
        "    lam_effk = config.get(\"lambda_effk\", 0.005)\n",
        "\n",
        "    for batch in loader:\n",
        "        x = batch[\"data\"].to(device)         # (B,C,T)\n",
        "        mask = batch[\"mask\"].to(device)      # (B,T)\n",
        "        y_count = batch[\"count\"].to(device)  # (B,)\n",
        "        length = batch[\"length\"].to(device)  # (B,)\n",
        "\n",
        "        duration = torch.clamp(length / fs, min=1e-6)  # sec\n",
        "        y_rate = y_count / duration                    # reps/s\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        rate_hat, z, x_hat, aux = model(x, mask, tau=tau)\n",
        "\n",
        "        loss_rate = F.mse_loss(rate_hat, y_rate)\n",
        "        loss_recon = masked_recon_mse(x_hat, x, mask)\n",
        "        loss_smooth = temporal_smoothness(aux[\"rep_rate_t\"], mask)\n",
        "        loss_phase_ent = phase_entropy_loss(aux[\"phase_p\"], mask)\n",
        "        loss_effk, _ = effK_usage_loss(aux[\"phase_p\"], mask)\n",
        "\n",
        "        loss = (loss_rate\n",
        "                + lam_recon * loss_recon\n",
        "                + lam_smooth * loss_smooth\n",
        "                + lam_phase_ent * loss_phase_ent\n",
        "                + lam_effk * loss_effk)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        count_hat = rate_hat * duration\n",
        "        stats['loss'] += loss.item()\n",
        "        stats['loss_rate'] += loss_rate.item()\n",
        "        stats['loss_recon'] += loss_recon.item()\n",
        "        stats['loss_smooth'] += loss_smooth.item()\n",
        "        stats['loss_phase_ent'] += loss_phase_ent.item()\n",
        "        stats['loss_effk'] += loss_effk.item()\n",
        "        stats['mae_count'] += torch.abs(count_hat - y_count).mean().item()\n",
        "\n",
        "    n = max(1, len(loader))\n",
        "    return {k: v / n for k, v in stats.items()}\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 6) Main\n",
        "# ---------------------------------------------------------------------\n",
        "def main_overfit_mmfit_npy():\n",
        "    CONFIG = {\n",
        "        \"seed\": 42,\n",
        "        \"x_npy\": \"/content/drive/MyDrive/Colab Notebooks/rpi_package/data/mmfit_X.npy\",\n",
        "        \"gt_count\": 10.0,\n",
        "\n",
        "        \"epochs\": 200,\n",
        "        \"lr\": 5e-4,\n",
        "        \"batch_size\": 64,\n",
        "\n",
        "        \"fs\": 100,\n",
        "        \"win_sec\": 8.0,\n",
        "        \"stride_sec\": 4.0,\n",
        "        \"drop_last\": True,\n",
        "\n",
        "        \"hidden_dim\": 128,\n",
        "        \"latent_dim\": 16,\n",
        "        \"K_max\": 6,\n",
        "\n",
        "        \"lambda_recon\": 1.0,\n",
        "        \"lambda_smooth\": 0.05,\n",
        "        \"lambda_phase_ent\": 0.01,\n",
        "        \"lambda_effk\": 0.0075,\n",
        "\n",
        "        \"tau\": 1.0,\n",
        "    }\n",
        "\n",
        "    set_strict_seed(CONFIG[\"seed\"])\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(\"Device:\", device)\n",
        "\n",
        "    # 1) ✅ Load X (반드시 trial_list보다 먼저!)\n",
        "    X = np.load(CONFIG[\"x_npy\"]).astype(np.float32)  # (T,C)\n",
        "    T, C = X.shape\n",
        "    print(f\"[Load] X shape={X.shape}, fs={CONFIG['fs']}, dur={T/CONFIG['fs']:.2f}s\")\n",
        "\n",
        "    # 2) ✅ GT 고정\n",
        "    gt_count = float(CONFIG[\"gt_count\"])\n",
        "    print(f\"[GT] count(reps) = {gt_count}\")\n",
        "\n",
        "    # 3) Single trial\n",
        "    trial_list = [{\n",
        "        \"data\": X,\n",
        "        \"count\": gt_count,\n",
        "        \"meta\": \"mmfit_single_trial\"\n",
        "    }]\n",
        "\n",
        "    # 4) Window-proxy training set\n",
        "    train_windows = trial_list_to_windows(\n",
        "        trial_list,\n",
        "        fs=CONFIG[\"fs\"],\n",
        "        win_sec=CONFIG[\"win_sec\"],\n",
        "        stride_sec=CONFIG[\"stride_sec\"],\n",
        "        drop_last=CONFIG[\"drop_last\"]\n",
        "    )\n",
        "    print(f\"[Train] windows={len(train_windows)} (win_sec={CONFIG['win_sec']}, stride_sec={CONFIG['stride_sec']})\")\n",
        "    assert len(train_windows) > 0\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        TrialDataset(train_windows),\n",
        "        batch_size=CONFIG[\"batch_size\"],\n",
        "        shuffle=True,\n",
        "        collate_fn=collate_variable_length,\n",
        "        num_workers=0\n",
        "    )\n",
        "\n",
        "    # 5) Model init\n",
        "    model = KAutoCountModel(\n",
        "        input_ch=C,\n",
        "        hidden_dim=CONFIG[\"hidden_dim\"],\n",
        "        latent_dim=CONFIG[\"latent_dim\"],\n",
        "        K_max=CONFIG[\"K_max\"]\n",
        "    ).to(device)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=CONFIG[\"lr\"])\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.5)\n",
        "\n",
        "    # 6) Train (overfit)\n",
        "    for epoch in range(1, CONFIG[\"epochs\"] + 1):\n",
        "        stats = train_one_epoch(model, train_loader, optimizer, CONFIG, device)\n",
        "        scheduler.step()\n",
        "\n",
        "        if epoch in [1, 5, 10, 20, 50, 100, CONFIG[\"epochs\"]]:\n",
        "            print(f\"Epoch {epoch:3d} | loss={stats['loss']:.4f} | mae_count={stats['mae_count']:.3f}\")\n",
        "\n",
        "    # 7) Self-check\n",
        "    model.eval()\n",
        "    pred_count, _ = predict_count_by_windowing(\n",
        "        model,\n",
        "        x_np=X,\n",
        "        fs=CONFIG[\"fs\"],\n",
        "        win_sec=CONFIG[\"win_sec\"],\n",
        "        stride_sec=CONFIG[\"stride_sec\"],\n",
        "        device=device,\n",
        "        tau=CONFIG[\"tau\"],\n",
        "        batch_size=CONFIG[\"batch_size\"]\n",
        "    )\n",
        "    print(f\"[Check] Pred(win)={pred_count:.2f} vs GT={gt_count:.2f} (diff={pred_count-gt_count:+.2f})\")\n",
        "\n",
        "    # 8) Save checkpoint (.pt)\n",
        "    out_dir = \"/content/drive/MyDrive/Colab Notebooks/rpi_package/models\"\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "    ckpt_path = os.path.join(out_dir, \"mmfit_model.pt\")\n",
        "\n",
        "    ckpt = {\n",
        "        \"model_state_dict\": model.state_dict(),\n",
        "        \"model_args\": {\n",
        "            \"input_ch\": C,\n",
        "            \"hidden_dim\": CONFIG[\"hidden_dim\"],\n",
        "            \"latent_dim\": CONFIG[\"latent_dim\"],\n",
        "            \"K_max\": CONFIG[\"K_max\"],\n",
        "        },\n",
        "        \"data_info\": {\n",
        "            \"fs\": CONFIG[\"fs\"],\n",
        "            \"win_sec\": CONFIG[\"win_sec\"],\n",
        "            \"stride_sec\": CONFIG[\"stride_sec\"],\n",
        "            \"gt_count\": float(gt_count),\n",
        "            \"x_shape\": [int(T), int(C)],\n",
        "            \"x_npy\": CONFIG[\"x_npy\"],\n",
        "        },\n",
        "        \"train_info\": {\n",
        "            \"seed\": CONFIG[\"seed\"],\n",
        "            \"epochs\": CONFIG[\"epochs\"],\n",
        "            \"lr\": CONFIG[\"lr\"],\n",
        "            \"batch_size\": CONFIG[\"batch_size\"],\n",
        "            \"tau\": CONFIG[\"tau\"],\n",
        "            \"lambda_recon\": CONFIG[\"lambda_recon\"],\n",
        "            \"lambda_smooth\": CONFIG[\"lambda_smooth\"],\n",
        "            \"lambda_phase_ent\": CONFIG[\"lambda_phase_ent\"],\n",
        "            \"lambda_effk\": CONFIG[\"lambda_effk\"],\n",
        "        }\n",
        "    }\n",
        "\n",
        "    torch.save(ckpt, ckpt_path)\n",
        "    print(\"[OK] Saved:\", ckpt_path)\n",
        "\n",
        "main_overfit_mmfit_npy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UrtwH9IK6_jT",
        "outputId": "94c49709-fa72-458a-b9ee-fd99a872a12b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n",
            "[Load] X shape=(1615, 6), fs=100, dur=16.15s\n",
            "[GT] count(reps) = 10.0\n",
            "[Train] windows=3 (win_sec=8.0, stride_sec=4.0)\n",
            "Epoch   1 | loss=4.8145 | mae_count=4.495\n",
            "Epoch   5 | loss=1.0518 | mae_count=4.091\n",
            "Epoch  10 | loss=0.5686 | mae_count=3.076\n",
            "Epoch  20 | loss=0.1787 | mae_count=0.631\n",
            "Epoch  50 | loss=0.0683 | mae_count=0.033\n",
            "Epoch 100 | loss=0.0488 | mae_count=0.046\n",
            "Epoch 200 | loss=0.0435 | mae_count=0.038\n",
            "[Check] Pred(win)=10.01 vs GT=10.00 (diff=+0.01)\n",
            "[OK] Saved: /content/drive/MyDrive/Colab Notebooks/rpi_package/models/mmfit_model.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# file check"
      ],
      "metadata": {
        "id": "_kvH9gBxIOsv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python \"/content/drive/MyDrive/Colab Notebooks/rpi_package/infer_count.py\" \\\n",
        "  --ckpt \"/content/drive/MyDrive/Colab Notebooks/rpi_package/models/mhealth_model.pt\" \\\n",
        "  --x_npy \"/content/drive/MyDrive/Colab Notebooks/rpi_package/data/mhealth_X.npy\" \\\n",
        "  --device cuda"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aj8aHcByHUq_",
        "outputId": "2bb8998b-c16d-4955-c097-a2d060052083"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Result] Pred=20.72 | GT=21.00 | Diff=-0.28\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python \"/content/drive/MyDrive/Colab Notebooks/rpi_package/infer_count.py\" \\\n",
        "  --ckpt \"/content/drive/MyDrive/Colab Notebooks/rpi_package/models/mmfit_model.pt\" \\\n",
        "  --x_npy \"/content/drive/MyDrive/Colab Notebooks/rpi_package/data/mmfit_X.npy\" \\\n",
        "  --device cuda\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ntIWrjo5HXsZ",
        "outputId": "ae91d118-37d8-4ba6-d1c0-8965338ff7ff"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Result] Pred=10.01 | GT=10.00 | Diff=+0.01\n"
          ]
        }
      ]
    }
  ]
}