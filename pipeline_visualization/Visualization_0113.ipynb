{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7lvpz5kk7828"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Count-only K-auto (Multi-event) + Windowing (LOSO)\n",
        "# + Qualitative 3-Panel Figure:\n",
        "#   (a) Raw ankle acc (3-axis, demean)\n",
        "#   (b) rep_rate(t) + peak markers\n",
        "#       - find_peaks로 후보 넉넉히\n",
        "#       - prominence 큰 순서\n",
        "#       - min_gap 제약 greedy\n",
        "#       - 정확히 topK=GT개 선택 (부족하면 gap relax)\n",
        "#   (c) 3D trajectory: latent manifold (PCA2 of z(t)) × rep_rate(t)\n",
        "#       - trajectory 중심 (line + light scatter)\n",
        "# ============================================================\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from scipy.signal import find_peaks, peak_prominences\n",
        "from mpl_toolkits.mplot3d import Axes3D  # noqa: F401\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 1) Strict Seeding\n",
        "# ---------------------------------------------------------------------\n",
        "def set_strict_seed(seed: int):\n",
        "    random.seed(seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 2) Data Loading\n",
        "# ---------------------------------------------------------------------\n",
        "def load_mhealth_dataset(data_dir, target_activities_map, column_names):\n",
        "    full_dataset = {}\n",
        "    file_list = sorted(glob.glob(os.path.join(data_dir, \"mHealth_subject*.log\")))\n",
        "\n",
        "    if not file_list:\n",
        "        print(f\"[Warning] No mHealth logs found in {data_dir}\")\n",
        "        return {}\n",
        "\n",
        "    print(f\"Loading {len(file_list)} subjects from {data_dir}...\")\n",
        "\n",
        "    for file_path in file_list:\n",
        "        file_name = os.path.basename(file_path)\n",
        "        subj_part = file_name.split(\".\")[0]\n",
        "        try:\n",
        "            subj_id_num = int(\"\".join(filter(str.isdigit, subj_part)))\n",
        "            subj_key = f\"subject{subj_id_num}\"\n",
        "        except Exception:\n",
        "            subj_key = subj_part\n",
        "\n",
        "        try:\n",
        "            df = pd.read_csv(file_path, sep=\"\\t\", header=None)\n",
        "            df = df.iloc[:, :len(column_names)]\n",
        "            df.columns = column_names\n",
        "\n",
        "            subj_data = {}\n",
        "            for label_code, activity_name in target_activities_map.items():\n",
        "                activity_df = df[df[\"activity_id\"] == label_code].copy()\n",
        "                if not activity_df.empty:\n",
        "                    subj_data[activity_name] = activity_df.drop(columns=[\"activity_id\"])\n",
        "\n",
        "            full_dataset[subj_key] = subj_data\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {file_name}: {e}\")\n",
        "\n",
        "    return full_dataset\n",
        "\n",
        "\n",
        "def prepare_trial_list(label_config, full_data, target_map, feature_map):\n",
        "    \"\"\"\n",
        "    trial item:\n",
        "      - data: normalized (T,C)  (model input)\n",
        "      - raw_selected: raw (T,C)  (for plotting)\n",
        "    \"\"\"\n",
        "    trial_list = []\n",
        "    for subj, act_id, gt_count in label_config:\n",
        "        act_name = target_map.get(act_id)\n",
        "        feats = feature_map.get(act_id)\n",
        "\n",
        "        if subj in full_data and act_name in full_data[subj]:\n",
        "            raw_df = full_data[subj][act_name][feats]\n",
        "            raw_np = raw_df.values.astype(np.float32)\n",
        "\n",
        "            mean = raw_np.mean(axis=0, keepdims=True)\n",
        "            std = raw_np.std(axis=0, keepdims=True) + 1e-6\n",
        "            norm_np = (raw_np - mean) / std\n",
        "\n",
        "            trial_list.append({\n",
        "                \"data\": norm_np.astype(np.float32),\n",
        "                \"raw_selected\": raw_np.astype(np.float32),\n",
        "                \"count\": float(gt_count),\n",
        "                \"meta\": f\"{subj}_{act_name}\"\n",
        "            })\n",
        "        else:\n",
        "            print(f\"[Skip] Missing data for {subj} - {act_name}\")\n",
        "\n",
        "    return trial_list\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 3) Window-level supervision (TRAIN) + windowing inference (TEST)\n",
        "# ---------------------------------------------------------------------\n",
        "def trial_list_to_windows(trial_list, fs, win_sec=8.0, stride_sec=4.0, drop_last=True):\n",
        "    win_len = int(round(win_sec * fs))\n",
        "    stride = int(round(stride_sec * fs))\n",
        "    assert win_len > 0 and stride > 0\n",
        "\n",
        "    windows = []\n",
        "    for item in trial_list:\n",
        "        x = item[\"data\"]\n",
        "        T = x.shape[0]\n",
        "        total_count = float(item[\"count\"])\n",
        "        meta = item[\"meta\"]\n",
        "\n",
        "        total_dur = max(T / float(fs), 1e-6)\n",
        "        rate_trial = total_count / total_dur\n",
        "\n",
        "        if T < win_len:\n",
        "            win_dur = T / float(fs)\n",
        "            windows.append({\n",
        "                \"data\": x,\n",
        "                \"count\": rate_trial * win_dur,\n",
        "                \"meta\": f\"{meta}__win[0:{T}]\",\n",
        "                \"parent_meta\": meta,\n",
        "                \"parent_T\": T,\n",
        "                \"win_start\": 0,\n",
        "                \"win_end\": T,\n",
        "            })\n",
        "            continue\n",
        "\n",
        "        last_start = T - win_len\n",
        "        starts = list(range(0, last_start + 1, stride))\n",
        "\n",
        "        for st in starts:\n",
        "            ed = st + win_len\n",
        "            win_dur = win_len / float(fs)\n",
        "            windows.append({\n",
        "                \"data\": x[st:ed],\n",
        "                \"count\": rate_trial * win_dur,\n",
        "                \"meta\": f\"{meta}__win[{st}:{ed}]\",\n",
        "                \"parent_meta\": meta,\n",
        "                \"parent_T\": T,\n",
        "                \"win_start\": st,\n",
        "                \"win_end\": ed,\n",
        "            })\n",
        "\n",
        "        if not drop_last:\n",
        "            last_st = starts[-1] + stride\n",
        "            if last_st < T:\n",
        "                ed = T\n",
        "                win_dur = (ed - last_st) / float(fs)\n",
        "                windows.append({\n",
        "                    \"data\": x[last_st:ed],\n",
        "                    \"count\": rate_trial * win_dur,\n",
        "                    \"meta\": f\"{meta}__win[{last_st}:{ed}]\",\n",
        "                    \"parent_meta\": meta,\n",
        "                    \"parent_T\": T,\n",
        "                    \"win_start\": last_st,\n",
        "                    \"win_end\": ed,\n",
        "                })\n",
        "\n",
        "    return windows\n",
        "\n",
        "\n",
        "def predict_count_by_windowing(model, x_np, fs, win_sec, stride_sec, device, tau=1.0, batch_size=64):\n",
        "    win_len = int(round(win_sec * fs))\n",
        "    stride = int(round(stride_sec * fs))\n",
        "    T = x_np.shape[0]\n",
        "    total_dur = T / float(fs)\n",
        "\n",
        "    if T <= win_len:\n",
        "        x_tensor = torch.tensor(x_np, dtype=torch.float32).transpose(0, 1).unsqueeze(0).to(device)\n",
        "        with torch.no_grad():\n",
        "            rate_hat, _, _, _ = model(x_tensor, mask=None, tau=tau)\n",
        "        pred_count = float(rate_hat.item() * total_dur)\n",
        "        return pred_count, np.array([float(rate_hat.item())], dtype=np.float32)\n",
        "\n",
        "    starts = list(range(0, T - win_len + 1, stride))\n",
        "    windows = np.stack([x_np[st:st + win_len] for st in starts], axis=0)\n",
        "\n",
        "    xw = torch.tensor(windows, dtype=torch.float32).permute(0, 2, 1).to(device)\n",
        "\n",
        "    rates = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, xw.shape[0], batch_size):\n",
        "            xb = xw[i:i + batch_size]\n",
        "            r_hat, _, _, _ = model(xb, mask=None, tau=tau)\n",
        "            rates.append(r_hat.detach().cpu().numpy())\n",
        "\n",
        "    rates = np.concatenate(rates, axis=0)\n",
        "    rate_mean = float(rates.mean())\n",
        "    pred_count = rate_mean * total_dur\n",
        "    return float(pred_count), rates\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 4) Dataset / Collate\n",
        "# ---------------------------------------------------------------------\n",
        "class TrialDataset(Dataset):\n",
        "    def __init__(self, trial_list):\n",
        "        self.trials = trial_list\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.trials)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.trials[idx]\n",
        "        data = torch.tensor(item[\"data\"], dtype=torch.float32).transpose(0, 1)  # (C,T)\n",
        "        count = torch.tensor(item[\"count\"], dtype=torch.float32)\n",
        "        return data, count, item[\"meta\"]\n",
        "\n",
        "\n",
        "def collate_variable_length(batch):\n",
        "    max_len = max([x[0].shape[1] for x in batch])\n",
        "    C = batch[0][0].shape[0]\n",
        "\n",
        "    padded_data, masks, counts, metas, lengths = [], [], [], [], []\n",
        "    for data, count, meta in batch:\n",
        "        T = data.shape[1]\n",
        "        lengths.append(T)\n",
        "\n",
        "        pad_size = max_len - T\n",
        "        if pad_size > 0:\n",
        "            pad = torch.zeros(C, pad_size)\n",
        "            d_padded = torch.cat([data, pad], dim=1)\n",
        "            mask = torch.cat([torch.ones(T), torch.zeros(pad_size)], dim=0)\n",
        "        else:\n",
        "            d_padded = data\n",
        "            mask = torch.ones(T)\n",
        "\n",
        "        padded_data.append(d_padded)\n",
        "        masks.append(mask)\n",
        "        counts.append(count)\n",
        "        metas.append(meta)\n",
        "\n",
        "    return {\n",
        "        \"data\": torch.stack(padded_data),\n",
        "        \"mask\": torch.stack(masks),\n",
        "        \"count\": torch.stack(counts),\n",
        "        \"length\": torch.tensor(lengths, dtype=torch.float32),\n",
        "        \"meta\": metas\n",
        "    }\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 5) Model\n",
        "# ---------------------------------------------------------------------\n",
        "class ManifoldEncoder(nn.Module):\n",
        "    def __init__(self, input_ch, hidden_dim=128, latent_dim=16):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv1d(input_ch, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, latent_dim, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        z = self.net(x)        # (B,D,T)\n",
        "        z = z.transpose(1, 2)  # (B,T,D)\n",
        "        return z\n",
        "\n",
        "\n",
        "class ManifoldDecoder(nn.Module):\n",
        "    def __init__(self, latent_dim, hidden_dim, out_ch):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv1d(latent_dim, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, out_ch, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        zt = z.transpose(1, 2)\n",
        "        x_hat = self.net(zt)\n",
        "        return x_hat\n",
        "\n",
        "\n",
        "class MultiRateHead(nn.Module):\n",
        "    def __init__(self, latent_dim=16, hidden=64, K_max=6):\n",
        "        super().__init__()\n",
        "        self.K_max = K_max\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(latent_dim, hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden, 1 + K_max)\n",
        "        )\n",
        "\n",
        "    def forward(self, z, tau=1.0):\n",
        "        out = self.net(z)\n",
        "        amp = F.softplus(out[..., 0])\n",
        "        phase_logits = out[..., 1:]\n",
        "        phase = F.softmax(phase_logits / tau, dim=-1)\n",
        "        return amp, phase, phase_logits\n",
        "\n",
        "\n",
        "class KAutoCountModel(nn.Module):\n",
        "    def __init__(self, input_ch, hidden_dim=128, latent_dim=16, K_max=6):\n",
        "        super().__init__()\n",
        "        self.encoder = ManifoldEncoder(input_ch, hidden_dim, latent_dim)\n",
        "        self.decoder = ManifoldDecoder(latent_dim, hidden_dim, input_ch)\n",
        "        self.rate_head = MultiRateHead(latent_dim, hidden=hidden_dim, K_max=K_max)\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, (nn.Conv1d, nn.Linear)):\n",
        "                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            b = self.rate_head.net[-1].bias\n",
        "            b.zero_()\n",
        "            b[0].fill_(-2.0)\n",
        "\n",
        "    @staticmethod\n",
        "    def _masked_mean_time(x, mask=None, eps=1e-6):\n",
        "        if mask is None:\n",
        "            return x.mean(dim=1)\n",
        "        if x.dim() == 2:\n",
        "            m = mask.to(dtype=x.dtype, device=x.device)\n",
        "            return (x * m).sum(dim=1) / (m.sum(dim=1) + eps)\n",
        "        elif x.dim() == 3:\n",
        "            m = mask.to(dtype=x.dtype, device=x.device).unsqueeze(-1)\n",
        "            return (x * m).sum(dim=1) / (m.sum(dim=1) + eps)\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported dim for masked mean: {x.dim()}\")\n",
        "\n",
        "    def forward(self, x, mask=None, tau=1.0):\n",
        "        z = self.encoder(x)\n",
        "        x_hat = self.decoder(z)\n",
        "\n",
        "        amp_t, phase_p, phase_logits = self.rate_head(z, tau=tau)\n",
        "        rates_k_t = amp_t.unsqueeze(-1) * phase_p\n",
        "\n",
        "        micro_rate_t = amp_t\n",
        "        p_bar = self._masked_mean_time(phase_p, mask)\n",
        "        k_hat = 1.0 / (p_bar.pow(2).sum(dim=1) + 1e-6)\n",
        "\n",
        "        rep_rate_t = micro_rate_t / (k_hat.unsqueeze(1) + 1e-6)\n",
        "        if mask is not None:\n",
        "            rep_rate_t = rep_rate_t * mask\n",
        "\n",
        "        if mask is None:\n",
        "            avg_rep_rate = rep_rate_t.mean(dim=1)\n",
        "        else:\n",
        "            avg_rep_rate = (rep_rate_t * mask).sum(dim=1) / (mask.sum(dim=1) + 1e-6)\n",
        "\n",
        "        aux = {\n",
        "            \"rates_k_t\": rates_k_t,\n",
        "            \"phase_p\": phase_p,\n",
        "            \"phase_logits\": phase_logits,\n",
        "            \"micro_rate_t\": micro_rate_t,\n",
        "            \"rep_rate_t\": rep_rate_t,\n",
        "            \"k_hat\": k_hat,\n",
        "        }\n",
        "        return avg_rep_rate, z, x_hat, aux\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 6) Loss utils\n",
        "# ---------------------------------------------------------------------\n",
        "def masked_recon_mse(x_hat, x, mask, eps=1e-6):\n",
        "    mask = mask.to(dtype=x.dtype, device=x.device)\n",
        "    mask_bc = mask.unsqueeze(1)\n",
        "    se = (x_hat - x) ** 2\n",
        "    se = se * mask_bc\n",
        "    denom = (mask.sum() * x.shape[1]) + eps\n",
        "    return se.sum() / denom\n",
        "\n",
        "\n",
        "def temporal_smoothness(v, mask=None, eps=1e-6):\n",
        "    dv = torch.abs(v[:, 1:] - v[:, :-1])\n",
        "    if mask is None:\n",
        "        return dv.mean()\n",
        "    m = mask[:, 1:] * mask[:, :-1]\n",
        "    m = m.to(dtype=dv.dtype, device=dv.device)\n",
        "    return (dv * m).sum() / (m.sum() + eps)\n",
        "\n",
        "\n",
        "def phase_entropy_loss(phase_p, mask=None, eps=1e-8):\n",
        "    ent = -(phase_p * (phase_p + eps).log()).sum(dim=-1)\n",
        "    if mask is None:\n",
        "        return ent.mean()\n",
        "    ent = ent * mask\n",
        "    return ent.sum() / (mask.sum() + eps)\n",
        "\n",
        "\n",
        "def effK_usage_loss(phase_p, mask=None, eps=1e-6):\n",
        "    if mask is None:\n",
        "        p_bar = phase_p.mean(dim=1)\n",
        "    else:\n",
        "        m = mask.to(dtype=phase_p.dtype, device=phase_p.device).unsqueeze(-1)\n",
        "        p_bar = (phase_p * m).sum(dim=1) / (m.sum(dim=1) + eps)\n",
        "\n",
        "    effK = 1.0 / (p_bar.pow(2).sum(dim=1) + eps)\n",
        "    return effK.mean(), effK.detach()\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 7) Train\n",
        "# ---------------------------------------------------------------------\n",
        "def train_one_epoch(model, loader, optimizer, config, device):\n",
        "    model.train()\n",
        "    stats = {k: 0.0 for k in [\n",
        "        \"loss\", \"loss_rate\", \"loss_recon\", \"loss_smooth\", \"loss_phase_ent\", \"loss_effk\",\n",
        "        \"mae_count\"\n",
        "    ]}\n",
        "\n",
        "    fs = config[\"fs\"]\n",
        "    tau = config.get(\"tau\", 1.0)\n",
        "    lam_recon = config.get(\"lambda_recon\", 1.0)\n",
        "    lam_smooth = config.get(\"lambda_smooth\", 0.05)\n",
        "    lam_phase_ent = config.get(\"lambda_phase_ent\", 0.01)\n",
        "    lam_effk = config.get(\"lambda_effk\", 0.005)\n",
        "\n",
        "    for batch in loader:\n",
        "        x = batch[\"data\"].to(device)\n",
        "        mask = batch[\"mask\"].to(device)\n",
        "        y_count = batch[\"count\"].to(device)\n",
        "        length = batch[\"length\"].to(device)\n",
        "\n",
        "        duration = torch.clamp(length / fs, min=1e-6)\n",
        "        y_rate = y_count / duration\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        rate_hat, _, x_hat, aux = model(x, mask, tau=tau)\n",
        "\n",
        "        loss_rate = F.mse_loss(rate_hat, y_rate)\n",
        "        loss_recon = masked_recon_mse(x_hat, x, mask)\n",
        "        loss_smooth = temporal_smoothness(aux[\"rep_rate_t\"], mask)\n",
        "        loss_phase_ent = phase_entropy_loss(aux[\"phase_p\"], mask)\n",
        "        loss_effk, _ = effK_usage_loss(aux[\"phase_p\"], mask)\n",
        "\n",
        "        loss = (loss_rate\n",
        "                + lam_recon * loss_recon\n",
        "                + lam_smooth * loss_smooth\n",
        "                + lam_phase_ent * loss_phase_ent\n",
        "                + lam_effk * loss_effk)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        count_hat = rate_hat * duration\n",
        "        stats[\"loss\"] += loss.item()\n",
        "        stats[\"loss_rate\"] += loss_rate.item()\n",
        "        stats[\"loss_recon\"] += loss_recon.item()\n",
        "        stats[\"loss_smooth\"] += loss_smooth.item()\n",
        "        stats[\"loss_phase_ent\"] += loss_phase_ent.item()\n",
        "        stats[\"loss_effk\"] += loss_effk.item()\n",
        "        stats[\"mae_count\"] += torch.abs(count_hat - y_count).mean().item()\n",
        "\n",
        "    n = max(1, len(loader))\n",
        "    return {k: v / n for k, v in stats.items()}\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 8) Visualization helpers\n",
        "# ---------------------------------------------------------------------\n",
        "def _moving_average(x, w):\n",
        "    if w <= 1:\n",
        "        return np.asarray(x, dtype=np.float32)\n",
        "    x = np.asarray(x, dtype=np.float32)\n",
        "    pad = w // 2\n",
        "    xp = np.pad(x, (pad, pad), mode=\"edge\")\n",
        "    k = np.ones(w, dtype=np.float32) / float(w)\n",
        "    y = np.convolve(xp, k, mode=\"valid\")\n",
        "    return y.astype(np.float32)\n",
        "\n",
        "\n",
        "def _pca_project_np(Z, n_components=2):\n",
        "    \"\"\"\n",
        "    Z: (T, D)\n",
        "    return:\n",
        "      P: (T, n_components)\n",
        "    \"\"\"\n",
        "    Z = np.asarray(Z, dtype=np.float32)\n",
        "    mu = Z.mean(axis=0, keepdims=True)\n",
        "    Zc = Z - mu\n",
        "    _, _, Vt = np.linalg.svd(Zc, full_matrices=False)\n",
        "    comps = Vt[:n_components].T  # (D, n_components)\n",
        "    P = Zc @ comps\n",
        "    return P.astype(np.float32)\n",
        "\n",
        "\n",
        "def _select_topk_by_prominence_greedy(peaks, prominences, topk, min_gap):\n",
        "    if peaks is None or len(peaks) == 0:\n",
        "        return np.array([], dtype=np.int64)\n",
        "\n",
        "    peaks = np.asarray(peaks, dtype=np.int64)\n",
        "    prominences = np.asarray(prominences, dtype=np.float32)\n",
        "\n",
        "    order = np.argsort(-prominences)  # desc\n",
        "    picked = []\n",
        "    for idx in order:\n",
        "        p = int(peaks[idx])\n",
        "        if all(abs(p - q) >= min_gap for q in picked):\n",
        "            picked.append(p)\n",
        "        if len(picked) >= topk:\n",
        "            break\n",
        "\n",
        "    return np.array(sorted(picked), dtype=np.int64)\n",
        "\n",
        "\n",
        "def pick_peaks_exact_topk(rr_s, fs, topk, min_gap_sec,\n",
        "                          base_distance_sec=0.15,\n",
        "                          base_prom_q=0.40):\n",
        "    \"\"\"\n",
        "    1) find_peaks로 후보 넉넉히\n",
        "    2) prominence 계산\n",
        "    3) prominence desc + greedy(min_gap)로 topk 선택\n",
        "    4) 부족하면 min_gap 완화\n",
        "    5) 그래도 부족하면 gap 무시로 topk 보장\n",
        "    \"\"\"\n",
        "    rr_s = np.asarray(rr_s, dtype=np.float32).reshape(-1)\n",
        "    base_dist = max(1, int(round(base_distance_sec * fs)))\n",
        "\n",
        "    cand, _ = find_peaks(rr_s, distance=base_dist)\n",
        "    if len(cand) == 0:\n",
        "        return np.array([], dtype=np.int64)\n",
        "\n",
        "    prom = peak_prominences(rr_s, cand)[0].astype(np.float32)\n",
        "\n",
        "    # (optional) 아주 작은 prominence 제거 (하지만 topk 유지 가능할 때만)\n",
        "    if len(prom) >= 10:\n",
        "        thr = float(np.quantile(prom, base_prom_q))\n",
        "        keep = prom >= thr\n",
        "        cand2, prom2 = cand[keep], prom[keep]\n",
        "        if len(cand2) >= topk:\n",
        "            cand, prom = cand2, prom2\n",
        "\n",
        "    min_gap = max(1, int(round(min_gap_sec * fs)))\n",
        "    picked = _select_topk_by_prominence_greedy(cand, prom, topk=topk, min_gap=min_gap)\n",
        "\n",
        "    if len(picked) < topk:\n",
        "        # gap relax\n",
        "        for relax in [0.85, 0.70, 0.55, 0.40, 0.25, 0.10]:\n",
        "            mg = max(1, int(round(min_gap * relax)))\n",
        "            picked = _select_topk_by_prominence_greedy(cand, prom, topk=topk, min_gap=mg)\n",
        "            if len(picked) >= topk:\n",
        "                break\n",
        "\n",
        "    if len(picked) < topk:\n",
        "        # 마지막 보장: gap 무시하고 prominence 큰 것부터 채움\n",
        "        order = np.argsort(-prom)\n",
        "        picked_set = set(picked.tolist())\n",
        "        for idx in order:\n",
        "            picked_set.add(int(cand[idx]))\n",
        "            if len(picked_set) >= topk:\n",
        "                break\n",
        "        picked = np.array(sorted(list(picked_set))[:topk], dtype=np.int64)\n",
        "\n",
        "    return picked\n",
        "\n",
        "\n",
        "def plot_raw_rep_and_3d_traj(\n",
        "    raw_ankle_acc_xyz,  # (T,3) raw\n",
        "    rep_rate_t,         # (T,)\n",
        "    Z_t,                # (T,D)\n",
        "    fs,\n",
        "    title,\n",
        "    out_png,\n",
        "    gt_count,\n",
        "    pred_count,\n",
        "    smooth_w=11,\n",
        "    min_gap_sec=2.25,\n",
        "    max_points_3d=1200,\n",
        "    # bigger 3D (c)\n",
        "    fig_w=15.5,\n",
        "    fig_h=12.0,\n",
        "    h_ratios=(1.0, 1.0, 3.2),\n",
        "    cb_shrink=0.95,\n",
        "    cb_pad=0.03,\n",
        "):\n",
        "    raw = np.asarray(raw_ankle_acc_xyz, dtype=np.float32)\n",
        "    rr = np.asarray(rep_rate_t, dtype=np.float32).reshape(-1)\n",
        "    Z = np.asarray(Z_t, dtype=np.float32)\n",
        "\n",
        "    T = rr.shape[0]\n",
        "    t = np.arange(T, dtype=np.float32) / float(fs)\n",
        "\n",
        "    # (a) demean raw\n",
        "    raw_demean = raw - raw.mean(axis=0, keepdims=True)\n",
        "\n",
        "    # (b) smooth rep_rate\n",
        "    rr_s = _moving_average(rr, smooth_w)\n",
        "\n",
        "    # (b) peaks: exact topK = GT\n",
        "    topk = int(round(float(gt_count)))\n",
        "    peaks = pick_peaks_exact_topk(\n",
        "        rr_s=rr_s,\n",
        "        fs=fs,\n",
        "        topk=topk,\n",
        "        min_gap_sec=min_gap_sec,\n",
        "        base_distance_sec=0.15,\n",
        "        base_prom_q=0.40,\n",
        "    )\n",
        "\n",
        "    # (c) 3D: PCA2(z(t)) + rep_rate(t)\n",
        "    P2 = _pca_project_np(Z, n_components=2)\n",
        "\n",
        "    # downsample for trajectory-centric view\n",
        "    stride = max(1, int(np.ceil(T / float(max_points_3d))))\n",
        "    idx_ds = np.arange(0, T, stride, dtype=np.int64)\n",
        "\n",
        "    P2_ds = P2[idx_ds]\n",
        "    rr_ds = rr_s[idx_ds]\n",
        "    t_ds = t[idx_ds]\n",
        "\n",
        "    P2_pk = P2[peaks] if len(peaks) > 0 else np.zeros((0, 2), dtype=np.float32)\n",
        "    rr_pk = rr_s[peaks] if len(peaks) > 0 else np.zeros((0,), dtype=np.float32)\n",
        "\n",
        "    # ---- figure layout ----\n",
        "    fig = plt.figure(figsize=(fig_w, fig_h))\n",
        "    gs = fig.add_gridspec(3, 1, height_ratios=list(h_ratios), hspace=0.28)\n",
        "\n",
        "    ax1 = fig.add_subplot(gs[0, 0])\n",
        "    ax2 = fig.add_subplot(gs[1, 0], sharex=ax1)\n",
        "    ax3 = fig.add_subplot(gs[2, 0], projection=\"3d\")\n",
        "\n",
        "    # (a)\n",
        "    ax1.plot(t, raw_demean[:, 0], lw=1.0, label=\"acc_ankle_x (demean)\")\n",
        "    ax1.plot(t, raw_demean[:, 1], lw=1.0, label=\"acc_ankle_y (demean)\")\n",
        "    ax1.plot(t, raw_demean[:, 2], lw=1.0, label=\"acc_ankle_z (demean)\")\n",
        "    ax1.set_title(\"(a) Raw ankle accelerometer (demean, 3-axis)\")\n",
        "    ax1.set_ylabel(\"Acceleration (demeaned)\")\n",
        "    ax1.grid(True, alpha=0.25)\n",
        "    ax1.legend(loc=\"upper right\")\n",
        "\n",
        "    # (b)\n",
        "    ax2.plot(t, rr_s, lw=1.4, label=\"rep_rate(t) (smoothed)\")\n",
        "    if len(peaks) > 0:\n",
        "        ax2.scatter(t[peaks], rr_s[peaks],\n",
        "                    marker=\"*\", s=220, c=\"red\",\n",
        "                    edgecolors=\"k\", linewidths=0.8,\n",
        "                    label=f\"topK peaks (K=GT={topk})\")\n",
        "    ax2.set_title(f\"(b) rep_rate(t) with topK=GT peak markers (min_gap≈{min_gap_sec:.2f}s)\")\n",
        "    ax2.set_xlabel(\"Time (s)\")\n",
        "    ax2.set_ylabel(\"Reps / second\")\n",
        "    ax2.grid(True, alpha=0.25)\n",
        "    ax2.legend(loc=\"upper right\")\n",
        "    ax2.text(\n",
        "        0.01, 0.96,\n",
        "        f\"GT={gt_count:.0f} | Pred={pred_count:.2f} | K(peaks)={len(peaks)}\",\n",
        "        transform=ax2.transAxes,\n",
        "        va=\"top\", ha=\"left\",\n",
        "        bbox=dict(boxstyle=\"round\", alpha=0.12)\n",
        "    )\n",
        "\n",
        "    # (c) trajectory-centered: line + light scatter colored by time\n",
        "    ax3.plot(P2_ds[:, 0], P2_ds[:, 1], rr_ds, lw=1.1, alpha=0.85)\n",
        "    sc = ax3.scatter(P2_ds[:, 0], P2_ds[:, 1], rr_ds, c=t_ds, s=10, alpha=0.28)\n",
        "\n",
        "    if len(peaks) > 0:\n",
        "        ax3.scatter(P2_pk[:, 0], P2_pk[:, 1], rr_pk,\n",
        "                    marker=\"*\", s=180, c=\"red\",\n",
        "                    edgecolors=\"k\", linewidths=0.8)\n",
        "\n",
        "    ax3.set_title(\"(c) 3D coupling: latent trajectory (PCA-2D) × rep_rate(t)\")\n",
        "    ax3.set_xlabel(\"PC1\")\n",
        "    ax3.set_ylabel(\"PC2\")\n",
        "    ax3.set_zlabel(\"rep_rate(t)\")\n",
        "    ax3.view_init(elev=22, azim=-62)\n",
        "    ax3.grid(True, alpha=0.25)\n",
        "\n",
        "    cb = fig.colorbar(sc, ax=ax3, pad=cb_pad, shrink=cb_shrink)\n",
        "    cb.set_label(\"Time (s)\")\n",
        "\n",
        "    fig.suptitle(title, fontsize=14)\n",
        "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
        "\n",
        "    os.makedirs(os.path.dirname(out_png), exist_ok=True)\n",
        "    plt.savefig(out_png, dpi=300)\n",
        "    plt.close()\n",
        "\n",
        "    print(f\"[Saved] {out_png}\")\n",
        "    print(f\"[Peaks] topK={topk} selected (K={len(peaks)}) | min_gap_sec={min_gap_sec}\")\n",
        "    return peaks\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 9) Main (LOSO)\n",
        "# ---------------------------------------------------------------------\n",
        "def main():\n",
        "    CONFIG = {\n",
        "        \"seed\": 42,\n",
        "        \"data_dir\": \"/content/drive/MyDrive/Colab Notebooks/HAR_data/MHEALTHDATASET\",\n",
        "\n",
        "        \"COLUMN_NAMES\": [\n",
        "            \"acc_chest_x\", \"acc_chest_y\", \"acc_chest_z\",\n",
        "            \"ecg_1\", \"ecg_2\",\n",
        "            \"acc_ankle_x\", \"acc_ankle_y\", \"acc_ankle_z\",\n",
        "            \"gyro_ankle_x\", \"gyro_ankle_y\", \"gyro_ankle_z\",\n",
        "            \"mag_ankle_x\", \"mag_ankle_y\", \"mag_ankle_z\",\n",
        "            \"acc_arm_x\", \"acc_arm_y\", \"acc_arm_z\",\n",
        "            \"gyro_arm_x\", \"gyro_arm_y\", \"gyro_arm_z\",\n",
        "            \"mag_arm_x\", \"mag_arm_y\", \"mag_arm_z\",\n",
        "            \"activity_id\"\n",
        "        ],\n",
        "\n",
        "        \"TARGET_ACTIVITIES_MAP\": {6: \"Waist bends forward\"},\n",
        "\n",
        "        \"ACT_FEATURE_MAP\": {\n",
        "            6: [\n",
        "                \"acc_chest_x\", \"acc_chest_y\", \"acc_chest_z\",\n",
        "                \"acc_ankle_x\", \"acc_ankle_y\", \"acc_ankle_z\",\n",
        "                \"gyro_ankle_x\", \"gyro_ankle_y\", \"gyro_ankle_z\",\n",
        "                \"acc_arm_x\", \"acc_arm_y\", \"acc_arm_z\",\n",
        "                \"gyro_arm_x\", \"gyro_arm_y\", \"gyro_arm_z\"\n",
        "            ],\n",
        "        },\n",
        "\n",
        "        \"ALL_LABELS\": [\n",
        "            (\"subject1\", 6, 21), (\"subject2\", 6, 19), (\"subject3\", 6, 21),\n",
        "            (\"subject4\", 6, 20), (\"subject5\", 6, 20), (\"subject6\", 6, 20),\n",
        "            (\"subject7\", 6, 20), (\"subject8\", 6, 21), (\"subject9\", 6, 21),\n",
        "            (\"subject10\", 6, 20),\n",
        "        ],\n",
        "\n",
        "        \"epochs\": 100,\n",
        "        \"lr\": 5e-4,\n",
        "        \"batch_size\": 64,\n",
        "        \"fs\": 50,\n",
        "\n",
        "        \"win_sec\": 8.0,\n",
        "        \"stride_sec\": 4.0,\n",
        "        \"drop_last\": True,\n",
        "\n",
        "        \"hidden_dim\": 128,\n",
        "        \"latent_dim\": 16,\n",
        "        \"K_max\": 6,\n",
        "\n",
        "        \"tau\": 1.0,\n",
        "\n",
        "        \"lambda_recon\": 1.0,\n",
        "        \"lambda_smooth\": 0.05,\n",
        "        \"lambda_phase_ent\": 0.01,\n",
        "        \"lambda_effk\": 0.0075,\n",
        "\n",
        "        # Visualization\n",
        "        \"VIZ_ENABLE\": True,\n",
        "        \"VIZ_TARGET_SUBJ\": \"subject1\",\n",
        "        \"VIZ_OUT_DIR\": \"./viz_raw_rep_3d\",\n",
        "        \"VIZ_SMOOTH_W\": 11,\n",
        "        \"VIZ_MIN_GAP_SEC\": 2.25,\n",
        "        \"VIZ_MAX_POINTS_3D\": 1200,\n",
        "    }\n",
        "\n",
        "    set_strict_seed(CONFIG[\"seed\"])\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Device: {device}\")\n",
        "\n",
        "    full_data = load_mhealth_dataset(CONFIG[\"data_dir\"], CONFIG[\"TARGET_ACTIVITIES_MAP\"], CONFIG[\"COLUMN_NAMES\"])\n",
        "    if not full_data:\n",
        "        return\n",
        "\n",
        "    subjects = [f\"subject{i}\" for i in range(1, 11)]\n",
        "    loso_results = []\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"LOSO subject shift (TRAIN: window-proxy / TEST: windowing inference)\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    for fold_idx, test_subj in enumerate(subjects):\n",
        "        set_strict_seed(CONFIG[\"seed\"])\n",
        "\n",
        "        train_labels = [x for x in CONFIG[\"ALL_LABELS\"] if x[0] != test_subj]\n",
        "        test_labels = [x for x in CONFIG[\"ALL_LABELS\"] if x[0] == test_subj]\n",
        "\n",
        "        train_trials = prepare_trial_list(train_labels, full_data, CONFIG[\"TARGET_ACTIVITIES_MAP\"], CONFIG[\"ACT_FEATURE_MAP\"])\n",
        "        test_trials = prepare_trial_list(test_labels, full_data, CONFIG[\"TARGET_ACTIVITIES_MAP\"], CONFIG[\"ACT_FEATURE_MAP\"])\n",
        "\n",
        "        if not test_trials:\n",
        "            print(f\"[Skip] Fold {fold_idx + 1}: {test_subj} has no data.\")\n",
        "            continue\n",
        "\n",
        "        train_data = trial_list_to_windows(\n",
        "            train_trials,\n",
        "            fs=CONFIG[\"fs\"],\n",
        "            win_sec=CONFIG[\"win_sec\"],\n",
        "            stride_sec=CONFIG[\"stride_sec\"],\n",
        "            drop_last=CONFIG[\"drop_last\"]\n",
        "        )\n",
        "        test_data = test_trials\n",
        "\n",
        "        g = torch.Generator()\n",
        "        g.manual_seed(CONFIG[\"seed\"])\n",
        "\n",
        "        train_loader = DataLoader(\n",
        "            TrialDataset(train_data),\n",
        "            batch_size=CONFIG[\"batch_size\"],\n",
        "            shuffle=True,\n",
        "            collate_fn=collate_variable_length,\n",
        "            generator=g,\n",
        "            num_workers=0\n",
        "        )\n",
        "\n",
        "        input_ch = train_data[0][\"data\"].shape[1]\n",
        "        model = KAutoCountModel(\n",
        "            input_ch=input_ch,\n",
        "            hidden_dim=CONFIG[\"hidden_dim\"],\n",
        "            latent_dim=CONFIG[\"latent_dim\"],\n",
        "            K_max=CONFIG[\"K_max\"]\n",
        "        ).to(device)\n",
        "\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=CONFIG[\"lr\"])\n",
        "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.5)\n",
        "\n",
        "        for _epoch in range(CONFIG[\"epochs\"]):\n",
        "            _ = train_one_epoch(model, train_loader, optimizer, CONFIG, device)\n",
        "            scheduler.step()\n",
        "\n",
        "        model.eval()\n",
        "\n",
        "        fold_mae = 0.0\n",
        "        fold_res_str = \"\"\n",
        "        viz_payload = None\n",
        "\n",
        "        for item in test_data:\n",
        "            x_np = item[\"data\"]\n",
        "            raw_selected = item[\"raw_selected\"]\n",
        "\n",
        "            count_pred_win, _ = predict_count_by_windowing(\n",
        "                model,\n",
        "                x_np=x_np,\n",
        "                fs=CONFIG[\"fs\"],\n",
        "                win_sec=CONFIG[\"win_sec\"],\n",
        "                stride_sec=CONFIG[\"stride_sec\"],\n",
        "                device=device,\n",
        "                tau=CONFIG.get(\"tau\", 1.0),\n",
        "                batch_size=CONFIG.get(\"batch_size\", 64)\n",
        "            )\n",
        "\n",
        "            count_gt = float(item[\"count\"])\n",
        "            fold_mae += abs(count_pred_win - count_gt)\n",
        "            fold_res_str += f\"[Pred(win): {count_pred_win:.1f} / GT: {count_gt:.0f}]\"\n",
        "\n",
        "            # full forward for rep_rate(t) and z(t)\n",
        "            x_tensor = torch.tensor(x_np, dtype=torch.float32).transpose(0, 1).unsqueeze(0).to(device)\n",
        "            with torch.no_grad():\n",
        "                _, z, _, aux = model(x_tensor, mask=None, tau=CONFIG.get(\"tau\", 1.0))\n",
        "\n",
        "            rep_rate = aux[\"rep_rate_t\"].squeeze(0).detach().cpu().numpy()\n",
        "            z_np = z.squeeze(0).detach().cpu().numpy()\n",
        "\n",
        "            feats = CONFIG[\"ACT_FEATURE_MAP\"][6]\n",
        "            ix = feats.index(\"acc_ankle_x\")\n",
        "            iy = feats.index(\"acc_ankle_y\")\n",
        "            iz = feats.index(\"acc_ankle_z\")\n",
        "            raw_ankle_xyz = raw_selected[:, [ix, iy, iz]]\n",
        "\n",
        "            viz_payload = {\n",
        "                \"raw_ankle_xyz\": raw_ankle_xyz,\n",
        "                \"rep_rate\": rep_rate,\n",
        "                \"z_np\": z_np,\n",
        "                \"gt\": count_gt,\n",
        "                \"pred\": float(count_pred_win),\n",
        "                \"meta\": item[\"meta\"]\n",
        "            }\n",
        "\n",
        "        fold_mae /= len(test_data)\n",
        "        loso_results.append(fold_mae)\n",
        "        print(f\"Fold {fold_idx + 1:2d} | Test: {test_subj} | MAE: {fold_mae:.2f} | {fold_res_str}\")\n",
        "\n",
        "        if CONFIG[\"VIZ_ENABLE\"] and (test_subj == CONFIG[\"VIZ_TARGET_SUBJ\"]) and (viz_payload is not None):\n",
        "            out_dir = CONFIG[\"VIZ_OUT_DIR\"]\n",
        "            os.makedirs(out_dir, exist_ok=True)\n",
        "            out_png = os.path.join(out_dir, f\"qual_raw_rep_3dtraj_topKGT_{test_subj}.png\")\n",
        "\n",
        "            plot_raw_rep_and_3d_traj(\n",
        "                raw_ankle_acc_xyz=viz_payload[\"raw_ankle_xyz\"],\n",
        "                rep_rate_t=viz_payload[\"rep_rate\"],\n",
        "                Z_t=viz_payload[\"z_np\"],\n",
        "                fs=CONFIG[\"fs\"],\n",
        "                title=f\"Qualitative example ({viz_payload['meta']})\",\n",
        "                out_png=out_png,\n",
        "                gt_count=viz_payload[\"gt\"],\n",
        "                pred_count=viz_payload[\"pred\"],\n",
        "                smooth_w=CONFIG[\"VIZ_SMOOTH_W\"],\n",
        "                min_gap_sec=CONFIG[\"VIZ_MIN_GAP_SEC\"],\n",
        "                max_points_3d=CONFIG[\"VIZ_MAX_POINTS_3D\"],\n",
        "                # 3D 크게\n",
        "                fig_w=15.5,\n",
        "                fig_h=12.0,\n",
        "                h_ratios=(1.0, 1.0, 3.2),\n",
        "                cb_shrink=0.95,\n",
        "                cb_pad=0.03,\n",
        "            )\n",
        "\n",
        "    print(\"=\" * 70)\n",
        "    print(f\" >>> Final LOSO Result (Average MAE): {np.mean(loso_results):.3f}\")\n",
        "    print(f\" >>> Standard Deviation: {np.std(loso_results):.3f}\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    }
  ]
}