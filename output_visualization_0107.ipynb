{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# Count-only K-auto (Multi-event) + Windowing version\n",
        "#\n",
        "# ✅ Windowing added:\n",
        "# - TRAIN: trial -> sliding windows (window-level count = trial-average rate * window duration)\n",
        "# - TEST : trial 그대로 두고, windowing inference로 window rate 평균 -> 전체 count 예측\n",
        "# - k_hat / entropy / rep_rate / phase heatmap은 (표현학습 확인용) full-trial 1회 forward로 기록\n",
        "#\n",
        "# ✅ Requested changes applied:\n",
        "# (1) \"Phase(k_hat) heatmap\" uses EXACT style/design of plot_phase_heatmap_and_dominant (provided code)\n",
        "# (2) PCA2D plot: ❌ REMOVED (as requested)\n",
        "# (3) All plots saved as PNG (dpi=600), no plt.show()\n",
        "#\n",
        "# ✅ Added in this request:\n",
        "# - (3) Raw signal + predicted rep_rate overlay (short segment) plot added\n",
        "#\n",
        "# ✅ Updated in this request:\n",
        "# - Use EXACT titles (no extra activity info inside the title):\n",
        "#   1) Phase Usage Heatmap for K-auto Decomposition\n",
        "#   2) 3D PCA Projection of the Latent Trajectory\n",
        "#   3) Raw Signal and Predicted rep_rate Overlay (Short Segment)\n",
        "# - REMOVE mean plot code (time-normalized averaging) and its calls.\n",
        "#\n",
        "# ✅ Plot tweaks in this request:\n",
        "# 1) Phase heatmap colorbar: match plot height, cleaner sizing, larger tick labels\n",
        "# 2) PCA3D colorbar: no overlap with PC3 label + better view/aspect to reveal shape\n",
        "# 3) Raw overlay: slightly different colors, smaller legend, label text updates\n",
        "# =========================\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import random\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.ndimage import gaussian_filter1d\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 0) IO helpers\n",
        "# ---------------------------------------------------------------------\n",
        "def _safe_filename(s: str) -> str:\n",
        "    s = str(s)\n",
        "    s = re.sub(r\"[^\\w\\-_\\. ]\", \"_\", s)\n",
        "    s = s.strip().replace(\" \", \"_\")\n",
        "    return s[:200] if len(s) > 200 else s\n",
        "\n",
        "\n",
        "def _ensure_dir(p: str):\n",
        "    os.makedirs(p, exist_ok=True)\n",
        "    return p\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 1) Strict Seeding\n",
        "# ---------------------------------------------------------------------\n",
        "def set_strict_seed(seed: int):\n",
        "    random.seed(seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 2) Data Loading\n",
        "# ---------------------------------------------------------------------\n",
        "def load_mhealth_dataset(data_dir, target_activities_map, column_names):\n",
        "    full_dataset = {}\n",
        "    file_list = sorted(glob.glob(os.path.join(data_dir, \"mHealth_subject*.log\")))\n",
        "\n",
        "    if not file_list:\n",
        "        print(f\"[Warning] No mHealth logs found in {data_dir}\")\n",
        "        return {}\n",
        "\n",
        "    print(f\"Loading {len(file_list)} subjects from {data_dir}...\")\n",
        "\n",
        "    for file_path in file_list:\n",
        "        file_name = os.path.basename(file_path)\n",
        "        subj_part = file_name.split('.')[0]\n",
        "        try:\n",
        "            subj_id_num = int(''.join(filter(str.isdigit, subj_part)))\n",
        "            subj_key = f\"subject{subj_id_num}\"\n",
        "        except:\n",
        "            subj_key = subj_part\n",
        "\n",
        "        try:\n",
        "            df = pd.read_csv(file_path, sep=\"\\t\", header=None)\n",
        "            df = df.iloc[:, :len(column_names)]\n",
        "            df.columns = column_names\n",
        "\n",
        "            subj_data = {}\n",
        "            for label_code, activity_name in target_activities_map.items():\n",
        "                activity_df = df[df['activity_id'] == label_code].copy()\n",
        "                if not activity_df.empty:\n",
        "                    subj_data[activity_name] = activity_df.drop(columns=['activity_id'])\n",
        "\n",
        "            full_dataset[subj_key] = subj_data\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {file_name}: {e}\")\n",
        "            pass\n",
        "\n",
        "    return full_dataset\n",
        "\n",
        "\n",
        "def prepare_trial_list(label_config, full_data, target_map, feature_map):\n",
        "    trial_list = []\n",
        "    for subj, act_id, gt_count in label_config:\n",
        "        act_name = target_map.get(act_id)\n",
        "        feats = feature_map.get(act_id)\n",
        "\n",
        "        if subj in full_data and act_name in full_data[subj]:\n",
        "            raw_df = full_data[subj][act_name][feats]\n",
        "            raw_np = raw_df.values.astype(np.float32)\n",
        "\n",
        "            # Z-score 정규화\n",
        "            mean = raw_np.mean(axis=0)\n",
        "            std = raw_np.std(axis=0) + 1e-6\n",
        "            norm_np = (raw_np - mean) / std\n",
        "\n",
        "            trial_list.append({\n",
        "                'data': norm_np,              # (T, C)\n",
        "                'count': float(gt_count),      # trial total count\n",
        "                'meta': f\"{subj}_{act_name}\"\n",
        "            })\n",
        "        else:\n",
        "            print(f\"[Skip] Missing data for {subj} - {act_name}\")\n",
        "\n",
        "    return trial_list\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 2.5) ✅ Windowing (added)\n",
        "# ---------------------------------------------------------------------\n",
        "def trial_list_to_windows(trial_list, fs, win_sec=8.0, stride_sec=4.0, drop_last=True):\n",
        "    \"\"\"\n",
        "    TRAIN 전용: trial -> sliding windows 확장\n",
        "    window 라벨은 trial-level 평균 rate로부터 생성:\n",
        "      rate_trial = count_total / total_duration\n",
        "      count_window = rate_trial * window_duration\n",
        "    \"\"\"\n",
        "    win_len = int(round(win_sec * fs))\n",
        "    stride = int(round(stride_sec * fs))\n",
        "    assert win_len > 0 and stride > 0\n",
        "\n",
        "    windows = []\n",
        "    for item in trial_list:\n",
        "        x = item[\"data\"]  # (T,C)\n",
        "        T = x.shape[0]\n",
        "        total_count = float(item[\"count\"])\n",
        "        meta = item[\"meta\"]\n",
        "\n",
        "        total_dur = max(T / float(fs), 1e-6)\n",
        "        rate_trial = total_count / total_dur  # reps/s\n",
        "\n",
        "        if T < win_len:\n",
        "            win_dur = T / float(fs)\n",
        "            windows.append({\n",
        "                \"data\": x,\n",
        "                \"count\": rate_trial * win_dur,\n",
        "                \"meta\": f\"{meta}__win[0:{T}]\",\n",
        "                \"parent_meta\": meta,\n",
        "                \"parent_T\": T,\n",
        "                \"win_start\": 0,\n",
        "                \"win_end\": T,\n",
        "            })\n",
        "            continue\n",
        "\n",
        "        last_start = T - win_len\n",
        "        starts = list(range(0, last_start + 1, stride))\n",
        "\n",
        "        for st in starts:\n",
        "            ed = st + win_len\n",
        "            win_dur = win_len / float(fs)\n",
        "            windows.append({\n",
        "                \"data\": x[st:ed],\n",
        "                \"count\": rate_trial * win_dur,\n",
        "                \"meta\": f\"{meta}__win[{st}:{ed}]\",\n",
        "                \"parent_meta\": meta,\n",
        "                \"parent_T\": T,\n",
        "                \"win_start\": st,\n",
        "                \"win_end\": ed,\n",
        "            })\n",
        "\n",
        "        if not drop_last:\n",
        "            last_st = starts[-1] + stride\n",
        "            if last_st < T:\n",
        "                ed = T\n",
        "                win_dur = (ed - last_st) / float(fs)\n",
        "                windows.append({\n",
        "                    \"data\": x[last_st:ed],\n",
        "                    \"count\": rate_trial * win_dur,\n",
        "                    \"meta\": f\"{meta}__win[{last_st}:{ed}]\",\n",
        "                    \"parent_meta\": meta,\n",
        "                    \"parent_T\": T,\n",
        "                    \"win_start\": last_st,\n",
        "                    \"win_end\": ed,\n",
        "                })\n",
        "\n",
        "    return windows\n",
        "\n",
        "\n",
        "def predict_count_by_windowing(model, x_np, fs, win_sec, stride_sec, device, tau=1.0, batch_size=64):\n",
        "    \"\"\"\n",
        "    TEST 전용: trial -> sliding windows inference -> window rate 평균 -> total count\n",
        "    x_np: (T,C) numpy (이미 정규화된 상태)\n",
        "    return: pred_count(float), window_rates(np.ndarray)\n",
        "    \"\"\"\n",
        "    win_len = int(round(win_sec * fs))\n",
        "    stride = int(round(stride_sec * fs))\n",
        "    T = x_np.shape[0]\n",
        "    total_dur = T / float(fs)\n",
        "\n",
        "    # short trial -> 1회 forward\n",
        "    if T <= win_len:\n",
        "        x_tensor = torch.tensor(x_np, dtype=torch.float32).transpose(0, 1).unsqueeze(0).to(device)  # (1,C,T)\n",
        "        with torch.no_grad():\n",
        "            rate_hat, _, _, _ = model(x_tensor, mask=None, tau=tau)\n",
        "        pred_count = float(rate_hat.item() * total_dur)\n",
        "        return pred_count, np.array([float(rate_hat.item())], dtype=np.float32)\n",
        "\n",
        "    starts = list(range(0, T - win_len + 1, stride))\n",
        "    windows = np.stack([x_np[st:st + win_len] for st in starts], axis=0)  # (N, win_len, C)\n",
        "\n",
        "    xw = torch.tensor(windows, dtype=torch.float32).permute(0, 2, 1).to(device)  # (N, C, win_len)\n",
        "\n",
        "    rates = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, xw.shape[0], batch_size):\n",
        "            xb = xw[i:i + batch_size]\n",
        "            r_hat, _, _, _ = model(xb, mask=None, tau=tau)  # (B,)\n",
        "            rates.append(r_hat.detach().cpu().numpy())\n",
        "\n",
        "    rates = np.concatenate(rates, axis=0)  # (N,)\n",
        "    rate_mean = float(rates.mean())\n",
        "    pred_count = rate_mean * total_dur\n",
        "    return float(pred_count), rates\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 2.8) Dataset / Collate\n",
        "# ---------------------------------------------------------------------\n",
        "class TrialDataset(Dataset):\n",
        "    def __init__(self, trial_list):\n",
        "        self.trials = trial_list\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.trials)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.trials[idx]\n",
        "        data = torch.tensor(item['data'], dtype=torch.float32).transpose(0, 1)  # (C, T)\n",
        "        count = torch.tensor(item['count'], dtype=torch.float32)\n",
        "        return data, count, item['meta']\n",
        "\n",
        "\n",
        "def collate_variable_length(batch):\n",
        "    max_len = max([x[0].shape[1] for x in batch])\n",
        "    C = batch[0][0].shape[0]\n",
        "\n",
        "    padded_data, masks, counts, metas, lengths = [], [], [], [], []\n",
        "    for data, count, meta in batch:\n",
        "        T = data.shape[1]\n",
        "        lengths.append(T)\n",
        "\n",
        "        pad_size = max_len - T\n",
        "        if pad_size > 0:\n",
        "            pad = torch.zeros(C, pad_size)\n",
        "            d_padded = torch.cat([data, pad], dim=1)\n",
        "            mask = torch.cat([torch.ones(T), torch.zeros(pad_size)], dim=0)\n",
        "        else:\n",
        "            d_padded = data\n",
        "            mask = torch.ones(T)\n",
        "\n",
        "        padded_data.append(d_padded)\n",
        "        masks.append(mask)\n",
        "        counts.append(count)\n",
        "        metas.append(meta)\n",
        "\n",
        "    return {\n",
        "        \"data\": torch.stack(padded_data),         # (B, C, T_max)\n",
        "        \"mask\": torch.stack(masks),               # (B, T_max)\n",
        "        \"count\": torch.stack(counts),             # (B,)\n",
        "        \"length\": torch.tensor(lengths, dtype=torch.float32),  # (B,)\n",
        "        \"meta\": metas\n",
        "    }\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 3) Model\n",
        "# ---------------------------------------------------------------------\n",
        "class ManifoldEncoder(nn.Module):\n",
        "    def __init__(self, input_ch, hidden_dim=128, latent_dim=16):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv1d(input_ch, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, latent_dim, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        z = self.net(x)            # (B, D, T)\n",
        "        z = z.transpose(1, 2)      # (B, T, D)\n",
        "        return z\n",
        "\n",
        "\n",
        "class ManifoldDecoder(nn.Module):\n",
        "    def __init__(self, latent_dim, hidden_dim, out_ch):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv1d(latent_dim, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, out_ch, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        zt = z.transpose(1, 2)     # (B, D, T)\n",
        "        x_hat = self.net(zt)       # (B, C, T)\n",
        "        return x_hat\n",
        "\n",
        "\n",
        "class MultiRateHead(nn.Module):\n",
        "    def __init__(self, latent_dim=16, hidden=64, K_max=6):\n",
        "        super().__init__()\n",
        "        self.K_max = K_max\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(latent_dim, hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden, 1 + K_max)  # [amp_logit | phase_logits...]\n",
        "        )\n",
        "\n",
        "    def forward(self, z, tau=1.0):\n",
        "        out = self.net(z)                     # (B,T,1+K)\n",
        "        amp = F.softplus(out[..., 0])         # (B,T) >=0\n",
        "        phase_logits = out[..., 1:]           # (B,T,K)\n",
        "        phase = F.softmax(phase_logits / tau, dim=-1)  # (B,T,K), sum=1\n",
        "        return amp, phase, phase_logits\n",
        "\n",
        "\n",
        "class KAutoCountModel(nn.Module):\n",
        "    \"\"\"\n",
        "    - outputs K_max micro-event rates r_k(t)\n",
        "    - predicts k_hat (>=1) per sample (via effective K on time-avg phase)\n",
        "    - rep_rate(t) = micro_rate(t) / k_hat\n",
        "    \"\"\"\n",
        "    def __init__(self, input_ch, hidden_dim=128, latent_dim=16, K_max=6, k_hidden=64):\n",
        "        super().__init__()\n",
        "        self.encoder = ManifoldEncoder(input_ch, hidden_dim, latent_dim)\n",
        "        self.decoder = ManifoldDecoder(latent_dim, hidden_dim, input_ch)\n",
        "        self.rate_head = MultiRateHead(latent_dim, hidden=hidden_dim, K_max=K_max)\n",
        "\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, (nn.Conv1d, nn.Linear)):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            b = self.rate_head.net[-1].bias\n",
        "            b.zero_()\n",
        "            b[0].fill_(-2.0)  # amp logit bias만 -2\n",
        "\n",
        "    @staticmethod\n",
        "    def _masked_mean_time(x, mask=None, eps=1e-6):\n",
        "        if mask is None:\n",
        "            return x.mean(dim=1)\n",
        "        if x.dim() == 2:\n",
        "            m = mask.to(dtype=x.dtype, device=x.device)\n",
        "            return (x * m).sum(dim=1) / (m.sum(dim=1) + eps)\n",
        "        elif x.dim() == 3:\n",
        "            m = mask.to(dtype=x.dtype, device=x.device).unsqueeze(-1)\n",
        "            return (x * m).sum(dim=1) / (m.sum(dim=1) + eps)\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported dim for masked mean: {x.dim()}\")\n",
        "\n",
        "    def forward(self, x, mask=None, tau=1.0):\n",
        "        z = self.encoder(x)              # (B,T,D)\n",
        "        x_hat = self.decoder(z)          # (B,C,T)\n",
        "\n",
        "        amp_t, phase_p, phase_logits = self.rate_head(z, tau=tau)\n",
        "        rates_k_t = amp_t.unsqueeze(-1) * phase_p  # (B,T,K)\n",
        "\n",
        "        micro_rate_t = amp_t  # (B,T)\n",
        "\n",
        "        p_bar = self._masked_mean_time(phase_p, mask)           # (B,K)\n",
        "        k_hat = 1.0 / (p_bar.pow(2).sum(dim=1) + 1e-6)          # (B,) in [1,K]\n",
        "\n",
        "        rep_rate_t = micro_rate_t / (k_hat.unsqueeze(1) + 1e-6) # (B,T)\n",
        "        if mask is not None:\n",
        "            rep_rate_t = rep_rate_t * mask\n",
        "\n",
        "        if mask is None:\n",
        "            avg_rep_rate = rep_rate_t.mean(dim=1)\n",
        "        else:\n",
        "            avg_rep_rate = (rep_rate_t * mask).sum(dim=1) / (mask.sum(dim=1) + 1e-6)\n",
        "\n",
        "        aux = {\n",
        "            \"rates_k_t\": rates_k_t,\n",
        "            \"phase_p\": phase_p,\n",
        "            \"phase_logits\": phase_logits,\n",
        "            \"micro_rate_t\": micro_rate_t,\n",
        "            \"rep_rate_t\": rep_rate_t,\n",
        "            \"k_hat\": k_hat,\n",
        "        }\n",
        "        return avg_rep_rate, z, x_hat, aux\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 4) Loss utils\n",
        "# ---------------------------------------------------------------------\n",
        "def masked_recon_mse(x_hat, x, mask, eps=1e-6):\n",
        "    mask = mask.to(dtype=x.dtype, device=x.device)\n",
        "    mask_bc = mask.unsqueeze(1)              # (B,1,T)\n",
        "    se = (x_hat - x) ** 2                    # (B,C,T)\n",
        "    se = se * mask_bc\n",
        "    denom = (mask.sum() * x.shape[1]) + eps\n",
        "    return se.sum() / denom\n",
        "\n",
        "\n",
        "def temporal_smoothness(v, mask=None, eps=1e-6):\n",
        "    dv = torch.abs(v[:, 1:] - v[:, :-1])  # (B,T-1)\n",
        "    if mask is None:\n",
        "        return dv.mean()\n",
        "    m = mask[:, 1:] * mask[:, :-1]\n",
        "    m = m.to(dtype=dv.dtype, device=dv.device)\n",
        "    return (dv * m).sum() / (m.sum() + eps)\n",
        "\n",
        "\n",
        "def phase_entropy_loss(phase_p, mask=None, eps=1e-8):\n",
        "    ent = -(phase_p * (phase_p + eps).log()).sum(dim=-1)  # (B,T)\n",
        "    if mask is None:\n",
        "        return ent.mean()\n",
        "    ent = ent * mask\n",
        "    return ent.sum() / (mask.sum() + eps)\n",
        "\n",
        "\n",
        "def effK_usage_loss(phase_p, mask=None, eps=1e-6):\n",
        "    if mask is None:\n",
        "        p_bar = phase_p.mean(dim=1)  # (B,K)\n",
        "    else:\n",
        "        m = mask.to(dtype=phase_p.dtype, device=phase_p.device).unsqueeze(-1)  # (B,T,1)\n",
        "        p_bar = (phase_p * m).sum(dim=1) / (m.sum(dim=1) + eps)\n",
        "\n",
        "    effK = 1.0 / (p_bar.pow(2).sum(dim=1) + eps)\n",
        "    return effK.mean(), effK.detach()\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 5) Train\n",
        "# ---------------------------------------------------------------------\n",
        "def train_one_epoch(model, loader, optimizer, config, device):\n",
        "    model.train()\n",
        "    stats = {k: 0.0 for k in [\n",
        "        'loss', 'loss_rate', 'loss_recon', 'loss_smooth', 'loss_phase_ent', 'loss_effk',\n",
        "        'mae_count'\n",
        "    ]}\n",
        "\n",
        "    fs = config[\"fs\"]\n",
        "    tau = config.get(\"tau\", 1.0)\n",
        "\n",
        "    lam_recon = config.get(\"lambda_recon\", 1.0)\n",
        "    lam_smooth = config.get(\"lambda_smooth\", 0.05)\n",
        "    lam_phase_ent = config.get(\"lambda_phase_ent\", 0.01)\n",
        "    lam_effk = config.get(\"lambda_effk\", 0.005)\n",
        "\n",
        "    for batch in loader:\n",
        "        x = batch[\"data\"].to(device)         # (B,C,T)\n",
        "        mask = batch[\"mask\"].to(device)      # (B,T)\n",
        "        y_count = batch[\"count\"].to(device)  # (B,)\n",
        "        length = batch[\"length\"].to(device)  # (B,)\n",
        "\n",
        "        duration = torch.clamp(length / fs, min=1e-6)  # sec\n",
        "        y_rate = y_count / duration                    # reps/s\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        rate_hat, z, x_hat, aux = model(x, mask, tau=tau)\n",
        "\n",
        "        loss_rate = F.mse_loss(rate_hat, y_rate)\n",
        "        loss_recon = masked_recon_mse(x_hat, x, mask)\n",
        "        loss_smooth = temporal_smoothness(aux[\"rep_rate_t\"], mask)\n",
        "        loss_phase_ent = phase_entropy_loss(aux[\"phase_p\"], mask)\n",
        "        loss_effk, _ = effK_usage_loss(aux[\"phase_p\"], mask)\n",
        "\n",
        "        loss = (loss_rate\n",
        "                + lam_recon * loss_recon\n",
        "                + lam_smooth * loss_smooth\n",
        "                + lam_phase_ent * loss_phase_ent\n",
        "                + lam_effk * loss_effk)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        count_hat = rate_hat * duration\n",
        "        stats['loss'] += loss.item()\n",
        "        stats['loss_rate'] += loss_rate.item()\n",
        "        stats['loss_recon'] += loss_recon.item()\n",
        "        stats['loss_smooth'] += loss_smooth.item()\n",
        "        stats['loss_phase_ent'] += loss_phase_ent.item()\n",
        "        stats['loss_effk'] += loss_effk.item()\n",
        "        stats['mae_count'] += torch.abs(count_hat - y_count).mean().item()\n",
        "\n",
        "    n = len(loader)\n",
        "    return {k: v / n for k, v in stats.items()}\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 6) Visualization helpers\n",
        "# ---------------------------------------------------------------------\n",
        "def _smooth_1d(y, sigma=2.0):\n",
        "    y = np.asarray(y, dtype=np.float32)\n",
        "    return gaussian_filter1d(y, sigma=sigma)\n",
        "\n",
        "\n",
        "def compute_phase_entropy_mean(phase_p_np, eps=1e-8):\n",
        "    phase_p_np = np.asarray(phase_p_np, dtype=np.float32)\n",
        "    ent_t = -(phase_p_np * np.log(phase_p_np + eps)).sum(axis=1)  # (T,)\n",
        "    return float(ent_t.mean())\n",
        "\n",
        "\n",
        "def downsample_time_axis(arr, max_T=2000):\n",
        "    T = arr.shape[0]\n",
        "    if T <= max_T:\n",
        "        idx = np.arange(T)\n",
        "        return arr, idx\n",
        "    idx = np.linspace(0, T - 1, max_T).astype(int)\n",
        "    return arr[idx], idx\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# PCA utils (no sklearn)\n",
        "# -----------------------------\n",
        "def _fit_pca_basis(X, n_comp=3):\n",
        "    X = np.asarray(X, dtype=np.float32)\n",
        "    mu = X.mean(axis=0, keepdims=True)\n",
        "    Xc = X - mu\n",
        "    _, _, Vt = np.linalg.svd(Xc, full_matrices=False)\n",
        "    W = Vt[:n_comp].T\n",
        "    return mu.squeeze(0), W\n",
        "\n",
        "\n",
        "def _pca_project(X, mu, W):\n",
        "    X = np.asarray(X, dtype=np.float32)\n",
        "    return (X - mu[None, :]) @ W\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# ✅ (1) Phase heatmap + PNG saving  (cbar aligned + larger tick labels)\n",
        "# ---------------------------------------------------------------------\n",
        "def plot_phase_heatmap_and_dominant(\n",
        "    phase_p_np,\n",
        "    fs,\n",
        "    title=\"phase_p heatmap + dominant phase\",\n",
        "    max_T=2000,\n",
        "    save_path=None,\n",
        "    dpi=600\n",
        "):\n",
        "    from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
        "\n",
        "    phase_p_np = np.asarray(phase_p_np, dtype=np.float32)\n",
        "    assert phase_p_np.ndim == 2, f\"phase_p_np must be (T,K), got {phase_p_np.shape}\"\n",
        "\n",
        "    phase_ds, idx = downsample_time_axis(phase_p_np, max_T=max_T)  # (T',K)\n",
        "    Tds, K = phase_ds.shape\n",
        "    t_sec = idx / float(fs)\n",
        "\n",
        "    fig = plt.figure(figsize=(12.5, 5))\n",
        "    cmap = sns.color_palette(\"magma\", as_cmap=True)\n",
        "\n",
        "    ax0 = fig.add_subplot(1, 1, 1)\n",
        "    im = ax0.imshow(\n",
        "        phase_ds.T,                     # (K, T')\n",
        "        aspect=\"auto\",\n",
        "        origin=\"lower\",\n",
        "        interpolation=\"nearest\",\n",
        "        extent=[t_sec[0], t_sec[-1], 0, K],\n",
        "        cmap=cmap\n",
        "    )\n",
        "    if title:\n",
        "        ax0.set_title(title, fontsize=24, pad=10)\n",
        "    ax0.set_ylabel(\"Phase k\", fontsize=26)\n",
        "    ax0.set_xlabel(\"Time (sec)\", fontsize=26)\n",
        "    ax0.tick_params(axis=\"both\", labelsize=18)\n",
        "\n",
        "    # ✅ colorbar: same height as axes (clean) + larger ticks\n",
        "    divider = make_axes_locatable(ax0)\n",
        "    cax = divider.append_axes(\"right\", size=\"2.6%\", pad=0.15)\n",
        "    cbar = fig.colorbar(im, cax=cax)\n",
        "    cbar.set_label(r\"$\\hat{p}_t(k)$\", fontsize=26, labelpad=10)\n",
        "    cbar.ax.tick_params(labelsize=18)\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    if save_path is not None:\n",
        "        _ensure_dir(os.path.dirname(save_path))\n",
        "        fig.savefig(save_path, dpi=dpi, bbox_inches=\"tight\")\n",
        "    plt.close(fig)\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# ✅ (2) PCA3D + PNG saving  (cbar no-overlap + better view/aspect)\n",
        "# ---------------------------------------------------------------------\n",
        "def plot_pca3d_z(z_np, fs, title=\"PCA3D of z(t)\", max_T=2000, save_path=None, dpi=600):\n",
        "    from mpl_toolkits.mplot3d import Axes3D  # noqa: F401\n",
        "\n",
        "    z_np = np.asarray(z_np, dtype=np.float32)\n",
        "    z_ds, idx = downsample_time_axis(z_np, max_T=max_T)  # (T',D)\n",
        "    t_sec = idx / float(fs)\n",
        "\n",
        "    mu, W = _fit_pca_basis(z_ds, n_comp=3)\n",
        "    Z3 = _pca_project(z_ds, mu, W)  # (T',3)\n",
        "\n",
        "    # ✅ (핵심) 오른쪽에 colorbar 자리 확보하려고 가로폭 약간 넓히고 right margin 확보\n",
        "    fig = plt.figure(figsize=(6.6, 5.2))\n",
        "    ax = fig.add_subplot(111, projection=\"3d\")\n",
        "\n",
        "    sc = ax.scatter(Z3[:, 0], Z3[:, 1], Z3[:, 2], c=t_sec, s=12, alpha=0.9)\n",
        "    ax.plot(Z3[:, 0], Z3[:, 1], Z3[:, 2], color=\"k\", alpha=0.15, linewidth=0.8)\n",
        "\n",
        "    ax.scatter(Z3[0, 0],  Z3[0, 1],  Z3[0, 2],  marker=\"o\", s=70, edgecolor=\"k\", linewidth=0.8)\n",
        "    ax.scatter(Z3[-1, 0], Z3[-1, 1], Z3[-1, 2], marker=\"^\", s=90, edgecolor=\"k\", linewidth=0.8)\n",
        "\n",
        "    if title:\n",
        "        ax.set_title(title, fontsize=14, pad=10)\n",
        "\n",
        "    ax.set_xlabel(\"PC1\", fontsize=10)\n",
        "    ax.set_ylabel(\"PC2\", fontsize=10)\n",
        "\n",
        "    # ✅ (겹침 방지 1) zlabel을 axis쪽으로 살짝 “당기기” (labelpad 음수)\n",
        "    ax.set_zlabel(\"PC3\", fontsize=10, labelpad=-2)\n",
        "\n",
        "    # ✅ (모양이 더 잘 보이게) view 각도 고정 (원하면 값만 바꿔서 취향 맞추면 됨)\n",
        "    ax.view_init(elev=18, azim=-62)\n",
        "\n",
        "    ax.tick_params(axis='x', which='major', pad=-2)\n",
        "    ax.tick_params(axis='y', which='major', pad=-2)\n",
        "    ax.tick_params(axis='z', which='major', pad=-2)\n",
        "\n",
        "    ax.grid(False)\n",
        "\n",
        "    # ✅ (겹침 방지 2: 결정타) colorbar를 별도 축(cax)에 배치해서 PC3 라벨 영역과 분리\n",
        "    #    right=0.80 → 오른쪽 20%를 비워두고, 그 공간에 cbar를 강제 배치\n",
        "    fig.subplots_adjust(right=0.80)\n",
        "    cax = fig.add_axes([0.84, 0.15, 0.03, 0.70])  # [left, bottom, width, height] in figure coords\n",
        "    cbar = fig.colorbar(sc, cax=cax)\n",
        "    cbar.ax.tick_params(labelsize=9)\n",
        "    cbar.set_label(\"time (sec)\", fontsize=12)\n",
        "\n",
        "    if save_path is not None:\n",
        "        _ensure_dir(os.path.dirname(save_path))\n",
        "        fig.savefig(save_path, dpi=dpi, bbox_inches=\"tight\")\n",
        "    plt.close(fig)\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# ✅ (3) NEW: Raw signal + predicted rep_rate overlay (short segment)\n",
        "#     - colors slightly different\n",
        "#     - smaller legend\n",
        "#     - label text updates\n",
        "# ---------------------------------------------------------------------\n",
        "def plot_raw_and_rep_rate_overlay_short_segment(\n",
        "    x_np,                 # (T,C) z-scored input\n",
        "    rep_rate,             # (T,) predicted rep_rate_t\n",
        "    fs,\n",
        "    title=\"\",\n",
        "    seg_sec=6.0,\n",
        "    channel_idx=0,\n",
        "    select_mode=\"max_rate\",   # \"max_rate\" or \"center\"\n",
        "    # ✅ smoothing controls\n",
        "    smooth_sigma_raw=0.8,     # raw는 약하게\n",
        "    smooth_sigma_rate=2.0,    # rep_rate는 조금 더\n",
        "    clip_rate_nonneg=True,\n",
        "    save_path=None,\n",
        "    dpi=600\n",
        "):\n",
        "    x_np = np.asarray(x_np, dtype=np.float32)\n",
        "    rep_rate = np.asarray(rep_rate, dtype=np.float32)\n",
        "\n",
        "    T = x_np.shape[0]\n",
        "    C = x_np.shape[1]\n",
        "    ch = int(np.clip(channel_idx, 0, C - 1))\n",
        "\n",
        "    L = int(round(seg_sec * fs))\n",
        "    L = max(10, min(L, T))\n",
        "\n",
        "    if T <= L:\n",
        "        st, ed = 0, T\n",
        "    else:\n",
        "        if select_mode == \"center\":\n",
        "            st = max(0, (T - L) // 2)\n",
        "            ed = st + L\n",
        "        else:\n",
        "            # segment where rep_rate is high (rolling mean peak)\n",
        "            rr_for_pick = gaussian_filter1d(rep_rate, sigma=max(0.5, smooth_sigma_rate))\n",
        "            kernel = np.ones(L, dtype=np.float32) / float(L)\n",
        "            roll = np.convolve(rr_for_pick, kernel, mode=\"valid\")\n",
        "            best = int(np.argmax(roll))\n",
        "            st = best\n",
        "            ed = st + L\n",
        "\n",
        "    raw_seg = x_np[st:ed, ch]\n",
        "    rr_seg = rep_rate[st:ed]\n",
        "\n",
        "    # ✅ smoothing (visual only)\n",
        "    if smooth_sigma_raw and smooth_sigma_raw > 0:\n",
        "        raw_seg_plot = gaussian_filter1d(raw_seg, sigma=smooth_sigma_raw)\n",
        "    else:\n",
        "        raw_seg_plot = raw_seg\n",
        "\n",
        "    if smooth_sigma_rate and smooth_sigma_rate > 0:\n",
        "        rr_seg_plot = gaussian_filter1d(rr_seg, sigma=smooth_sigma_rate)\n",
        "    else:\n",
        "        rr_seg_plot = rr_seg\n",
        "\n",
        "    if clip_rate_nonneg:\n",
        "        rr_seg_plot = np.clip(rr_seg_plot, 0.0, None)\n",
        "\n",
        "    t = np.arange(st, ed, dtype=np.float32) / float(fs)\n",
        "\n",
        "    fig = plt.figure(figsize=(11.5, 5))\n",
        "    ax1 = fig.add_subplot(1, 1, 1)\n",
        "\n",
        "    # ✅ raw: 조금 얇고 부드럽게\n",
        "    ax1.plot(t, raw_seg_plot, linewidth=1.85, alpha=0.90, color=\"tab:blue\", label=\"Raw (z-scored)\")\n",
        "    ax1.set_xlabel(\"Time (sec)\", fontsize=19)\n",
        "    ax1.set_ylabel(\"Raw signal\", fontsize=19)\n",
        "    ax1.grid(True, linestyle=\"--\", alpha=0.25)\n",
        "\n",
        "    ax2 = ax1.twinx()\n",
        "\n",
        "    # ✅ rep_rate: 더 두껍고 부드럽게\n",
        "    ax2.plot(t, rr_seg_plot, linewidth=2.4, alpha=0.95, color=\"tab:orange\", label=\"Predicted (Rep rate)\")\n",
        "    ax2.set_ylabel(\"Rep rate (reps/s)\", fontsize=19)\n",
        "\n",
        "    if title:\n",
        "        ax1.set_title(title, fontsize=19, pad=6)\n",
        "\n",
        "    # legend (merge) - 살짝 작게\n",
        "    h1, l1 = ax1.get_legend_handles_labels()\n",
        "    h2, l2 = ax2.get_legend_handles_labels()\n",
        "    ax1.legend(h1 + h2, l1 + l2, loc=\"upper right\", fontsize=15, frameon=True)\n",
        "\n",
        "    ax1.tick_params(axis=\"both\", labelsize=14)\n",
        "    ax2.tick_params(axis=\"y\", labelsize=14)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    if save_path is not None:\n",
        "        _ensure_dir(os.path.dirname(save_path))\n",
        "        fig.savefig(save_path, dpi=dpi, bbox_inches=\"tight\")\n",
        "    plt.close(fig)\n",
        "\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 7) Main (LOSO) - ALL activities + requested plots (PNG export)\n",
        "# ---------------------------------------------------------------------\n",
        "def main():\n",
        "    CONFIG = {\n",
        "        \"seed\": 42,\n",
        "        \"data_dir\": \"/content/drive/MyDrive/Colab Notebooks/HAR_data/MHEALTHDATASET\",\n",
        "\n",
        "        \"out_dir\": \"./outputs_loso_activity_plots\",\n",
        "\n",
        "        \"COLUMN_NAMES\": [\n",
        "            'acc_chest_x', 'acc_chest_y', 'acc_chest_z',\n",
        "            'ecg_1', 'ecg_2',\n",
        "            'acc_ankle_x', 'acc_ankle_y', 'acc_ankle_z',\n",
        "            'gyro_ankle_x', 'gyro_ankle_y', 'gyro_ankle_z',\n",
        "            'mag_ankle_x', 'mag_ankle_y', 'mag_ankle_z',\n",
        "            'acc_arm_x', 'acc_arm_y', 'acc_arm_z',\n",
        "            'gyro_arm_x', 'gyro_arm_y', 'gyro_arm_z',\n",
        "            'mag_arm_x', 'mag_arm_y', 'mag_arm_z',\n",
        "            'activity_id'\n",
        "        ],\n",
        "\n",
        "        \"TARGET_ACTIVITIES_MAP\": {\n",
        "            8:  'Knees bending',\n",
        "            10:  'Jogging',\n",
        "            12: 'Jump front & back',\n",
        "        },\n",
        "\n",
        "        \"ACT_FEATURE_MAP\": {\n",
        "            8:  ['acc_chest_x','acc_chest_y','acc_chest_z','acc_ankle_x','acc_ankle_y','acc_ankle_z',\n",
        "                 'gyro_ankle_x','gyro_ankle_y','gyro_ankle_z','acc_arm_x','acc_arm_y','acc_arm_z',\n",
        "                 'gyro_arm_x','gyro_arm_y','gyro_arm_z'],\n",
        "            10:  ['acc_chest_x','acc_chest_y','acc_chest_z','acc_ankle_x','acc_ankle_y','acc_ankle_z',\n",
        "                 'gyro_ankle_x','gyro_ankle_y','gyro_ankle_z','acc_arm_x','acc_arm_y','acc_arm_z',\n",
        "                 'gyro_arm_x','gyro_arm_y','gyro_arm_z'],\n",
        "            12: ['acc_chest_x','acc_chest_y','acc_chest_z','acc_ankle_x','acc_ankle_y','acc_ankle_z',\n",
        "                 'gyro_ankle_x','gyro_ankle_y','gyro_ankle_z','acc_arm_x','acc_arm_y','acc_arm_z',\n",
        "                 'gyro_arm_x','gyro_arm_y','gyro_arm_z'],\n",
        "        },\n",
        "\n",
        "        # Training Params\n",
        "        \"epochs\": 100,\n",
        "        \"lr\": 5e-4,\n",
        "        \"batch_size\": 64,\n",
        "        \"fs\": 50,\n",
        "\n",
        "        # ✅ Windowing Params\n",
        "        \"win_sec\": 8.0,\n",
        "        \"stride_sec\": 4.0,\n",
        "        \"drop_last\": True,\n",
        "\n",
        "        # Model\n",
        "        \"hidden_dim\": 128,\n",
        "        \"latent_dim\": 16,\n",
        "        \"K_max\": 6,\n",
        "\n",
        "        # Loss Weights\n",
        "        \"lambda_recon\": 1.0,\n",
        "        \"lambda_smooth\": 0.05,\n",
        "        \"lambda_phase_ent\": 0.01,\n",
        "        \"lambda_effk\": 0.0075,\n",
        "\n",
        "        # temperature\n",
        "        \"tau\": 1.0,\n",
        "\n",
        "        # plotting controls\n",
        "        \"plot_max_T\": 2000,     # per-fold plot downsample cap\n",
        "        \"dpi\": 600,\n",
        "\n",
        "        # ✅ overlay controls\n",
        "        \"overlay_seg_sec\": 6.0,        # short segment length (sec)\n",
        "        \"overlay_channel_idx\": 0,      # which channel to show from x_np (z-scored)\n",
        "        \"overlay_select_mode\": \"max_rate\",  # \"max_rate\" or \"center\"\n",
        "\n",
        "        # Count-only labels\n",
        "        \"ALL_LABELS\": [\n",
        "            # --- act 8 ---\n",
        "            (\"subject1\", 8, 20), (\"subject2\", 8, 21), (\"subject3\", 8, 21), (\"subject4\", 8, 19), (\"subject5\", 8, 20),\n",
        "            (\"subject6\", 8, 20), (\"subject7\", 8, 21), (\"subject8\", 8, 21), (\"subject9\", 8, 21), (\"subject10\", 8, 21),\n",
        "\n",
        "            # --- act 10 ---\n",
        "            (\"subject1\", 10, 157), (\"subject2\", 10, 161), (\"subject3\", 10, 154), (\"subject4\", 10, 154), (\"subject5\", 10, 160),\n",
        "            (\"subject6\", 10, 156), (\"subject7\", 10, 153), (\"subject8\", 10, 160), (\"subject9\", 10, 166), (\"subject10\", 10, 156),\n",
        "\n",
        "            # --- act 12 ---\n",
        "            (\"subject1\", 12, 20), (\"subject2\", 12, 22), (\"subject3\", 12, 21), (\"subject4\", 12, 21), (\"subject5\", 12, 20),\n",
        "            (\"subject6\", 12, 21), (\"subject7\", 12, 19), (\"subject8\", 12, 20), (\"subject9\", 12, 20), (\"subject10\", 12, 20),\n",
        "        ],\n",
        "    }\n",
        "\n",
        "    # -----------------------------------------------------------------\n",
        "    # ✅ Exact paper titles (as requested)\n",
        "    # -----------------------------------------------------------------\n",
        "    TITLE_PHASE = \"\"\n",
        "    TITLE_PCA3D = \"\"\n",
        "    TITLE_RAW_OVERLAY = \"\"\n",
        "\n",
        "    _ensure_dir(CONFIG[\"out_dir\"])\n",
        "\n",
        "    set_strict_seed(CONFIG[\"seed\"])\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Device: {device}\")\n",
        "\n",
        "    full_data = load_mhealth_dataset(CONFIG[\"data_dir\"], CONFIG[\"TARGET_ACTIVITIES_MAP\"], CONFIG[\"COLUMN_NAMES\"])\n",
        "    if not full_data:\n",
        "        return\n",
        "\n",
        "    print(\"\\n\" + \"-\" * 90)\n",
        "    print(\" >>> Starting LOSO (count-only, K-auto) + WINDOWING — ALL ACTIVITIES (PNG export)\")\n",
        "    print(\"-\" * 90)\n",
        "\n",
        "    all_results = {}\n",
        "\n",
        "    # Evaluate each activity independently (LOSO within activity)\n",
        "    for act_id, act_name in CONFIG[\"TARGET_ACTIVITIES_MAP\"].items():\n",
        "        labels_act = [x for x in CONFIG[\"ALL_LABELS\"] if x[1] == act_id]\n",
        "        if len(labels_act) == 0:\n",
        "            print(f\"\\n[Skip activity] act_id={act_id} ({act_name}): no labels in ALL_LABELS.\")\n",
        "            continue\n",
        "\n",
        "        subjects_act = sorted(list(set([x[0] for x in labels_act])))\n",
        "\n",
        "        print(\"\\n\" + \"=\" * 90)\n",
        "        print(f\" [Activity] {act_id}: {act_name} | #subjects={len(subjects_act)}\")\n",
        "        print(\"=\" * 90)\n",
        "\n",
        "        act_out_dir = _ensure_dir(os.path.join(CONFIG[\"out_dir\"], f\"act{act_id}_{_safe_filename(act_name)}\"))\n",
        "\n",
        "        loso_maes = []\n",
        "\n",
        "        for fold_idx, test_subj in enumerate(subjects_act):\n",
        "            set_strict_seed(CONFIG[\"seed\"])\n",
        "\n",
        "            fold_out_dir = _ensure_dir(os.path.join(act_out_dir, f\"fold{fold_idx+1:02d}_{_safe_filename(test_subj)}\"))\n",
        "\n",
        "            train_labels = [x for x in labels_act if x[0] != test_subj]\n",
        "            test_labels  = [x for x in labels_act if x[0] == test_subj]\n",
        "\n",
        "            train_trials = prepare_trial_list(train_labels, full_data, CONFIG[\"TARGET_ACTIVITIES_MAP\"], CONFIG[\"ACT_FEATURE_MAP\"])\n",
        "            test_trials  = prepare_trial_list(test_labels,  full_data, CONFIG[\"TARGET_ACTIVITIES_MAP\"], CONFIG[\"ACT_FEATURE_MAP\"])\n",
        "\n",
        "            if not test_trials:\n",
        "                print(f\"[Skip] Activity {act_id} | Fold {fold_idx+1}: {test_subj} has no data.\")\n",
        "                continue\n",
        "\n",
        "            # ✅ TRAIN only: trial -> windows\n",
        "            train_data = trial_list_to_windows(\n",
        "                train_trials,\n",
        "                fs=CONFIG[\"fs\"],\n",
        "                win_sec=CONFIG[\"win_sec\"],\n",
        "                stride_sec=CONFIG[\"stride_sec\"],\n",
        "                drop_last=CONFIG[\"drop_last\"]\n",
        "            )\n",
        "\n",
        "            # TEST: trial 그대로\n",
        "            test_data = test_trials\n",
        "\n",
        "            g = torch.Generator()\n",
        "            g.manual_seed(CONFIG[\"seed\"])\n",
        "\n",
        "            train_loader = DataLoader(\n",
        "                TrialDataset(train_data),\n",
        "                batch_size=CONFIG[\"batch_size\"],\n",
        "                shuffle=True,\n",
        "                collate_fn=collate_variable_length,\n",
        "                generator=g,\n",
        "                num_workers=0\n",
        "            )\n",
        "\n",
        "            input_ch = train_data[0]['data'].shape[1]\n",
        "            model = KAutoCountModel(\n",
        "                input_ch=input_ch,\n",
        "                hidden_dim=CONFIG[\"hidden_dim\"],\n",
        "                latent_dim=CONFIG[\"latent_dim\"],\n",
        "                K_max=CONFIG[\"K_max\"]\n",
        "            ).to(device)\n",
        "\n",
        "            optimizer = torch.optim.Adam(model.parameters(), lr=CONFIG[\"lr\"])\n",
        "            scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.5)\n",
        "\n",
        "            for epoch in range(CONFIG[\"epochs\"]):\n",
        "                _ = train_one_epoch(model, train_loader, optimizer, CONFIG, device)\n",
        "                scheduler.step()\n",
        "\n",
        "            model.eval()\n",
        "\n",
        "            # ---- fold test ----\n",
        "            fold_mae = 0.0\n",
        "            fold_res_str = \"\"\n",
        "\n",
        "            test_gt = None\n",
        "            test_pred = None\n",
        "            test_diff = None\n",
        "            test_khat = None\n",
        "            test_entropy = None\n",
        "\n",
        "            test_x_np = None\n",
        "            test_rep_rate = None\n",
        "            test_phase_p = None\n",
        "            test_z_np = None\n",
        "            test_meta = None\n",
        "\n",
        "            for item in test_data:\n",
        "                x_np = item[\"data\"]  # (T,C)\n",
        "\n",
        "                # pred: windowing inference\n",
        "                count_pred_win, _win_rates = predict_count_by_windowing(\n",
        "                    model,\n",
        "                    x_np=x_np,\n",
        "                    fs=CONFIG[\"fs\"],\n",
        "                    win_sec=CONFIG[\"win_sec\"],\n",
        "                    stride_sec=CONFIG[\"stride_sec\"],\n",
        "                    device=device,\n",
        "                    tau=CONFIG.get(\"tau\", 1.0),\n",
        "                    batch_size=CONFIG.get(\"batch_size\", 64)\n",
        "                )\n",
        "\n",
        "                count_gt = float(item[\"count\"])\n",
        "                abs_err = abs(count_pred_win - count_gt)\n",
        "                fold_mae += abs_err\n",
        "                fold_res_str += f\"[Pred(win): {count_pred_win:.1f} / GT: {count_gt:.0f}]\"\n",
        "\n",
        "                # full-trial 1회 forward (rep_rate, phase_p, z 포함)\n",
        "                x_tensor = torch.tensor(x_np, dtype=torch.float32).transpose(0, 1).unsqueeze(0).to(device)  # (1,C,T)\n",
        "                with torch.no_grad():\n",
        "                    _, z_full, _, aux = model(x_tensor, mask=None, tau=CONFIG.get(\"tau\", 1.0))\n",
        "\n",
        "                phase_p = aux[\"phase_p\"].squeeze(0).detach().cpu().numpy()      # (T,K)\n",
        "                rep_rate = aux[\"rep_rate_t\"].squeeze(0).detach().cpu().numpy()  # (T,)\n",
        "                z_np_full = z_full.squeeze(0).detach().cpu().numpy()            # (T,D)\n",
        "\n",
        "                k_hat = float(aux[\"k_hat\"].item())\n",
        "                ent = compute_phase_entropy_mean(phase_p)\n",
        "\n",
        "                test_gt = count_gt\n",
        "                test_pred = float(count_pred_win)\n",
        "                test_diff = float(count_pred_win - count_gt)\n",
        "                test_khat = k_hat\n",
        "                test_entropy = ent\n",
        "\n",
        "                test_x_np = x_np\n",
        "                test_rep_rate = rep_rate\n",
        "                test_phase_p = phase_p\n",
        "                test_z_np = z_np_full\n",
        "                test_meta = item.get(\"meta\", f\"{test_subj}_{act_name}\")\n",
        "\n",
        "            fold_mae /= len(test_data)\n",
        "            loso_maes.append(fold_mae)\n",
        "\n",
        "            print(f\"  Fold {fold_idx+1:2d} | Test: {test_subj:<9s} | MAE: {fold_mae:.2f} | {fold_res_str}\")\n",
        "            if (test_gt is not None) and (test_pred is not None):\n",
        "                print(\n",
        "                    f\"    [Fold Summary] act={act_id} | {test_subj} | GT={test_gt:.0f} | Pred(win)={test_pred:.2f} | \"\n",
        "                    f\"Diff={test_diff:+.2f} | k_hat(full)={test_khat:.2f} | phase_entropy(full)={test_entropy:.3f}\"\n",
        "                )\n",
        "\n",
        "            # =========================\n",
        "            # ✅ Requested 3 plots (per fold) -> PNG\n",
        "            #   1) Phase heatmap\n",
        "            #   2) PCA3D\n",
        "            #   3) Raw + rep_rate overlay (short segment)\n",
        "            # =========================\n",
        "            if (test_phase_p is not None) and (test_z_np is not None) and (test_rep_rate is not None) and (test_x_np is not None):\n",
        "                # (1) Phase heatmap\n",
        "                plot_phase_heatmap_and_dominant(\n",
        "                    test_phase_p,\n",
        "                    fs=CONFIG[\"fs\"],\n",
        "                    title=TITLE_PHASE,\n",
        "                    max_T=int(CONFIG[\"plot_max_T\"]),\n",
        "                    save_path=os.path.join(fold_out_dir, \"01_phase_heatmap.png\"),\n",
        "                    dpi=int(CONFIG[\"dpi\"])\n",
        "                )\n",
        "\n",
        "                # (2) PCA3D\n",
        "                plot_pca3d_z(\n",
        "                    test_z_np,\n",
        "                    fs=CONFIG[\"fs\"],\n",
        "                    title=TITLE_PCA3D,\n",
        "                    max_T=int(CONFIG[\"plot_max_T\"]),\n",
        "                    save_path=os.path.join(fold_out_dir, \"02_pca3d.png\"),\n",
        "                    dpi=int(CONFIG[\"dpi\"])\n",
        "                )\n",
        "\n",
        "                # (3) Raw + rep_rate overlay (short segment)\n",
        "                plot_raw_and_rep_rate_overlay_short_segment(\n",
        "                    x_np=test_x_np,\n",
        "                    rep_rate=test_rep_rate,\n",
        "                    fs=CONFIG[\"fs\"],\n",
        "                    title=TITLE_RAW_OVERLAY,\n",
        "                    seg_sec=float(CONFIG[\"overlay_seg_sec\"]),\n",
        "                    channel_idx=int(CONFIG[\"overlay_channel_idx\"]),\n",
        "                    select_mode=str(CONFIG[\"overlay_select_mode\"]),\n",
        "                    save_path=os.path.join(fold_out_dir, \"03_raw_rep_rate_overlay.png\"),\n",
        "                    dpi=int(CONFIG[\"dpi\"])\n",
        "                )\n",
        "\n",
        "        # store results\n",
        "        if len(loso_maes) > 0:\n",
        "            all_results[act_id] = loso_maes\n",
        "            print(\"-\" * 90)\n",
        "            print(f\" [Activity Result] {act_name} | Mean MAE: {np.mean(loso_maes):.3f} | Std: {np.std(loso_maes):.3f}\")\n",
        "            print(\"-\" * 90)\n",
        "\n",
        "    # final summary across activities\n",
        "    print(\"\\n\" + \"#\" * 90)\n",
        "    print(\" >>> Summary (per activity)\")\n",
        "    print(\"#\" * 90)\n",
        "    for act_id, maes in all_results.items():\n",
        "        print(f\"  - act {act_id:>2d} ({CONFIG['TARGET_ACTIVITIES_MAP'][act_id]}): mean={np.mean(maes):.3f}, std={np.std(maes):.3f}\")\n",
        "    print(\"#\" * 90)\n",
        "    print(f\"[Saved plots] {os.path.abspath(CONFIG['out_dir'])}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8BNVeKYs1RF3",
        "outputId": "a7dc5b5d-4233-47bd-babe-12a8afe5ce89"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n",
            "Loading 10 subjects from /content/drive/MyDrive/Colab Notebooks/HAR_data/MHEALTHDATASET...\n",
            "\n",
            "------------------------------------------------------------------------------------------\n",
            " >>> Starting LOSO (count-only, K-auto) + WINDOWING — ALL ACTIVITIES (PNG export)\n",
            "------------------------------------------------------------------------------------------\n",
            "\n",
            "==========================================================================================\n",
            " [Activity] 8: Knees bending | #subjects=10\n",
            "==========================================================================================\n",
            "  Fold  1 | Test: subject1  | MAE: 12.18 | [Pred(win): 32.2 / GT: 20]\n",
            "    [Fold Summary] act=8 | subject1 | GT=20 | Pred(win)=32.18 | Diff=+12.18 | k_hat(full)=1.04 | phase_entropy(full)=0.084\n",
            "  Fold  2 | Test: subject10 | MAE: 2.31 | [Pred(win): 18.7 / GT: 21]\n",
            "    [Fold Summary] act=8 | subject10 | GT=21 | Pred(win)=18.69 | Diff=-2.31 | k_hat(full)=1.02 | phase_entropy(full)=0.059\n",
            "  Fold  3 | Test: subject2  | MAE: 1.54 | [Pred(win): 22.5 / GT: 21]\n",
            "    [Fold Summary] act=8 | subject2 | GT=21 | Pred(win)=22.54 | Diff=+1.54 | k_hat(full)=1.05 | phase_entropy(full)=0.103\n",
            "  Fold  4 | Test: subject3  | MAE: 4.99 | [Pred(win): 16.0 / GT: 21]\n",
            "    [Fold Summary] act=8 | subject3 | GT=21 | Pred(win)=16.01 | Diff=-4.99 | k_hat(full)=1.04 | phase_entropy(full)=0.085\n",
            "  Fold  5 | Test: subject4  | MAE: 5.68 | [Pred(win): 24.7 / GT: 19]\n",
            "    [Fold Summary] act=8 | subject4 | GT=19 | Pred(win)=24.68 | Diff=+5.68 | k_hat(full)=1.79 | phase_entropy(full)=0.248\n",
            "  Fold  6 | Test: subject5  | MAE: 1.30 | [Pred(win): 18.7 / GT: 20]\n",
            "    [Fold Summary] act=8 | subject5 | GT=20 | Pred(win)=18.70 | Diff=-1.30 | k_hat(full)=1.04 | phase_entropy(full)=0.076\n",
            "  Fold  7 | Test: subject6  | MAE: 4.94 | [Pred(win): 15.1 / GT: 20]\n",
            "    [Fold Summary] act=8 | subject6 | GT=20 | Pred(win)=15.06 | Diff=-4.94 | k_hat(full)=1.01 | phase_entropy(full)=0.037\n",
            "  Fold  8 | Test: subject7  | MAE: 2.24 | [Pred(win): 18.8 / GT: 21]\n",
            "    [Fold Summary] act=8 | subject7 | GT=21 | Pred(win)=18.76 | Diff=-2.24 | k_hat(full)=1.02 | phase_entropy(full)=0.049\n",
            "  Fold  9 | Test: subject8  | MAE: 3.25 | [Pred(win): 17.7 / GT: 21]\n",
            "    [Fold Summary] act=8 | subject8 | GT=21 | Pred(win)=17.75 | Diff=-3.25 | k_hat(full)=1.05 | phase_entropy(full)=0.101\n",
            "  Fold 10 | Test: subject9  | MAE: 4.32 | [Pred(win): 25.3 / GT: 21]\n",
            "    [Fold Summary] act=8 | subject9 | GT=21 | Pred(win)=25.32 | Diff=+4.32 | k_hat(full)=1.01 | phase_entropy(full)=0.032\n",
            "------------------------------------------------------------------------------------------\n",
            " [Activity Result] Knees bending | Mean MAE: 4.274 | Std: 3.012\n",
            "------------------------------------------------------------------------------------------\n",
            "\n",
            "==========================================================================================\n",
            " [Activity] 10: Jogging | #subjects=10\n",
            "==========================================================================================\n",
            "  Fold  1 | Test: subject1  | MAE: 3.03 | [Pred(win): 154.0 / GT: 157]\n",
            "    [Fold Summary] act=10 | subject1 | GT=157 | Pred(win)=153.97 | Diff=-3.03 | k_hat(full)=1.29 | phase_entropy(full)=0.365\n",
            "  Fold  2 | Test: subject10 | MAE: 7.68 | [Pred(win): 163.7 / GT: 156]\n",
            "    [Fold Summary] act=10 | subject10 | GT=156 | Pred(win)=163.68 | Diff=+7.68 | k_hat(full)=1.54 | phase_entropy(full)=0.353\n",
            "  Fold  3 | Test: subject2  | MAE: 5.05 | [Pred(win): 166.0 / GT: 161]\n",
            "    [Fold Summary] act=10 | subject2 | GT=161 | Pred(win)=166.05 | Diff=+5.05 | k_hat(full)=1.74 | phase_entropy(full)=0.504\n",
            "  Fold  4 | Test: subject3  | MAE: 6.57 | [Pred(win): 147.4 / GT: 154]\n",
            "    [Fold Summary] act=10 | subject3 | GT=154 | Pred(win)=147.43 | Diff=-6.57 | k_hat(full)=1.47 | phase_entropy(full)=0.345\n",
            "  Fold  5 | Test: subject4  | MAE: 4.35 | [Pred(win): 158.3 / GT: 154]\n",
            "    [Fold Summary] act=10 | subject4 | GT=154 | Pred(win)=158.35 | Diff=+4.35 | k_hat(full)=1.44 | phase_entropy(full)=0.309\n",
            "  Fold  6 | Test: subject5  | MAE: 2.54 | [Pred(win): 157.5 / GT: 160]\n",
            "    [Fold Summary] act=10 | subject5 | GT=160 | Pred(win)=157.46 | Diff=-2.54 | k_hat(full)=1.53 | phase_entropy(full)=0.320\n",
            "  Fold  7 | Test: subject6  | MAE: 41.59 | [Pred(win): 197.6 / GT: 156]\n",
            "    [Fold Summary] act=10 | subject6 | GT=156 | Pred(win)=197.59 | Diff=+41.59 | k_hat(full)=1.37 | phase_entropy(full)=0.410\n",
            "  Fold  8 | Test: subject7  | MAE: 11.78 | [Pred(win): 141.2 / GT: 153]\n",
            "    [Fold Summary] act=10 | subject7 | GT=153 | Pred(win)=141.22 | Diff=-11.78 | k_hat(full)=1.66 | phase_entropy(full)=0.388\n",
            "  Fold  9 | Test: subject8  | MAE: 13.48 | [Pred(win): 146.5 / GT: 160]\n",
            "    [Fold Summary] act=10 | subject8 | GT=160 | Pred(win)=146.52 | Diff=-13.48 | k_hat(full)=1.49 | phase_entropy(full)=0.420\n",
            "  Fold 10 | Test: subject9  | MAE: 8.15 | [Pred(win): 174.2 / GT: 166]\n",
            "    [Fold Summary] act=10 | subject9 | GT=166 | Pred(win)=174.15 | Diff=+8.15 | k_hat(full)=1.67 | phase_entropy(full)=0.313\n",
            "------------------------------------------------------------------------------------------\n",
            " [Activity Result] Jogging | Mean MAE: 10.421 | Std: 10.920\n",
            "------------------------------------------------------------------------------------------\n",
            "\n",
            "==========================================================================================\n",
            " [Activity] 12: Jump front & back | #subjects=10\n",
            "==========================================================================================\n",
            "  Fold  1 | Test: subject1  | MAE: 0.42 | [Pred(win): 20.4 / GT: 20]\n",
            "    [Fold Summary] act=12 | subject1 | GT=20 | Pred(win)=20.42 | Diff=+0.42 | k_hat(full)=2.68 | phase_entropy(full)=0.880\n",
            "  Fold  2 | Test: subject10 | MAE: 2.05 | [Pred(win): 22.1 / GT: 20]\n",
            "    [Fold Summary] act=12 | subject10 | GT=20 | Pred(win)=22.05 | Diff=+2.05 | k_hat(full)=2.24 | phase_entropy(full)=0.788\n",
            "  Fold  3 | Test: subject2  | MAE: 0.30 | [Pred(win): 22.3 / GT: 22]\n",
            "    [Fold Summary] act=12 | subject2 | GT=22 | Pred(win)=22.30 | Diff=+0.30 | k_hat(full)=1.73 | phase_entropy(full)=0.663\n",
            "  Fold  4 | Test: subject3  | MAE: 0.33 | [Pred(win): 20.7 / GT: 21]\n",
            "    [Fold Summary] act=12 | subject3 | GT=21 | Pred(win)=20.67 | Diff=-0.33 | k_hat(full)=2.24 | phase_entropy(full)=0.783\n",
            "  Fold  5 | Test: subject4  | MAE: 2.14 | [Pred(win): 23.1 / GT: 21]\n",
            "    [Fold Summary] act=12 | subject4 | GT=21 | Pred(win)=23.14 | Diff=+2.14 | k_hat(full)=2.25 | phase_entropy(full)=0.838\n",
            "  Fold  6 | Test: subject5  | MAE: 3.56 | [Pred(win): 16.4 / GT: 20]\n",
            "    [Fold Summary] act=12 | subject5 | GT=20 | Pred(win)=16.44 | Diff=-3.56 | k_hat(full)=2.59 | phase_entropy(full)=0.898\n",
            "  Fold  7 | Test: subject6  | MAE: 0.31 | [Pred(win): 21.3 / GT: 21]\n",
            "    [Fold Summary] act=12 | subject6 | GT=21 | Pred(win)=21.31 | Diff=+0.31 | k_hat(full)=2.68 | phase_entropy(full)=0.884\n",
            "  Fold  8 | Test: subject7  | MAE: 0.61 | [Pred(win): 18.4 / GT: 19]\n",
            "    [Fold Summary] act=12 | subject7 | GT=19 | Pred(win)=18.39 | Diff=-0.61 | k_hat(full)=2.74 | phase_entropy(full)=0.896\n",
            "  Fold  9 | Test: subject8  | MAE: 0.03 | [Pred(win): 20.0 / GT: 20]\n",
            "    [Fold Summary] act=12 | subject8 | GT=20 | Pred(win)=19.97 | Diff=-0.03 | k_hat(full)=2.17 | phase_entropy(full)=0.851\n",
            "  Fold 10 | Test: subject9  | MAE: 1.28 | [Pred(win): 18.7 / GT: 20]\n",
            "    [Fold Summary] act=12 | subject9 | GT=20 | Pred(win)=18.72 | Diff=-1.28 | k_hat(full)=2.77 | phase_entropy(full)=0.872\n",
            "------------------------------------------------------------------------------------------\n",
            " [Activity Result] Jump front & back | Mean MAE: 1.103 | Std: 1.083\n",
            "------------------------------------------------------------------------------------------\n",
            "\n",
            "##########################################################################################\n",
            " >>> Summary (per activity)\n",
            "##########################################################################################\n",
            "  - act  8 (Knees bending): mean=4.274, std=3.012\n",
            "  - act 10 (Jogging): mean=10.421, std=10.920\n",
            "  - act 12 (Jump front & back): mean=1.103, std=1.083\n",
            "##########################################################################################\n",
            "[Saved plots] /content/outputs_loso_activity_plots\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# ✅ UPDATED: only 2 plots per scenario\n",
        "#   1) Window-level rep_rate mean±std (+ boundary + A/B shading)\n",
        "#   2) Latent trajectory PCA2 (scatter + mean trajectory line + boundary marker + start/end + std circles)\n",
        "#   - NO titles\n",
        "#   - Bigger fonts\n",
        "# =========================\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns  # kept\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.ndimage import gaussian_filter1d  # kept\n",
        "from matplotlib.patches import Circle\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# ✅ Save helpers\n",
        "# ---------------------------------------------------------------------\n",
        "def _ensure_dir(path: str):\n",
        "    if path is None or path == \"\":\n",
        "        return\n",
        "    os.makedirs(path, exist_ok=True)\n",
        "\n",
        "def _safe_fname(s: str):\n",
        "    s = str(s)\n",
        "    for ch in ['/', '\\\\', ':', '*', '?', '\"', '<', '>', '|', '\\n', '\\t']:\n",
        "        s = s.replace(ch, '_')\n",
        "    s = s.strip().replace(' ', '_')\n",
        "    while '__' in s:\n",
        "        s = s.replace('__', '_')\n",
        "    return s\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 1) Strict Seeding\n",
        "# ---------------------------------------------------------------------\n",
        "def set_strict_seed(seed: int):\n",
        "    random.seed(seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 2) Data Loading\n",
        "# ---------------------------------------------------------------------\n",
        "def load_mhealth_dataset(data_dir, target_activities_map, column_names):\n",
        "    full_dataset = {}\n",
        "    file_list = sorted(glob.glob(os.path.join(data_dir, \"mHealth_subject*.log\")))\n",
        "\n",
        "    if not file_list:\n",
        "        print(f\"[Warning] No mHealth logs found in {data_dir}\")\n",
        "        return {}\n",
        "\n",
        "    print(f\"Loading {len(file_list)} subjects from {data_dir}...\")\n",
        "\n",
        "    for file_path in file_list:\n",
        "        file_name = os.path.basename(file_path)\n",
        "        subj_part = file_name.split('.')[0]\n",
        "        try:\n",
        "            subj_id_num = int(''.join(filter(str.isdigit, subj_part)))\n",
        "            subj_key = f\"subject{subj_id_num}\"\n",
        "        except:\n",
        "            subj_key = subj_part\n",
        "\n",
        "        try:\n",
        "            df = pd.read_csv(file_path, sep=\"\\t\", header=None)\n",
        "            df = df.iloc[:, :len(column_names)]\n",
        "            df.columns = column_names\n",
        "\n",
        "            subj_data = {}\n",
        "            for label_code, activity_name in target_activities_map.items():\n",
        "                activity_df = df[df['activity_id'] == label_code].copy()\n",
        "                if not activity_df.empty:\n",
        "                    subj_data[activity_name] = activity_df.drop(columns=['activity_id'])\n",
        "\n",
        "            full_dataset[subj_key] = subj_data\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {file_name}: {e}\")\n",
        "            pass\n",
        "\n",
        "    return full_dataset\n",
        "\n",
        "\n",
        "def prepare_trial_list(label_config, full_data, target_map, feature_map):\n",
        "    trial_list = []\n",
        "    for subj, act_id, gt_count in label_config:\n",
        "        act_name = target_map.get(act_id)\n",
        "        feats = feature_map.get(act_id)\n",
        "\n",
        "        if subj in full_data and act_name in full_data[subj]:\n",
        "            raw_df = full_data[subj][act_name][feats]\n",
        "            raw_np = raw_df.values.astype(np.float32)\n",
        "\n",
        "            mean = raw_np.mean(axis=0)\n",
        "            std = raw_np.std(axis=0) + 1e-6\n",
        "            norm_np = (raw_np - mean) / std\n",
        "\n",
        "            trial_list.append({\n",
        "                'data': norm_np,\n",
        "                'count': float(gt_count),\n",
        "                'meta': f\"{subj}_{act_name}\"\n",
        "            })\n",
        "        else:\n",
        "            print(f\"[Skip] Missing data for {subj} - {act_name}\")\n",
        "\n",
        "    return trial_list\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# ✅ Mixed A-B builder\n",
        "# ---------------------------------------------------------------------\n",
        "def get_single_trial_from_full_data(subj, act_id, gt_count, full_data, target_map, feature_map, normalize=True):\n",
        "    act_name = target_map.get(act_id)\n",
        "    feats = feature_map.get(act_id)\n",
        "\n",
        "    if subj not in full_data or act_name not in full_data[subj]:\n",
        "        return None\n",
        "\n",
        "    raw_df = full_data[subj][act_name][feats]\n",
        "    x = raw_df.values.astype(np.float32)\n",
        "\n",
        "    if normalize:\n",
        "        mean = x.mean(axis=0)\n",
        "        std = x.std(axis=0) + 1e-6\n",
        "        x = (x - mean) / std\n",
        "\n",
        "    return {\n",
        "        \"data\": x,\n",
        "        \"count\": float(gt_count),\n",
        "        \"meta\": f\"{subj}_{act_name}\"\n",
        "    }\n",
        "\n",
        "\n",
        "def build_mixed_ab_trial(subj, actA_id, actB_id, config, full_data):\n",
        "    gt_map = config.get(\"GT_BY_ACT\", {})\n",
        "    if actA_id not in gt_map or actB_id not in gt_map:\n",
        "        return None\n",
        "    if subj not in gt_map[actA_id] or subj not in gt_map[actB_id]:\n",
        "        return None\n",
        "\n",
        "    gtA = float(gt_map[actA_id][subj])\n",
        "    gtB = float(gt_map[actB_id][subj])\n",
        "    gt_total = gtA + gtB\n",
        "\n",
        "    A = get_single_trial_from_full_data(\n",
        "        subj, actA_id, gtA, full_data,\n",
        "        config[\"TARGET_ACTIVITIES_MAP\"], config[\"ACT_FEATURE_MAP\"],\n",
        "        normalize=False\n",
        "    )\n",
        "    B = get_single_trial_from_full_data(\n",
        "        subj, actB_id, gtB, full_data,\n",
        "        config[\"TARGET_ACTIVITIES_MAP\"], config[\"ACT_FEATURE_MAP\"],\n",
        "        normalize=False\n",
        "    )\n",
        "    if A is None or B is None:\n",
        "        return None\n",
        "\n",
        "    xA_raw = A[\"data\"]\n",
        "    xB_raw = B[\"data\"]\n",
        "    boundary = int(xA_raw.shape[0])\n",
        "\n",
        "    x_mix_raw = np.concatenate([xA_raw, xB_raw], axis=0).astype(np.float32)\n",
        "\n",
        "    mean = x_mix_raw.mean(axis=0)\n",
        "    std = x_mix_raw.std(axis=0) + 1e-6\n",
        "    x_mix = (x_mix_raw - mean) / std\n",
        "\n",
        "    xA = (xA_raw - mean) / std\n",
        "    xB = (xB_raw - mean) / std\n",
        "\n",
        "    actA_name = config[\"TARGET_ACTIVITIES_MAP\"].get(actA_id, str(actA_id))\n",
        "    actB_name = config[\"TARGET_ACTIVITIES_MAP\"].get(actB_id, str(actB_id))\n",
        "\n",
        "    return {\n",
        "        \"data\": x_mix,\n",
        "        \"count\": float(gt_total),\n",
        "        \"meta\": f\"{subj}__{actA_name}__TO__{actB_name}\",\n",
        "        \"boundary\": boundary,\n",
        "        \"meta_detail\": {\n",
        "            \"subj\": subj,\n",
        "            \"actA_id\": actA_id, \"actB_id\": actB_id,\n",
        "            \"actA_name\": actA_name, \"actB_name\": actB_name,\n",
        "            \"gtA\": gtA, \"gtB\": gtB,\n",
        "            \"gt_total\": gt_total,\n",
        "        },\n",
        "        \"data_A\": xA,\n",
        "        \"data_B\": xB,\n",
        "        \"T_A\": int(xA.shape[0]),\n",
        "        \"T_B\": int(xB.shape[0]),\n",
        "    }\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 2.5) Windowing\n",
        "# ---------------------------------------------------------------------\n",
        "def trial_list_to_windows(trial_list, fs, win_sec=8.0, stride_sec=4.0, drop_last=True):\n",
        "    win_len = int(round(win_sec * fs))\n",
        "    stride = int(round(stride_sec * fs))\n",
        "    assert win_len > 0 and stride > 0\n",
        "\n",
        "    windows = []\n",
        "    for item in trial_list:\n",
        "        x = item[\"data\"]\n",
        "        T = x.shape[0]\n",
        "        total_count = float(item[\"count\"])\n",
        "        meta = item[\"meta\"]\n",
        "\n",
        "        total_dur = max(T / float(fs), 1e-6)\n",
        "        rate_trial = total_count / total_dur\n",
        "\n",
        "        if T < win_len:\n",
        "            win_dur = T / float(fs)\n",
        "            windows.append({\n",
        "                \"data\": x,\n",
        "                \"count\": rate_trial * win_dur,\n",
        "                \"meta\": f\"{meta}__win[0:{T}]\",\n",
        "                \"parent_meta\": meta,\n",
        "                \"parent_T\": T,\n",
        "                \"win_start\": 0,\n",
        "                \"win_end\": T,\n",
        "            })\n",
        "            continue\n",
        "\n",
        "        last_start = T - win_len\n",
        "        starts = list(range(0, last_start + 1, stride))\n",
        "\n",
        "        for st in starts:\n",
        "            ed = st + win_len\n",
        "            win_dur = win_len / float(fs)\n",
        "            windows.append({\n",
        "                \"data\": x[st:ed],\n",
        "                \"count\": rate_trial * win_dur,\n",
        "                \"meta\": f\"{meta}__win[{st}:{ed}]\",\n",
        "                \"parent_meta\": meta,\n",
        "                \"parent_T\": T,\n",
        "                \"win_start\": st,\n",
        "                \"win_end\": ed,\n",
        "            })\n",
        "\n",
        "        if not drop_last:\n",
        "            last_st = starts[-1] + stride\n",
        "            if last_st < T:\n",
        "                ed = T\n",
        "                win_dur = (ed - last_st) / float(fs)\n",
        "                windows.append({\n",
        "                    \"data\": x[last_st:ed],\n",
        "                    \"count\": rate_trial * win_dur,\n",
        "                    \"meta\": f\"{meta}__win[{last_st}:{ed}]\",\n",
        "                    \"parent_meta\": meta,\n",
        "                    \"parent_T\": T,\n",
        "                    \"win_start\": last_st,\n",
        "                    \"win_end\": ed,\n",
        "                })\n",
        "\n",
        "    return windows\n",
        "\n",
        "\n",
        "def predict_count_by_windowing(model, x_np, fs, win_sec, stride_sec, device, tau=1.0, batch_size=64):\n",
        "    win_len = int(round(win_sec * fs))\n",
        "    stride = int(round(stride_sec * fs))\n",
        "    T = x_np.shape[0]\n",
        "    total_dur = T / float(fs)\n",
        "\n",
        "    if T <= win_len:\n",
        "        x_tensor = torch.tensor(x_np, dtype=torch.float32).transpose(0, 1).unsqueeze(0).to(device)\n",
        "        with torch.no_grad():\n",
        "            rate_hat, _, _, _ = model(x_tensor, mask=None, tau=tau)\n",
        "        pred_count = float(rate_hat.item() * total_dur)\n",
        "        return pred_count, np.array([float(rate_hat.item())], dtype=np.float32)\n",
        "\n",
        "    starts = list(range(0, T - win_len + 1, stride))\n",
        "    windows = np.stack([x_np[st:st + win_len] for st in starts], axis=0)\n",
        "\n",
        "    xw = torch.tensor(windows, dtype=torch.float32).permute(0, 2, 1).to(device)\n",
        "\n",
        "    rates = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, xw.shape[0], batch_size):\n",
        "            xb = xw[i:i + batch_size]\n",
        "            r_hat, _, _, _ = model(xb, mask=None, tau=tau)\n",
        "            rates.append(r_hat.detach().cpu().numpy())\n",
        "\n",
        "    rates = np.concatenate(rates, axis=0)\n",
        "    rate_mean = float(rates.mean())\n",
        "    pred_count = rate_mean * total_dur\n",
        "    return float(pred_count), rates\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# Helpers for scenario-avg plots\n",
        "# ---------------------------------------------------------------------\n",
        "def get_window_rate_curve(model, x_np, fs, win_sec, stride_sec, device, tau=1.0, batch_size=64):\n",
        "    win_len = int(round(win_sec * fs))\n",
        "    stride = int(round(stride_sec * fs))\n",
        "    T = x_np.shape[0]\n",
        "\n",
        "    if T <= win_len:\n",
        "        pred_count, rates = predict_count_by_windowing(\n",
        "            model, x_np, fs, win_sec, stride_sec, device, tau=tau, batch_size=batch_size\n",
        "        )\n",
        "        t_cent = np.array([0.5 * (T / float(fs))], dtype=np.float32)\n",
        "        return t_cent, rates.astype(np.float32)\n",
        "\n",
        "    starts = list(range(0, T - win_len + 1, stride))\n",
        "    windows = np.stack([x_np[st:st + win_len] for st in starts], axis=0)\n",
        "\n",
        "    xw = torch.tensor(windows, dtype=torch.float32).permute(0, 2, 1).to(device)\n",
        "\n",
        "    rates = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, xw.shape[0], batch_size):\n",
        "            xb = xw[i:i + batch_size]\n",
        "            r_hat, _, _, _ = model(xb, mask=None, tau=tau)\n",
        "            rates.append(r_hat.detach().cpu().numpy())\n",
        "    rates = np.concatenate(rates, axis=0).astype(np.float32)\n",
        "\n",
        "    centers = np.array([(st + 0.5 * win_len) / float(fs) for st in starts], dtype=np.float32)\n",
        "    return centers, rates\n",
        "\n",
        "\n",
        "def resample_1d(y, new_len):\n",
        "    y = np.asarray(y, dtype=np.float32)\n",
        "    if y.size == 0:\n",
        "        return np.zeros((new_len,), dtype=np.float32)\n",
        "    if y.size == 1:\n",
        "        return np.full((new_len,), float(y[0]), dtype=np.float32)\n",
        "    x_old = np.linspace(0.0, 1.0, num=y.size, dtype=np.float32)\n",
        "    x_new = np.linspace(0.0, 1.0, num=new_len, dtype=np.float32)\n",
        "    return np.interp(x_new, x_old, y).astype(np.float32)\n",
        "\n",
        "\n",
        "def resample_2d(Y, new_len):\n",
        "    Y = np.asarray(Y, dtype=np.float32)\n",
        "    if Y.shape[0] == 0:\n",
        "        return np.zeros((new_len, Y.shape[1]), dtype=np.float32)\n",
        "    if Y.shape[0] == 1:\n",
        "        return np.repeat(Y, repeats=new_len, axis=0).astype(np.float32)\n",
        "    x_old = np.linspace(0.0, 1.0, num=Y.shape[0], dtype=np.float32)\n",
        "    x_new = np.linspace(0.0, 1.0, num=new_len, dtype=np.float32)\n",
        "    out = []\n",
        "    for d in range(Y.shape[1]):\n",
        "        out.append(np.interp(x_new, x_old, Y[:, d]))\n",
        "    return np.stack(out, axis=1).astype(np.float32)\n",
        "\n",
        "\n",
        "def global_pca2(Z_all):\n",
        "    Z_all = np.asarray(Z_all, dtype=np.float32)\n",
        "    mu = Z_all.mean(axis=0, keepdims=True)\n",
        "    Zc = Z_all - mu\n",
        "    _, _, Vt = np.linalg.svd(Zc, full_matrices=False)\n",
        "    V2 = Vt[:2].astype(np.float32)\n",
        "    return mu.squeeze(0).astype(np.float32), V2\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# ✅ UPDATED PLOTTER: only (1) and (2), no titles, bigger fonts\n",
        "# ---------------------------------------------------------------------\n",
        "def plot_scenario_mean_std_two_plots(\n",
        "    save_dir,\n",
        "    scen_name,\n",
        "    win_t, win_mean, win_std,\n",
        "    lat_xy_mean, lat_xy_std,   # (std는 이제 안 쓰지만 signature 유지)\n",
        "    n_subjects,\n",
        "    dpi=600,\n",
        "):\n",
        "    _ensure_dir(save_dir if save_dir is not None else \"scenario_viz\")\n",
        "    out_dir = save_dir if save_dir is not None else \"scenario_viz\"\n",
        "    scen_tag = _safe_fname(scen_name)\n",
        "\n",
        "    # -------------------------\n",
        "    # (1) Window-level rep_rate mean±std (+ boundary + A/B shading)\n",
        "    # - A/B 글자 삭제\n",
        "    # - figsize/폰트 크게\n",
        "    # -------------------------\n",
        "    fig1 = plt.figure(figsize=(26.0, 9.2))  # ✅ bigger\n",
        "    ax = fig1.gca()\n",
        "\n",
        "    ax.axvspan(0.0, 1.0, alpha=0.06, color=\"k\", lw=0)\n",
        "    ax.axvspan(1.0, 2.0, alpha=0.03, color=\"k\", lw=0)\n",
        "\n",
        "    ax.plot(win_t, win_mean, linewidth=6.4, label=\"Window rep_rate (mean)\")\n",
        "    ax.fill_between(win_t, win_mean - win_std, win_mean + win_std, alpha=0.22, label=\"±1 std\")\n",
        "    ax.axvline(1.0, linestyle=\"--\", linewidth=6.0, label=\"Boundary (A→B)\")\n",
        "\n",
        "    ax.set_xlabel(\"Normalized time (A:0→1, B:1→2)\", fontsize=46, labelpad=10)\n",
        "    ax.set_ylabel(\"rep_rate (reps/s)\", fontsize=46, labelpad=10)\n",
        "    ax.tick_params(axis=\"both\", labelsize=37)\n",
        "\n",
        "    ax.legend(fontsize=38, frameon=True, loc=\"upper left\")\n",
        "    fig1.tight_layout()\n",
        "    fig1.savefig(os.path.join(out_dir, f\"{scen_tag}__1_win_rep_rate.png\"), dpi=dpi, bbox_inches=\"tight\")\n",
        "    plt.close(fig1)\n",
        "\n",
        "    # -------------------------\n",
        "    # (2) Latent trajectory PCA2\n",
        "    # - std circle 삭제\n",
        "    # - trajectory line 더 얇게\n",
        "    # -------------------------\n",
        "    fig2 = plt.figure(figsize=(8.8, 8.8))\n",
        "    ax = fig2.gca()\n",
        "\n",
        "    T = lat_xy_mean.shape[0]\n",
        "    t_lat = np.linspace(0.0, 2.0, num=T, dtype=np.float32)\n",
        "\n",
        "    sc = ax.scatter(lat_xy_mean[:, 0], lat_xy_mean[:, 1], c=t_lat, s=22, alpha=0.95)\n",
        "\n",
        "    ax.set_xlabel(\"PC1\", fontsize=24, labelpad=10)\n",
        "    ax.set_ylabel(\"PC2\", fontsize=24, labelpad=10)\n",
        "    ax.tick_params(axis=\"both\", labelsize=18)\n",
        "    ax.set_aspect(\"equal\", adjustable=\"box\")\n",
        "\n",
        "    # square bounds 유지\n",
        "    x = lat_xy_mean[:, 0]; y = lat_xy_mean[:, 1]\n",
        "    xmin, xmax = float(np.min(x)), float(np.max(x))\n",
        "    ymin, ymax = float(np.min(y)), float(np.max(y))\n",
        "    cx = 0.5 * (xmin + xmax); cy = 0.5 * (ymin + ymax)\n",
        "    rx = 0.5 * (xmax - xmin); ry = 0.5 * (ymax - ymin)\n",
        "    r = max(rx, ry) * 1.12 + 1e-6\n",
        "    ax.set_xlim(cx - r, cx + r)\n",
        "    ax.set_ylim(cy - r, cy + r)\n",
        "\n",
        "    cbar = fig2.colorbar(sc, ax=ax, fraction=0.046, pad=0.04)\n",
        "    cbar.set_label(\"normalized time (0→2)\", fontsize=20, labelpad=10)\n",
        "    cbar.ax.tick_params(labelsize=16)\n",
        "\n",
        "    fig2.tight_layout()\n",
        "    fig2.savefig(os.path.join(out_dir, f\"{scen_tag}__2_latent_pca2.png\"), dpi=dpi, bbox_inches=\"tight\")\n",
        "    plt.close(fig2)\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# Dataset / Collate\n",
        "# ---------------------------------------------------------------------\n",
        "class TrialDataset(Dataset):\n",
        "    def __init__(self, trial_list):\n",
        "        self.trials = trial_list\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.trials)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.trials[idx]\n",
        "        data = torch.tensor(item['data'], dtype=torch.float32).transpose(0, 1)  # (C, T)\n",
        "        count = torch.tensor(item['count'], dtype=torch.float32)\n",
        "        return data, count, item['meta']\n",
        "\n",
        "\n",
        "def collate_variable_length(batch):\n",
        "    max_len = max([x[0].shape[1] for x in batch])\n",
        "    C = batch[0][0].shape[0]\n",
        "\n",
        "    padded_data, masks, counts, metas, lengths = [], [], [], [], []\n",
        "    for data, count, meta in batch:\n",
        "        T = data.shape[1]\n",
        "        lengths.append(T)\n",
        "\n",
        "        pad_size = max_len - T\n",
        "        if pad_size > 0:\n",
        "            pad = torch.zeros(C, pad_size)\n",
        "            d_padded = torch.cat([data, pad], dim=1)\n",
        "            mask = torch.cat([torch.ones(T), torch.zeros(pad_size)], dim=0)\n",
        "        else:\n",
        "            d_padded = data\n",
        "            mask = torch.ones(T)\n",
        "\n",
        "        padded_data.append(d_padded)\n",
        "        masks.append(mask)\n",
        "        counts.append(count)\n",
        "        metas.append(meta)\n",
        "\n",
        "    return {\n",
        "        \"data\": torch.stack(padded_data),\n",
        "        \"mask\": torch.stack(masks),\n",
        "        \"count\": torch.stack(counts),\n",
        "        \"length\": torch.tensor(lengths, dtype=torch.float32),\n",
        "        \"meta\": metas\n",
        "    }\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# Model\n",
        "# ---------------------------------------------------------------------\n",
        "class ManifoldEncoder(nn.Module):\n",
        "    def __init__(self, input_ch, hidden_dim=128, latent_dim=16):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv1d(input_ch, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, latent_dim, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        z = self.net(x)\n",
        "        z = z.transpose(1, 2)\n",
        "        return z\n",
        "\n",
        "\n",
        "class ManifoldDecoder(nn.Module):\n",
        "    def __init__(self, latent_dim, hidden_dim, out_ch):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv1d(latent_dim, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, out_ch, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        zt = z.transpose(1, 2)\n",
        "        x_hat = self.net(zt)\n",
        "        return x_hat\n",
        "\n",
        "\n",
        "class MultiRateHead(nn.Module):\n",
        "    def __init__(self, latent_dim=16, hidden=64, K_max=6):\n",
        "        super().__init__()\n",
        "        self.K_max = K_max\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(latent_dim, hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden, 1 + K_max)\n",
        "        )\n",
        "\n",
        "    def forward(self, z, tau=1.0):\n",
        "        out = self.net(z)\n",
        "        amp = F.softplus(out[..., 0])\n",
        "        phase_logits = out[..., 1:]\n",
        "        phase = F.softmax(phase_logits / tau, dim=-1)\n",
        "        return amp, phase, phase_logits\n",
        "\n",
        "\n",
        "class KAutoCountModel(nn.Module):\n",
        "    def __init__(self, input_ch, hidden_dim=128, latent_dim=16, K_max=6, k_hidden=64):\n",
        "        super().__init__()\n",
        "        self.encoder = ManifoldEncoder(input_ch, hidden_dim, latent_dim)\n",
        "        self.decoder = ManifoldDecoder(latent_dim, hidden_dim, input_ch)\n",
        "        self.rate_head = MultiRateHead(latent_dim, hidden=hidden_dim, K_max=K_max)\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, (nn.Conv1d, nn.Linear)):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            b = self.rate_head.net[-1].bias\n",
        "            b.zero_()\n",
        "            b[0].fill_(-2.0)\n",
        "\n",
        "    @staticmethod\n",
        "    def _masked_mean_time(x, mask=None, eps=1e-6):\n",
        "        if mask is None:\n",
        "            return x.mean(dim=1)\n",
        "        if x.dim() == 2:\n",
        "            m = mask.to(dtype=x.dtype, device=x.device)\n",
        "            return (x * m).sum(dim=1) / (m.sum(dim=1) + eps)\n",
        "        elif x.dim() == 3:\n",
        "            m = mask.to(dtype=x.dtype, device=x.device).unsqueeze(-1)\n",
        "            return (x * m).sum(dim=1) / (m.sum(dim=1) + eps)\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported dim for masked mean: {x.dim()}\")\n",
        "\n",
        "    def forward(self, x, mask=None, tau=1.0):\n",
        "        z = self.encoder(x)\n",
        "        x_hat = self.decoder(z)\n",
        "\n",
        "        amp_t, phase_p, phase_logits = self.rate_head(z, tau=tau)\n",
        "        micro_rate_t = amp_t\n",
        "\n",
        "        p_bar = self._masked_mean_time(phase_p, mask)\n",
        "        k_hat = 1.0 / (p_bar.pow(2).sum(dim=1) + 1e-6)\n",
        "\n",
        "        rep_rate_t = micro_rate_t / (k_hat.unsqueeze(1) + 1e-6)\n",
        "        if mask is not None:\n",
        "            rep_rate_t = rep_rate_t * mask\n",
        "\n",
        "        if mask is None:\n",
        "            avg_rep_rate = rep_rate_t.mean(dim=1)\n",
        "        else:\n",
        "            avg_rep_rate = (rep_rate_t * mask).sum(dim=1) / (mask.sum(dim=1) + 1e-6)\n",
        "\n",
        "        aux = {\n",
        "            \"phase_p\": phase_p,\n",
        "            \"phase_logits\": phase_logits,\n",
        "            \"micro_rate_t\": micro_rate_t,\n",
        "            \"rep_rate_t\": rep_rate_t,\n",
        "            \"k_hat\": k_hat,\n",
        "        }\n",
        "        return avg_rep_rate, z, x_hat, aux\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# Loss utils\n",
        "# ---------------------------------------------------------------------\n",
        "def masked_recon_mse(x_hat, x, mask, eps=1e-6):\n",
        "    mask = mask.to(dtype=x.dtype, device=x.device)\n",
        "    mask_bc = mask.unsqueeze(1)\n",
        "    se = (x_hat - x) ** 2\n",
        "    se = se * mask_bc\n",
        "    denom = (mask.sum() * x.shape[1]) + eps\n",
        "    return se.sum() / denom\n",
        "\n",
        "\n",
        "def temporal_smoothness(v, mask=None, eps=1e-6):\n",
        "    dv = torch.abs(v[:, 1:] - v[:, :-1])\n",
        "    if mask is None:\n",
        "        return dv.mean()\n",
        "    m = mask[:, 1:] * mask[:, :-1]\n",
        "    m = m.to(dtype=dv.dtype, device=dv.device)\n",
        "    return (dv * m).sum() / (m.sum() + eps)\n",
        "\n",
        "\n",
        "def phase_entropy_loss(phase_p, mask=None, eps=1e-8):\n",
        "    ent = -(phase_p * (phase_p + eps).log()).sum(dim=-1)\n",
        "    if mask is None:\n",
        "        return ent.mean()\n",
        "    ent = ent * mask\n",
        "    return ent.sum() / (mask.sum() + eps)\n",
        "\n",
        "\n",
        "def effK_usage_loss(phase_p, mask=None, eps=1e-6):\n",
        "    if mask is None:\n",
        "        p_bar = phase_p.mean(dim=1)\n",
        "    else:\n",
        "        m = mask.to(dtype=phase_p.dtype, device=phase_p.device).unsqueeze(-1)\n",
        "        p_bar = (phase_p * m).sum(dim=1) / (m.sum(dim=1) + eps)\n",
        "\n",
        "    effK = 1.0 / (p_bar.pow(2).sum(dim=1) + eps)\n",
        "    return effK.mean(), effK.detach()\n",
        "\n",
        "\n",
        "def compute_phase_entropy_mean(phase_p_np, eps=1e-8):\n",
        "    phase_p_np = np.asarray(phase_p_np, dtype=np.float32)\n",
        "    ent_t = -(phase_p_np * np.log(phase_p_np + eps)).sum(axis=1)\n",
        "    return float(ent_t.mean())\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# Train\n",
        "# ---------------------------------------------------------------------\n",
        "def train_one_epoch(model, loader, optimizer, config, device):\n",
        "    model.train()\n",
        "    stats = {k: 0.0 for k in [\n",
        "        'loss', 'loss_rate', 'loss_recon', 'loss_smooth', 'loss_phase_ent', 'loss_effk',\n",
        "        'mae_count'\n",
        "    ]}\n",
        "\n",
        "    fs = config[\"fs\"]\n",
        "    tau = config.get(\"tau\", 1.0)\n",
        "\n",
        "    lam_recon = config.get(\"lambda_recon\", 1.0)\n",
        "    lam_smooth = config.get(\"lambda_smooth\", 0.05)\n",
        "    lam_phase_ent = config.get(\"lambda_phase_ent\", 0.01)\n",
        "    lam_effk = config.get(\"lambda_effk\", 0.005)\n",
        "\n",
        "    for batch in loader:\n",
        "        x = batch[\"data\"].to(device)\n",
        "        mask = batch[\"mask\"].to(device)\n",
        "        y_count = batch[\"count\"].to(device)\n",
        "        length = batch[\"length\"].to(device)\n",
        "\n",
        "        duration = torch.clamp(length / fs, min=1e-6)\n",
        "        y_rate = y_count / duration\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        rate_hat, z, x_hat, aux = model(x, mask, tau=tau)\n",
        "\n",
        "        loss_rate = F.mse_loss(rate_hat, y_rate)\n",
        "        loss_recon = masked_recon_mse(x_hat, x, mask)\n",
        "        loss_smooth = temporal_smoothness(aux[\"rep_rate_t\"], mask)\n",
        "        loss_phase_ent = phase_entropy_loss(aux[\"phase_p\"], mask)\n",
        "        loss_effk, _ = effK_usage_loss(aux[\"phase_p\"], mask)\n",
        "\n",
        "        loss = (loss_rate\n",
        "                + lam_recon * loss_recon\n",
        "                + lam_smooth * loss_smooth\n",
        "                + lam_phase_ent * loss_phase_ent\n",
        "                + lam_effk * loss_effk)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        count_hat = rate_hat * duration\n",
        "        stats['loss'] += loss.item()\n",
        "        stats['loss_rate'] += loss_rate.item()\n",
        "        stats['loss_recon'] += loss_recon.item()\n",
        "        stats['loss_smooth'] += loss_smooth.item()\n",
        "        stats['loss_phase_ent'] += loss_phase_ent.item()\n",
        "        stats['loss_effk'] += loss_effk.item()\n",
        "        stats['mae_count'] += torch.abs(count_hat - y_count).mean().item()\n",
        "\n",
        "    n = len(loader)\n",
        "    return {k: v / n for k, v in stats.items()}\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# Main (Scenario-wise LOSO)\n",
        "# ---------------------------------------------------------------------\n",
        "def main():\n",
        "    CONFIG = {\n",
        "        \"seed\": 42,\n",
        "        \"data_dir\": \"/content/drive/MyDrive/Colab Notebooks/HAR_data/MHEALTHDATASET\",\n",
        "\n",
        "        \"COLUMN_NAMES\": [\n",
        "            'acc_chest_x', 'acc_chest_y', 'acc_chest_z',\n",
        "            'ecg_1', 'ecg_2',\n",
        "            'acc_ankle_x', 'acc_ankle_y', 'acc_ankle_z',\n",
        "            'gyro_ankle_x', 'gyro_ankle_y', 'gyro_ankle_z',\n",
        "            'mag_ankle_x', 'mag_ankle_y', 'mag_ankle_z',\n",
        "            'acc_arm_x', 'acc_arm_y', 'acc_arm_z',\n",
        "            'gyro_arm_x', 'gyro_arm_y', 'gyro_arm_z',\n",
        "            'mag_arm_x', 'mag_arm_y', 'mag_arm_z',\n",
        "            'activity_id'\n",
        "        ],\n",
        "\n",
        "        \"TARGET_ACTIVITIES_MAP\": {\n",
        "            6:  'Waist bends forward',\n",
        "            7:  'Frontal elevation of arms',\n",
        "            8:  'Knees bending',\n",
        "            10:  'Jogging',\n",
        "            11: 'Running',\n",
        "            12: 'Jump front & back',\n",
        "        },\n",
        "\n",
        "        \"ACT_FEATURE_MAP\": {\n",
        "            6:  ['acc_chest_x','acc_chest_y','acc_chest_z','acc_ankle_x','acc_ankle_y','acc_ankle_z',\n",
        "                 'gyro_ankle_x','gyro_ankle_y','gyro_ankle_z','acc_arm_x','acc_arm_y','acc_arm_z',\n",
        "                 'gyro_arm_x','gyro_arm_y','gyro_arm_z'],\n",
        "            7:  ['acc_chest_x','acc_chest_y','acc_chest_z','acc_ankle_x','acc_ankle_y','acc_ankle_z',\n",
        "                 'gyro_ankle_x','gyro_ankle_y','gyro_ankle_z','acc_arm_x','acc_arm_y','acc_arm_z',\n",
        "                 'gyro_arm_x','gyro_arm_y','gyro_arm_z'],\n",
        "            8:  ['acc_chest_x','acc_chest_y','acc_chest_z','acc_ankle_x','acc_ankle_y','acc_ankle_z',\n",
        "                 'gyro_ankle_x','gyro_ankle_y','gyro_ankle_z','acc_arm_x','acc_arm_y','acc_arm_z',\n",
        "                 'gyro_arm_x','gyro_arm_y','gyro_arm_z'],\n",
        "            10: ['acc_chest_x','acc_chest_y','acc_chest_z','acc_ankle_x','acc_ankle_y','acc_ankle_z',\n",
        "                 'gyro_ankle_x','gyro_ankle_y','gyro_ankle_z','acc_arm_x','acc_arm_y','acc_arm_z',\n",
        "                 'gyro_arm_x','gyro_arm_y','gyro_arm_z'],\n",
        "            11: ['acc_chest_x','acc_chest_y','acc_chest_z','acc_ankle_x','acc_ankle_y','acc_ankle_z',\n",
        "                 'gyro_ankle_x','gyro_ankle_y','gyro_ankle_z','acc_arm_x','acc_arm_y','acc_arm_z',\n",
        "                 'gyro_arm_x','gyro_arm_y','gyro_arm_z'],\n",
        "            12: ['acc_chest_x','acc_chest_y','acc_chest_z','acc_ankle_x','acc_ankle_y','acc_ankle_z',\n",
        "                 'gyro_ankle_x','gyro_ankle_y','gyro_ankle_z','acc_arm_x','acc_arm_y','acc_arm_z',\n",
        "                 'gyro_arm_x','gyro_arm_y','gyro_arm_z'],\n",
        "        },\n",
        "\n",
        "        # Training Params\n",
        "        \"epochs\": 100,\n",
        "        \"lr\": 5e-4,\n",
        "        \"batch_size\": 64,\n",
        "        \"fs\": 50,\n",
        "\n",
        "        # Windowing Params\n",
        "        \"win_sec\": 8.0,\n",
        "        \"stride_sec\": 4.0,\n",
        "        \"drop_last\": True,\n",
        "\n",
        "        # Model\n",
        "        \"hidden_dim\": 128,\n",
        "        \"latent_dim\": 16,\n",
        "        \"K_max\": 6,\n",
        "\n",
        "        # Loss Weights\n",
        "        \"lambda_recon\": 1.0,\n",
        "        \"lambda_smooth\": 0.05,\n",
        "        \"lambda_phase_ent\": 0.01,\n",
        "        \"lambda_effk\": 0.0075,\n",
        "\n",
        "        \"tau\": 1.0,\n",
        "\n",
        "        \"GT_BY_ACT\": {\n",
        "            6:  {\"subject1\": 21, \"subject2\": 19, \"subject3\": 21, \"subject4\": 20, \"subject5\": 20,\n",
        "                 \"subject6\": 20, \"subject7\": 20, \"subject8\": 21, \"subject9\": 21, \"subject10\": 20},\n",
        "            7:  {\"subject1\": 20, \"subject2\": 20, \"subject3\": 20, \"subject4\": 20, \"subject5\": 20,\n",
        "                 \"subject6\": 20, \"subject7\": 20, \"subject8\": 19, \"subject9\": 19, \"subject10\": 20},\n",
        "            8:  {\"subject1\": 20, \"subject2\": 21, \"subject3\": 21, \"subject4\": 19, \"subject5\": 20,\n",
        "                 \"subject6\": 20, \"subject7\": 21, \"subject8\": 21, \"subject9\": 21, \"subject10\": 21},\n",
        "            10: {\"subject1\": 157, \"subject2\": 161, \"subject3\": 154, \"subject4\": 154, \"subject5\": 160,\n",
        "                 \"subject6\": 156, \"subject7\": 153, \"subject8\": 160, \"subject9\": 166, \"subject10\": 156},\n",
        "            11: {\"subject1\": 165, \"subject2\": 158, \"subject3\": 174, \"subject4\": 163, \"subject5\": 157,\n",
        "                 \"subject6\": 172, \"subject7\": 149, \"subject8\": 166, \"subject9\": 174, \"subject10\": 172},\n",
        "            12: {\"subject1\": 20, \"subject2\": 22, \"subject3\": 21, \"subject4\": 21, \"subject5\": 20,\n",
        "                 \"subject6\": 21, \"subject7\": 19, \"subject8\": 20, \"subject9\": 20, \"subject10\": 20},\n",
        "        },\n",
        "\n",
        "        \"MIX_SCENARIOS\": [\n",
        "            (\"Knees bending-Frontal elevation of arms\", 8, 7),\n",
        "            (\"Waist bends forward-Jump front & back\", 6, 12),\n",
        "            (\"Jogging-Running\", 10, 11),\n",
        "        ],\n",
        "    }\n",
        "\n",
        "    set_strict_seed(CONFIG[\"seed\"])\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Device: {device}\")\n",
        "\n",
        "    full_data = load_mhealth_dataset(CONFIG[\"data_dir\"], CONFIG[\"TARGET_ACTIVITIES_MAP\"], CONFIG[\"COLUMN_NAMES\"])\n",
        "    if not full_data:\n",
        "        return\n",
        "\n",
        "    subjects = [f\"subject{i}\" for i in range(1, 11)]\n",
        "\n",
        "    VIZ_DIR = \"scenario_viz\"\n",
        "    os.makedirs(VIZ_DIR, exist_ok=True)\n",
        "\n",
        "    # resample lengths (fixed for averaging)\n",
        "    NW = 60     # window-rate points per segment (A,B)\n",
        "    NL = 200    # latent points per segment (A,B)\n",
        "\n",
        "    for scen_idx, (scen_name, actA_id, actB_id) in enumerate(CONFIG.get(\"MIX_SCENARIOS\", []), start=1):\n",
        "        TRAIN_ACT_ID = actA_id\n",
        "\n",
        "        print(\"\\n\" + \"=\" * 100)\n",
        "        print(f\"1. 시나리오{scen_idx}: {scen_name}\")\n",
        "        print(\"=\" * 100)\n",
        "\n",
        "        loso_results = []\n",
        "        scenario_records = []\n",
        "        scenario_diffs = []\n",
        "        scenario_diffs_A = []\n",
        "        scenario_diffs_B = []\n",
        "\n",
        "        # ✅ buffers for TWO plots only\n",
        "        buf_win_concat = []     # (2*NW,)\n",
        "        buf_z_concat = []       # list of (Tmix,D)\n",
        "        per_subject_latent = [] # list of {\"zA\",\"zB\"}\n",
        "\n",
        "        for fold_idx, test_subj in enumerate(subjects):\n",
        "            set_strict_seed(CONFIG[\"seed\"])\n",
        "\n",
        "            gt_train_map = CONFIG[\"GT_BY_ACT\"][TRAIN_ACT_ID]\n",
        "            train_labels = [(s, TRAIN_ACT_ID, gt_train_map[s]) for s in subjects if s != test_subj]\n",
        "            test_labels  = [(test_subj, TRAIN_ACT_ID, gt_train_map[test_subj])]\n",
        "\n",
        "            train_trials = prepare_trial_list(train_labels, full_data, CONFIG[\"TARGET_ACTIVITIES_MAP\"], CONFIG[\"ACT_FEATURE_MAP\"])\n",
        "            test_trials  = prepare_trial_list(test_labels,  full_data, CONFIG[\"TARGET_ACTIVITIES_MAP\"], CONFIG[\"ACT_FEATURE_MAP\"])\n",
        "            if not test_trials or not train_trials:\n",
        "                continue\n",
        "\n",
        "            train_data = trial_list_to_windows(\n",
        "                train_trials, fs=CONFIG[\"fs\"],\n",
        "                win_sec=CONFIG[\"win_sec\"], stride_sec=CONFIG[\"stride_sec\"],\n",
        "                drop_last=CONFIG[\"drop_last\"]\n",
        "            )\n",
        "\n",
        "            g = torch.Generator()\n",
        "            g.manual_seed(CONFIG[\"seed\"])\n",
        "\n",
        "            train_loader = DataLoader(\n",
        "                TrialDataset(train_data),\n",
        "                batch_size=CONFIG[\"batch_size\"],\n",
        "                shuffle=True,\n",
        "                collate_fn=collate_variable_length,\n",
        "                generator=g,\n",
        "                num_workers=0\n",
        "            )\n",
        "\n",
        "            input_ch = train_data[0]['data'].shape[1]\n",
        "            model = KAutoCountModel(\n",
        "                input_ch=input_ch,\n",
        "                hidden_dim=CONFIG[\"hidden_dim\"],\n",
        "                latent_dim=CONFIG[\"latent_dim\"],\n",
        "                K_max=CONFIG[\"K_max\"]\n",
        "            ).to(device)\n",
        "\n",
        "            optimizer = torch.optim.Adam(model.parameters(), lr=CONFIG[\"lr\"])\n",
        "            scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.5)\n",
        "\n",
        "            for epoch in range(CONFIG[\"epochs\"]):\n",
        "                _ = train_one_epoch(model, train_loader, optimizer, CONFIG, device)\n",
        "                scheduler.step()\n",
        "\n",
        "            model.eval\n",
        "            model.eval()\n",
        "\n",
        "            # (A) LOSO on TRAIN_ACT_ID\n",
        "            item = test_trials[0]\n",
        "            x_np = item[\"data\"]\n",
        "            count_gt = float(item[\"count\"])\n",
        "            count_pred_win, _ = predict_count_by_windowing(\n",
        "                model, x_np=x_np,\n",
        "                fs=CONFIG[\"fs\"], win_sec=CONFIG[\"win_sec\"], stride_sec=CONFIG[\"stride_sec\"],\n",
        "                device=device, tau=CONFIG.get(\"tau\", 1.0),\n",
        "                batch_size=CONFIG.get(\"batch_size\", 64)\n",
        "            )\n",
        "            fold_mae = float(abs(count_pred_win - count_gt))\n",
        "            loso_results.append(fold_mae)\n",
        "\n",
        "            # (B) Scenario eval\n",
        "            mixed_item = build_mixed_ab_trial(test_subj, actA_id, actB_id, CONFIG, full_data)\n",
        "            if mixed_item is None:\n",
        "                continue\n",
        "\n",
        "            x_mix = mixed_item[\"data\"]\n",
        "            boundary = mixed_item[\"boundary\"]\n",
        "            gt_total = float(mixed_item[\"count\"])\n",
        "            md = mixed_item[\"meta_detail\"]\n",
        "\n",
        "            pred_total, _ = predict_count_by_windowing(\n",
        "                model, x_np=x_mix,\n",
        "                fs=CONFIG[\"fs\"], win_sec=CONFIG[\"win_sec\"], stride_sec=CONFIG[\"stride_sec\"],\n",
        "                device=device, tau=CONFIG.get(\"tau\", 1.0),\n",
        "                batch_size=CONFIG.get(\"batch_size\", 64)\n",
        "            )\n",
        "\n",
        "            diff = float(pred_total - gt_total)\n",
        "            mae = float(abs(diff))\n",
        "            scenario_diffs.append(diff)\n",
        "\n",
        "            # A/B split MAE\n",
        "            xA = mixed_item[\"data_A\"]\n",
        "            xB = mixed_item[\"data_B\"]\n",
        "            gtA = float(md[\"gtA\"])\n",
        "            gtB = float(md[\"gtB\"])\n",
        "\n",
        "            pred_A, _ = predict_count_by_windowing(\n",
        "                model, x_np=xA,\n",
        "                fs=CONFIG[\"fs\"], win_sec=CONFIG[\"win_sec\"], stride_sec=CONFIG[\"stride_sec\"],\n",
        "                device=device, tau=CONFIG.get(\"tau\", 1.0),\n",
        "                batch_size=CONFIG.get(\"batch_size\", 64)\n",
        "            )\n",
        "            pred_B, _ = predict_count_by_windowing(\n",
        "                model, x_np=xB,\n",
        "                fs=CONFIG[\"fs\"], win_sec=CONFIG[\"win_sec\"], stride_sec=CONFIG[\"stride_sec\"],\n",
        "                device=device, tau=CONFIG.get(\"tau\", 1.0),\n",
        "                batch_size=CONFIG.get(\"batch_size\", 64)\n",
        "            )\n",
        "\n",
        "            diff_A = float(pred_A - gtA)\n",
        "            diff_B = float(pred_B - gtB)\n",
        "            mae_A = float(abs(diff_A))\n",
        "            mae_B = float(abs(diff_B))\n",
        "            scenario_diffs_A.append(diff_A)\n",
        "            scenario_diffs_B.append(diff_B)\n",
        "\n",
        "            # full forward for z (latent) (+ keep entropy/k_hat log if you want)\n",
        "            x_tensor = torch.tensor(x_mix, dtype=torch.float32).transpose(0, 1).unsqueeze(0).to(device)\n",
        "            with torch.no_grad():\n",
        "                _, z_m, _, aux_m = model(x_tensor, mask=None, tau=CONFIG.get(\"tau\", 1.0))\n",
        "\n",
        "            phase_p_m = aux_m[\"phase_p\"].squeeze(0).detach().cpu().numpy()  # (Tmix,K)\n",
        "            k_hat_m = float(aux_m[\"k_hat\"].item())\n",
        "            ent_m = compute_phase_entropy_mean(phase_p_m)\n",
        "\n",
        "            z_td = z_m.squeeze(0).detach().cpu().numpy()  # (Tmix,D)\n",
        "\n",
        "            scenario_records.append({\n",
        "                \"subj\": test_subj,\n",
        "                \"line\": (\n",
        "                    f\"[Scenario] {scen_name} | {test_subj} | \"\n",
        "                    f\"{md['actA_name']}({md['gtA']:.0f}) -> {md['actB_name']}({md['gtB']:.0f}) | \"\n",
        "                    f\"GT_total={gt_total:.0f} | Pred_total(win)={pred_total:.2f} | Diff_total={diff:+.2f} | \"\n",
        "                    f\"MAE_total={mae:.2f} | \"\n",
        "                    f\"Pred_A={pred_A:.2f} (Diff_A={diff_A:+.2f}, MAE_A={mae_A:.2f}) | \"\n",
        "                    f\"Pred_B={pred_B:.2f} (Diff_B={diff_B:+.2f}, MAE_B={mae_B:.2f}) | \"\n",
        "                    f\"k_hat(full)={k_hat_m:.2f} | ent(full)={ent_m:.3f} | boundary={boundary}\"\n",
        "                )\n",
        "            })\n",
        "\n",
        "            # (1) window-level rep_rate curves: A/B separately -> resample -> concat\n",
        "            _, rA_w = get_window_rate_curve(\n",
        "                model, xA, fs=CONFIG[\"fs\"],\n",
        "                win_sec=CONFIG[\"win_sec\"], stride_sec=CONFIG[\"stride_sec\"],\n",
        "                device=device, tau=CONFIG.get(\"tau\", 1.0),\n",
        "                batch_size=CONFIG.get(\"batch_size\", 64)\n",
        "            )\n",
        "            _, rB_w = get_window_rate_curve(\n",
        "                model, xB, fs=CONFIG[\"fs\"],\n",
        "                win_sec=CONFIG[\"win_sec\"], stride_sec=CONFIG[\"stride_sec\"],\n",
        "                device=device, tau=CONFIG.get(\"tau\", 1.0),\n",
        "                batch_size=CONFIG.get(\"batch_size\", 64)\n",
        "            )\n",
        "            rA_rs = resample_1d(rA_w, NW)\n",
        "            rB_rs = resample_1d(rB_w, NW)\n",
        "            buf_win_concat.append(np.concatenate([rA_rs, rB_rs], axis=0))\n",
        "\n",
        "            # (2) latent: store raw z split for global PCA later\n",
        "            zA = z_td[:boundary, :]\n",
        "            zB = z_td[boundary:, :]\n",
        "            buf_z_concat.append(z_td)\n",
        "            per_subject_latent.append({\"zA\": zA, \"zB\": zB})\n",
        "\n",
        "        # -------------------------\n",
        "        # Print summaries (same as your original)\n",
        "        # -------------------------\n",
        "        print(\"-\" * 100)\n",
        "        if len(loso_results) > 0:\n",
        "            print(\"TRain 결과 ->\")\n",
        "            print(\"-\" * 100)\n",
        "            print(f\" >>> Final LOSO Result (Average MAE): {np.mean(loso_results):.3f}\")\n",
        "            print(f\" >>> Standard Deviation: {np.std(loso_results):.3f}\")\n",
        "            print(\"-\" * 100)\n",
        "        else:\n",
        "            print(\"TRain 결과 -> (no folds computed)\")\n",
        "            print(\"-\" * 100)\n",
        "\n",
        "        subj2line = {r[\"subj\"]: r[\"line\"] for r in scenario_records}\n",
        "        for s in subjects:\n",
        "            if s in subj2line:\n",
        "                print(subj2line[s])\n",
        "\n",
        "        print(\"\\n\" + \"-\" * 100)\n",
        "        print(f\"시나리오{scen_idx} 결과\")\n",
        "        print(\"-\" * 100)\n",
        "\n",
        "        if len(scenario_diffs) == 0:\n",
        "            print(f\"[{scen_name}] No samples (all skipped). Fill GT_BY_ACT for required activities.\")\n",
        "        else:\n",
        "            diffs = np.array(scenario_diffs, dtype=np.float32)\n",
        "            mae = float(np.mean(np.abs(diffs)))\n",
        "            rmse = float(np.sqrt(np.mean(diffs ** 2)))\n",
        "            print(f\"[TOTAL] [{scen_name}] N={len(diffs):2d} | MAE={mae:.3f} | RMSE={rmse:.3f} | mean(diff)={diffs.mean():+.3f} | std(diff)={diffs.std():.3f}\")\n",
        "\n",
        "            diffsA = np.array(scenario_diffs_A, dtype=np.float32)\n",
        "            maeA = float(np.mean(np.abs(diffsA)))\n",
        "            rmseA = float(np.sqrt(np.mean(diffsA ** 2)))\n",
        "            print(f\"[A]     [{scen_name}] N={len(diffsA):2d} | MAE={maeA:.3f} | RMSE={rmseA:.3f} | mean(diff)={diffsA.mean():+.3f} | std(diff)={diffsA.std():.3f}\")\n",
        "\n",
        "            diffsB = np.array(scenario_diffs_B, dtype=np.float32)\n",
        "            maeB = float(np.mean(np.abs(diffsB)))\n",
        "            rmseB = float(np.sqrt(np.mean(diffsB ** 2)))\n",
        "            print(f\"[B]     [{scen_name}] N={len(diffsB):2d} | MAE={maeB:.3f} | RMSE={rmseB:.3f} | mean(diff)={diffsB.mean():+.3f} | std(diff)={diffsB.std():.3f}\")\n",
        "\n",
        "        print(\"-\" * 100)\n",
        "\n",
        "        # -------------------------\n",
        "        # ✅ TWO-PLOT scenario-avg visualization\n",
        "        # -------------------------\n",
        "        n_ok = len(buf_win_concat)\n",
        "        if n_ok == 0:\n",
        "            print(\"[Viz Skip] No scenario samples collected for averaging.\")\n",
        "            continue\n",
        "\n",
        "        # (1) window-level mean±std on normalized axis [0,2]\n",
        "        W = np.stack(buf_win_concat, axis=0)  # (N, 2*NW)\n",
        "        win_mean = W.mean(axis=0)\n",
        "        win_std  = W.std(axis=0)\n",
        "        win_t = np.linspace(0.0, 2.0, num=2*NW, dtype=np.float32)\n",
        "\n",
        "        # (2) latent: global PCA basis from all z\n",
        "        Z_all = np.concatenate(buf_z_concat, axis=0)  # (sumT, D)\n",
        "        mu, V2 = global_pca2(Z_all)\n",
        "\n",
        "        # project each subject, resample A/B to NL then concat -> (2*NL,2)\n",
        "        lat_list = []\n",
        "        for d in per_subject_latent:\n",
        "            zA = d[\"zA\"]; zB = d[\"zB\"]\n",
        "            pcA = (zA - mu) @ V2.T\n",
        "            pcB = (zB - mu) @ V2.T\n",
        "            pcA_rs = resample_2d(pcA, NL)\n",
        "            pcB_rs = resample_2d(pcB, NL)\n",
        "            lat_list.append(np.concatenate([pcA_rs, pcB_rs], axis=0))\n",
        "\n",
        "        LAT = np.stack(lat_list, axis=0)  # (N,2*NL,2)\n",
        "        lat_xy_mean = LAT.mean(axis=0)\n",
        "        lat_xy_std  = LAT.std(axis=0)\n",
        "\n",
        "        plot_scenario_mean_std_two_plots(\n",
        "            save_dir=VIZ_DIR,\n",
        "            scen_name=scen_name,\n",
        "            win_t=win_t, win_mean=win_mean, win_std=win_std,\n",
        "            lat_xy_mean=lat_xy_mean, lat_xy_std=lat_xy_std,\n",
        "            n_subjects=n_ok,\n",
        "            dpi=600\n",
        "        )\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nfucjE5Q7nHH",
        "outputId": "ad9e0fae-6096-4329-b0f8-31f3385a7990"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n",
            "Loading 10 subjects from /content/drive/MyDrive/Colab Notebooks/HAR_data/MHEALTHDATASET...\n",
            "\n",
            "====================================================================================================\n",
            "1. 시나리오1: Knees bending-Frontal elevation of arms\n",
            "====================================================================================================\n",
            "----------------------------------------------------------------------------------------------------\n",
            "TRain 결과 ->\n",
            "----------------------------------------------------------------------------------------------------\n",
            " >>> Final LOSO Result (Average MAE): 4.274\n",
            " >>> Standard Deviation: 3.012\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[Scenario] Knees bending-Frontal elevation of arms | subject1 | Knees bending(20) -> Frontal elevation of arms(20) | GT_total=40 | Pred_total(win)=66.34 | Diff_total=+26.34 | MAE_total=26.34 | Pred_A=47.42 (Diff_A=+27.42, MAE_A=27.42) | Pred_B=18.71 (Diff_B=-1.29, MAE_B=1.29) | k_hat(full)=1.02 | ent(full)=0.059 | boundary=3379\n",
            "[Scenario] Knees bending-Frontal elevation of arms | subject2 | Knees bending(21) -> Frontal elevation of arms(20) | GT_total=41 | Pred_total(win)=42.73 | Diff_total=+1.73 | MAE_total=1.73 | Pred_A=24.43 (Diff_A=+3.43, MAE_A=3.43) | Pred_B=18.26 (Diff_B=-1.74, MAE_B=1.74) | k_hat(full)=1.01 | ent(full)=0.041 | boundary=3430\n",
            "[Scenario] Knees bending-Frontal elevation of arms | subject3 | Knees bending(21) -> Frontal elevation of arms(20) | GT_total=41 | Pred_total(win)=30.07 | Diff_total=-10.93 | MAE_total=10.93 | Pred_A=14.94 (Diff_A=-6.06, MAE_A=6.06) | Pred_B=15.02 (Diff_B=-4.98, MAE_B=4.98) | k_hat(full)=1.01 | ent(full)=0.036 | boundary=3175\n",
            "[Scenario] Knees bending-Frontal elevation of arms | subject4 | Knees bending(19) -> Frontal elevation of arms(20) | GT_total=39 | Pred_total(win)=48.13 | Diff_total=+9.13 | MAE_total=9.13 | Pred_A=26.86 (Diff_A=+7.86, MAE_A=7.86) | Pred_B=21.45 (Diff_B=+1.45, MAE_B=1.45) | k_hat(full)=1.60 | ent(full)=0.163 | boundary=3123\n",
            "[Scenario] Knees bending-Frontal elevation of arms | subject5 | Knees bending(20) -> Frontal elevation of arms(20) | GT_total=40 | Pred_total(win)=31.99 | Diff_total=-8.01 | MAE_total=8.01 | Pred_A=14.43 (Diff_A=-5.57, MAE_A=5.57) | Pred_B=17.82 (Diff_B=-2.18, MAE_B=2.18) | k_hat(full)=1.02 | ent(full)=0.050 | boundary=2714\n",
            "[Scenario] Knees bending-Frontal elevation of arms | subject6 | Knees bending(20) -> Frontal elevation of arms(20) | GT_total=40 | Pred_total(win)=25.42 | Diff_total=-14.58 | MAE_total=14.58 | Pred_A=13.74 (Diff_A=-6.26, MAE_A=6.26) | Pred_B=11.71 (Diff_B=-8.29, MAE_B=8.29) | k_hat(full)=1.01 | ent(full)=0.033 | boundary=2304\n",
            "[Scenario] Knees bending-Frontal elevation of arms | subject7 | Knees bending(21) -> Frontal elevation of arms(20) | GT_total=41 | Pred_total(win)=36.12 | Diff_total=-4.88 | MAE_total=4.88 | Pred_A=12.28 (Diff_A=-8.72, MAE_A=8.72) | Pred_B=24.25 (Diff_B=+4.25, MAE_B=4.25) | k_hat(full)=1.00 | ent(full)=0.012 | boundary=2816\n",
            "[Scenario] Knees bending-Frontal elevation of arms | subject8 | Knees bending(21) -> Frontal elevation of arms(19) | GT_total=40 | Pred_total(win)=35.73 | Diff_total=-4.27 | MAE_total=4.27 | Pred_A=12.78 (Diff_A=-8.22, MAE_A=8.22) | Pred_B=23.25 (Diff_B=+4.25, MAE_B=4.25) | k_hat(full)=1.01 | ent(full)=0.036 | boundary=2560\n",
            "[Scenario] Knees bending-Frontal elevation of arms | subject9 | Knees bending(21) -> Frontal elevation of arms(19) | GT_total=40 | Pred_total(win)=36.83 | Diff_total=-3.17 | MAE_total=3.17 | Pred_A=12.01 (Diff_A=-8.99, MAE_A=8.99) | Pred_B=24.88 (Diff_B=+5.88, MAE_B=5.88) | k_hat(full)=1.01 | ent(full)=0.017 | boundary=2969\n",
            "[Scenario] Knees bending-Frontal elevation of arms | subject10 | Knees bending(21) -> Frontal elevation of arms(20) | GT_total=41 | Pred_total(win)=32.91 | Diff_total=-8.09 | MAE_total=8.09 | Pred_A=11.21 (Diff_A=-9.79, MAE_A=9.79) | Pred_B=21.97 (Diff_B=+1.97, MAE_B=1.97) | k_hat(full)=1.01 | ent(full)=0.018 | boundary=2867\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "시나리오1 결과\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[TOTAL] [Knees bending-Frontal elevation of arms] N=10 | MAE=9.113 | RMSE=11.375 | mean(diff)=-1.672 | std(diff)=11.251\n",
            "[A]     [Knees bending-Frontal elevation of arms] N=10 | MAE=9.234 | RMSE=11.193 | mean(diff)=-1.490 | std(diff)=11.094\n",
            "[B]     [Knees bending-Frontal elevation of arms] N=10 | MAE=3.629 | RMSE=4.240 | mean(diff)=-0.068 | std(diff)=4.240\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "====================================================================================================\n",
            "1. 시나리오2: Waist bends forward-Jump front & back\n",
            "====================================================================================================\n",
            "----------------------------------------------------------------------------------------------------\n",
            "TRain 결과 ->\n",
            "----------------------------------------------------------------------------------------------------\n",
            " >>> Final LOSO Result (Average MAE): 5.261\n",
            " >>> Standard Deviation: 3.006\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[Scenario] Waist bends forward-Jump front & back | subject1 | Waist bends forward(21) -> Jump front & back(20) | GT_total=41 | Pred_total(win)=26.26 | Diff_total=-14.74 | MAE_total=14.74 | Pred_A=13.11 (Diff_A=-7.89, MAE_A=7.89) | Pred_B=15.15 (Diff_B=-4.85, MAE_B=4.85) | k_hat(full)=1.10 | ent(full)=0.214 | boundary=3072\n",
            "[Scenario] Waist bends forward-Jump front & back | subject2 | Waist bends forward(19) -> Jump front & back(22) | GT_total=41 | Pred_total(win)=31.27 | Diff_total=-9.73 | MAE_total=9.73 | Pred_A=16.44 (Diff_A=-2.56, MAE_A=2.56) | Pred_B=18.16 (Diff_B=-3.84, MAE_B=3.84) | k_hat(full)=1.07 | ent(full)=0.155 | boundary=3174\n",
            "[Scenario] Waist bends forward-Jump front & back | subject3 | Waist bends forward(21) -> Jump front & back(21) | GT_total=42 | Pred_total(win)=39.70 | Diff_total=-2.30 | MAE_total=2.30 | Pred_A=15.13 (Diff_A=-5.87, MAE_A=5.87) | Pred_B=26.94 (Diff_B=+5.94, MAE_B=5.94) | k_hat(full)=1.04 | ent(full)=0.101 | boundary=3226\n",
            "[Scenario] Waist bends forward-Jump front & back | subject4 | Waist bends forward(20) -> Jump front & back(21) | GT_total=41 | Pred_total(win)=29.09 | Diff_total=-11.91 | MAE_total=11.91 | Pred_A=14.76 (Diff_A=-5.24, MAE_A=5.24) | Pred_B=16.69 (Diff_B=-4.31, MAE_B=4.31) | k_hat(full)=1.05 | ent(full)=0.121 | boundary=3328\n",
            "[Scenario] Waist bends forward-Jump front & back | subject5 | Waist bends forward(20) -> Jump front & back(20) | GT_total=40 | Pred_total(win)=37.98 | Diff_total=-2.02 | MAE_total=2.02 | Pred_A=20.05 (Diff_A=+0.05, MAE_A=0.05) | Pred_B=20.57 (Diff_B=+0.57, MAE_B=0.57) | k_hat(full)=1.06 | ent(full)=0.139 | boundary=2765\n",
            "[Scenario] Waist bends forward-Jump front & back | subject6 | Waist bends forward(20) -> Jump front & back(21) | GT_total=41 | Pred_total(win)=23.48 | Diff_total=-17.52 | MAE_total=17.52 | Pred_A=9.06 (Diff_A=-10.94, MAE_A=10.94) | Pred_B=14.99 (Diff_B=-6.01, MAE_B=6.01) | k_hat(full)=1.01 | ent(full)=0.043 | boundary=2202\n",
            "[Scenario] Waist bends forward-Jump front & back | subject7 | Waist bends forward(20) -> Jump front & back(19) | GT_total=39 | Pred_total(win)=39.25 | Diff_total=+0.25 | MAE_total=0.25 | Pred_A=17.74 (Diff_A=-2.26, MAE_A=2.26) | Pred_B=23.68 (Diff_B=+4.68, MAE_B=4.68) | k_hat(full)=1.09 | ent(full)=0.190 | boundary=3072\n",
            "[Scenario] Waist bends forward-Jump front & back | subject8 | Waist bends forward(21) -> Jump front & back(20) | GT_total=41 | Pred_total(win)=27.09 | Diff_total=-13.91 | MAE_total=13.91 | Pred_A=9.27 (Diff_A=-11.73, MAE_A=11.73) | Pred_B=20.10 (Diff_B=+0.10, MAE_B=0.10) | k_hat(full)=1.01 | ent(full)=0.037 | boundary=2151\n",
            "[Scenario] Waist bends forward-Jump front & back | subject9 | Waist bends forward(21) -> Jump front & back(20) | GT_total=41 | Pred_total(win)=35.21 | Diff_total=-5.79 | MAE_total=5.79 | Pred_A=15.25 (Diff_A=-5.75, MAE_A=5.75) | Pred_B=22.36 (Diff_B=+2.36, MAE_B=2.36) | k_hat(full)=1.01 | ent(full)=0.022 | boundary=2867\n",
            "[Scenario] Waist bends forward-Jump front & back | subject10 | Waist bends forward(20) -> Jump front & back(20) | GT_total=40 | Pred_total(win)=27.36 | Diff_total=-12.64 | MAE_total=12.64 | Pred_A=13.27 (Diff_A=-6.73, MAE_A=6.73) | Pred_B=15.24 (Diff_B=-4.76, MAE_B=4.76) | k_hat(full)=1.03 | ent(full)=0.073 | boundary=2458\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "시나리오2 결과\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[TOTAL] [Waist bends forward-Jump front & back] N=10 | MAE=9.080 | RMSE=10.756 | mean(diff)=-9.030 | std(diff)=5.843\n",
            "[A]     [Waist bends forward-Jump front & back] N=10 | MAE=5.902 | RMSE=6.867 | mean(diff)=-5.893 | std(diff)=3.526\n",
            "[B]     [Waist bends forward-Jump front & back] N=10 | MAE=3.742 | RMSE=4.227 | mean(diff)=-1.012 | std(diff)=4.105\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "====================================================================================================\n",
            "1. 시나리오3: Jogging-Running\n",
            "====================================================================================================\n",
            "----------------------------------------------------------------------------------------------------\n",
            "TRain 결과 ->\n",
            "----------------------------------------------------------------------------------------------------\n",
            " >>> Final LOSO Result (Average MAE): 10.421\n",
            " >>> Standard Deviation: 10.920\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[Scenario] Jogging-Running | subject1 | Jogging(157) -> Running(165) | GT_total=322 | Pred_total(win)=321.54 | Diff_total=-0.46 | MAE_total=0.46 | Pred_A=121.58 (Diff_A=-35.42, MAE_A=35.42) | Pred_B=201.85 (Diff_B=+36.85, MAE_B=36.85) | k_hat(full)=1.23 | ent(full)=0.311 | boundary=3072\n",
            "[Scenario] Jogging-Running | subject2 | Jogging(161) -> Running(158) | GT_total=319 | Pred_total(win)=338.97 | Diff_total=+19.97 | MAE_total=19.97 | Pred_A=148.89 (Diff_A=-12.11, MAE_A=12.11) | Pred_B=189.01 (Diff_B=+31.01, MAE_B=31.01) | k_hat(full)=1.73 | ent(full)=0.485 | boundary=3072\n",
            "[Scenario] Jogging-Running | subject3 | Jogging(154) -> Running(174) | GT_total=328 | Pred_total(win)=335.16 | Diff_total=+7.16 | MAE_total=7.16 | Pred_A=154.89 (Diff_A=+0.89, MAE_A=0.89) | Pred_B=182.06 (Diff_B=+8.06, MAE_B=8.06) | k_hat(full)=1.34 | ent(full)=0.315 | boundary=3072\n",
            "[Scenario] Jogging-Running | subject4 | Jogging(154) -> Running(163) | GT_total=317 | Pred_total(win)=371.57 | Diff_total=+54.57 | MAE_total=54.57 | Pred_A=184.04 (Diff_A=+30.04, MAE_A=30.04) | Pred_B=187.40 (Diff_B=+24.40, MAE_B=24.40) | k_hat(full)=1.28 | ent(full)=0.278 | boundary=3072\n",
            "[Scenario] Jogging-Running | subject5 | Jogging(160) -> Running(157) | GT_total=317 | Pred_total(win)=357.05 | Diff_total=+40.05 | MAE_total=40.05 | Pred_A=175.40 (Diff_A=+15.40, MAE_A=15.40) | Pred_B=181.58 (Diff_B=+24.58, MAE_B=24.58) | k_hat(full)=1.41 | ent(full)=0.330 | boundary=3072\n",
            "[Scenario] Jogging-Running | subject6 | Jogging(156) -> Running(172) | GT_total=328 | Pred_total(win)=373.09 | Diff_total=+45.09 | MAE_total=45.09 | Pred_A=150.75 (Diff_A=-5.25, MAE_A=5.25) | Pred_B=225.75 (Diff_B=+53.75, MAE_B=53.75) | k_hat(full)=1.46 | ent(full)=0.359 | boundary=3072\n",
            "[Scenario] Jogging-Running | subject7 | Jogging(153) -> Running(149) | GT_total=302 | Pred_total(win)=344.47 | Diff_total=+42.47 | MAE_total=42.47 | Pred_A=169.21 (Diff_A=+16.21, MAE_A=16.21) | Pred_B=174.35 (Diff_B=+25.35, MAE_B=25.35) | k_hat(full)=1.45 | ent(full)=0.356 | boundary=3072\n",
            "[Scenario] Jogging-Running | subject8 | Jogging(160) -> Running(166) | GT_total=326 | Pred_total(win)=319.89 | Diff_total=-6.11 | MAE_total=6.11 | Pred_A=121.44 (Diff_A=-38.56, MAE_A=38.56) | Pred_B=201.68 (Diff_B=+35.68, MAE_B=35.68) | k_hat(full)=1.47 | ent(full)=0.433 | boundary=3072\n",
            "[Scenario] Jogging-Running | subject9 | Jogging(166) -> Running(174) | GT_total=340 | Pred_total(win)=381.31 | Diff_total=+41.31 | MAE_total=41.31 | Pred_A=178.65 (Diff_A=+12.65, MAE_A=12.65) | Pred_B=203.41 (Diff_B=+29.41, MAE_B=29.41) | k_hat(full)=1.61 | ent(full)=0.324 | boundary=3072\n",
            "[Scenario] Jogging-Running | subject10 | Jogging(156) -> Running(172) | GT_total=328 | Pred_total(win)=392.60 | Diff_total=+64.60 | MAE_total=64.60 | Pred_A=189.32 (Diff_A=+33.32, MAE_A=33.32) | Pred_B=203.67 (Diff_B=+31.67, MAE_B=31.67) | k_hat(full)=1.39 | ent(full)=0.330 | boundary=3072\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "시나리오3 결과\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[TOTAL] [Jogging-Running] N=10 | MAE=32.179 | RMSE=38.452 | mean(diff)=+30.864 | std(diff)=22.934\n",
            "[A]     [Jogging-Running] N=10 | MAE=19.986 | RMSE=23.642 | mean(diff)=+1.715 | std(diff)=23.580\n",
            "[B]     [Jogging-Running] N=10 | MAE=30.075 | RMSE=32.026 | mean(diff)=+30.075 | std(diff)=11.007\n",
            "----------------------------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns  # kept\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.ndimage import gaussian_filter1d  # kept\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# ✅ Save helpers (needed)\n",
        "# ---------------------------------------------------------------------\n",
        "def _ensure_dir(path: str):\n",
        "    if path is None or path == \"\":\n",
        "        return\n",
        "    os.makedirs(path, exist_ok=True)\n",
        "\n",
        "def _safe_fname(s: str):\n",
        "    s = str(s)\n",
        "    for ch in ['/', '\\\\', ':', '*', '?', '\"', '<', '>', '|', '\\n', '\\t']:\n",
        "        s = s.replace(ch, '_')\n",
        "    s = s.strip().replace(' ', '_')\n",
        "    while '__' in s:\n",
        "        s = s.replace('__', '_')\n",
        "    return s\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 1) Strict Seeding\n",
        "# ---------------------------------------------------------------------\n",
        "def set_strict_seed(seed: int):\n",
        "    random.seed(seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 2) Data Loading\n",
        "# ---------------------------------------------------------------------\n",
        "def load_mhealth_dataset(data_dir, target_activities_map, column_names):\n",
        "    full_dataset = {}\n",
        "    file_list = sorted(glob.glob(os.path.join(data_dir, \"mHealth_subject*.log\")))\n",
        "\n",
        "    if not file_list:\n",
        "        print(f\"[Warning] No mHealth logs found in {data_dir}\")\n",
        "        return {}\n",
        "\n",
        "    print(f\"Loading {len(file_list)} subjects from {data_dir}...\")\n",
        "\n",
        "    for file_path in file_list:\n",
        "        file_name = os.path.basename(file_path)\n",
        "        subj_part = file_name.split('.')[0]\n",
        "        try:\n",
        "            subj_id_num = int(''.join(filter(str.isdigit, subj_part)))\n",
        "            subj_key = f\"subject{subj_id_num}\"\n",
        "        except:\n",
        "            subj_key = subj_part\n",
        "\n",
        "        try:\n",
        "            df = pd.read_csv(file_path, sep=\"\\t\", header=None)\n",
        "            df = df.iloc[:, :len(column_names)]\n",
        "            df.columns = column_names\n",
        "\n",
        "            subj_data = {}\n",
        "            for label_code, activity_name in target_activities_map.items():\n",
        "                activity_df = df[df['activity_id'] == label_code].copy()\n",
        "                if not activity_df.empty:\n",
        "                    subj_data[activity_name] = activity_df.drop(columns=['activity_id'])\n",
        "\n",
        "            full_dataset[subj_key] = subj_data\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {file_name}: {e}\")\n",
        "            pass\n",
        "\n",
        "    return full_dataset\n",
        "\n",
        "\n",
        "def prepare_trial_list(label_config, full_data, target_map, feature_map):\n",
        "    trial_list = []\n",
        "    for subj, act_id, gt_count in label_config:\n",
        "        act_name = target_map.get(act_id)\n",
        "        feats = feature_map.get(act_id)\n",
        "\n",
        "        if subj in full_data and act_name in full_data[subj]:\n",
        "            raw_df = full_data[subj][act_name][feats]\n",
        "            raw_np = raw_df.values.astype(np.float32)\n",
        "\n",
        "            # Z-score normalize per-trial\n",
        "            mean = raw_np.mean(axis=0)\n",
        "            std = raw_np.std(axis=0) + 1e-6\n",
        "            norm_np = (raw_np - mean) / std\n",
        "\n",
        "            trial_list.append({\n",
        "                'data': norm_np,              # (T, C)\n",
        "                'count': float(gt_count),      # trial total count\n",
        "                'meta': f\"{subj}_{act_name}\"\n",
        "            })\n",
        "        else:\n",
        "            print(f\"[Skip] Missing data for {subj} - {act_name}\")\n",
        "\n",
        "    return trial_list\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# ✅ Mixed builder helpers\n",
        "# ---------------------------------------------------------------------\n",
        "def get_single_trial_from_full_data(subj, act_id, gt_count, full_data, target_map, feature_map, normalize=True):\n",
        "    act_name = target_map.get(act_id)\n",
        "    feats = feature_map.get(act_id)\n",
        "\n",
        "    if subj not in full_data or act_name not in full_data[subj]:\n",
        "        return None\n",
        "\n",
        "    raw_df = full_data[subj][act_name][feats]\n",
        "    x = raw_df.values.astype(np.float32)\n",
        "\n",
        "    if normalize:\n",
        "        mean = x.mean(axis=0)\n",
        "        std = x.std(axis=0) + 1e-6\n",
        "        x = (x - mean) / std\n",
        "\n",
        "    return {\n",
        "        \"data\": x,\n",
        "        \"count\": float(gt_count),\n",
        "        \"meta\": f\"{subj}_{act_name}\"\n",
        "    }\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# ✅ Mixed A-B builder (kept; not used in ABAB run but left intact)\n",
        "# ---------------------------------------------------------------------\n",
        "def build_mixed_ab_trial(subj, actA_id, actB_id, config, full_data):\n",
        "    gt_map = config.get(\"GT_BY_ACT\", {})\n",
        "    if actA_id not in gt_map or actB_id not in gt_map:\n",
        "        return None\n",
        "    if subj not in gt_map[actA_id] or subj not in gt_map[actB_id]:\n",
        "        return None\n",
        "\n",
        "    gtA = float(gt_map[actA_id][subj])\n",
        "    gtB = float(gt_map[actB_id][subj])\n",
        "    gt_total = gtA + gtB\n",
        "\n",
        "    A = get_single_trial_from_full_data(\n",
        "        subj, actA_id, gtA, full_data,\n",
        "        config[\"TARGET_ACTIVITIES_MAP\"], config[\"ACT_FEATURE_MAP\"],\n",
        "        normalize=False\n",
        "    )\n",
        "    B = get_single_trial_from_full_data(\n",
        "        subj, actB_id, gtB, full_data,\n",
        "        config[\"TARGET_ACTIVITIES_MAP\"], config[\"ACT_FEATURE_MAP\"],\n",
        "        normalize=False\n",
        "    )\n",
        "    if A is None or B is None:\n",
        "        return None\n",
        "\n",
        "    xA_raw = A[\"data\"]\n",
        "    xB_raw = B[\"data\"]\n",
        "    boundary = int(xA_raw.shape[0])\n",
        "\n",
        "    x_mix_raw = np.concatenate([xA_raw, xB_raw], axis=0).astype(np.float32)\n",
        "\n",
        "    mean = x_mix_raw.mean(axis=0)\n",
        "    std = x_mix_raw.std(axis=0) + 1e-6\n",
        "    x_mix = (x_mix_raw - mean) / std\n",
        "\n",
        "    xA = (xA_raw - mean) / std\n",
        "    xB = (xB_raw - mean) / std\n",
        "\n",
        "    actA_name = config[\"TARGET_ACTIVITIES_MAP\"].get(actA_id, str(actA_id))\n",
        "    actB_name = config[\"TARGET_ACTIVITIES_MAP\"].get(actB_id, str(actB_id))\n",
        "\n",
        "    return {\n",
        "        \"data\": x_mix,\n",
        "        \"count\": float(gt_total),\n",
        "        \"meta\": f\"{subj}__{actA_name}__TO__{actB_name}\",\n",
        "        \"boundary\": boundary,\n",
        "        \"meta_detail\": {\n",
        "            \"subj\": subj,\n",
        "            \"actA_id\": actA_id, \"actB_id\": actB_id,\n",
        "            \"actA_name\": actA_name, \"actB_name\": actB_name,\n",
        "            \"gtA\": gtA, \"gtB\": gtB,\n",
        "            \"gt_total\": gt_total,\n",
        "        },\n",
        "        \"data_A\": xA,\n",
        "        \"data_B\": xB,\n",
        "        \"T_A\": int(xA.shape[0]),\n",
        "        \"T_B\": int(xB.shape[0]),\n",
        "    }\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# ✅ Mixed A-B-A-B builder\n",
        "#     - raw concat: A | B | A | B\n",
        "#     - mixed 전체로 1회 z-score\n",
        "#     - boundaries: [TA, TA+TB, TA+TB+TA]\n",
        "#     - GT_total = 2*gtA + 2*gtB\n",
        "# ---------------------------------------------------------------------\n",
        "def build_mixed_abab_trial(subj, actA_id, actB_id, config, full_data):\n",
        "    gt_map = config.get(\"GT_BY_ACT\", {})\n",
        "    if actA_id not in gt_map or actB_id not in gt_map:\n",
        "        return None\n",
        "    if subj not in gt_map[actA_id] or subj not in gt_map[actB_id]:\n",
        "        return None\n",
        "\n",
        "    gtA = float(gt_map[actA_id][subj])\n",
        "    gtB = float(gt_map[actB_id][subj])\n",
        "    gt_total = 2.0 * gtA + 2.0 * gtB\n",
        "\n",
        "    A = get_single_trial_from_full_data(\n",
        "        subj, actA_id, gtA, full_data,\n",
        "        config[\"TARGET_ACTIVITIES_MAP\"], config[\"ACT_FEATURE_MAP\"],\n",
        "        normalize=False\n",
        "    )\n",
        "    B = get_single_trial_from_full_data(\n",
        "        subj, actB_id, gtB, full_data,\n",
        "        config[\"TARGET_ACTIVITIES_MAP\"], config[\"ACT_FEATURE_MAP\"],\n",
        "        normalize=False\n",
        "    )\n",
        "    if A is None or B is None:\n",
        "        return None\n",
        "\n",
        "    xA_raw = A[\"data\"]\n",
        "    xB_raw = B[\"data\"]\n",
        "    TA = int(xA_raw.shape[0])\n",
        "    TB = int(xB_raw.shape[0])\n",
        "\n",
        "    b1 = TA\n",
        "    b2 = TA + TB\n",
        "    b3 = TA + TB + TA\n",
        "\n",
        "    x_mix_raw = np.concatenate([xA_raw, xB_raw, xA_raw, xB_raw], axis=0).astype(np.float32)\n",
        "\n",
        "    mean = x_mix_raw.mean(axis=0)\n",
        "    std = x_mix_raw.std(axis=0) + 1e-6\n",
        "    x_mix = (x_mix_raw - mean) / std\n",
        "\n",
        "    xA1 = (xA_raw - mean) / std\n",
        "    xB1 = (xB_raw - mean) / std\n",
        "    xA2 = xA1.copy()\n",
        "    xB2 = xB1.copy()\n",
        "\n",
        "    actA_name = config[\"TARGET_ACTIVITIES_MAP\"].get(actA_id, str(actA_id))\n",
        "    actB_name = config[\"TARGET_ACTIVITIES_MAP\"].get(actB_id, str(actB_id))\n",
        "\n",
        "    return {\n",
        "        \"data\": x_mix,                   # (TA+TB+TA+TB, C)\n",
        "        \"count\": float(gt_total),\n",
        "        \"meta\": f\"{subj}__{actA_name}__TO__{actB_name}__TO__{actA_name}__TO__{actB_name}\",\n",
        "        \"boundaries\": [b1, b2, b3],\n",
        "        \"meta_detail\": {\n",
        "            \"subj\": subj,\n",
        "            \"actA_id\": actA_id, \"actB_id\": actB_id,\n",
        "            \"actA_name\": actA_name, \"actB_name\": actB_name,\n",
        "            \"gtA\": gtA, \"gtB\": gtB,\n",
        "            \"gt_total\": gt_total,\n",
        "        },\n",
        "        \"data_A1\": xA1,\n",
        "        \"data_B1\": xB1,\n",
        "        \"data_A2\": xA2,\n",
        "        \"data_B2\": xB2,\n",
        "        \"T_A\": TA,\n",
        "        \"T_B\": TB,\n",
        "    }\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 2.5) ✅ Windowing (added)\n",
        "# ---------------------------------------------------------------------\n",
        "def trial_list_to_windows(trial_list, fs, win_sec=8.0, stride_sec=4.0, drop_last=True):\n",
        "    win_len = int(round(win_sec * fs))\n",
        "    stride = int(round(stride_sec * fs))\n",
        "    assert win_len > 0 and stride > 0\n",
        "\n",
        "    windows = []\n",
        "    for item in trial_list:\n",
        "        x = item[\"data\"]\n",
        "        T = x.shape[0]\n",
        "        total_count = float(item[\"count\"])\n",
        "        meta = item[\"meta\"]\n",
        "\n",
        "        total_dur = max(T / float(fs), 1e-6)\n",
        "        rate_trial = total_count / total_dur\n",
        "\n",
        "        if T < win_len:\n",
        "            win_dur = T / float(fs)\n",
        "            windows.append({\n",
        "                \"data\": x,\n",
        "                \"count\": rate_trial * win_dur,\n",
        "                \"meta\": f\"{meta}__win[0:{T}]\",\n",
        "                \"parent_meta\": meta,\n",
        "                \"parent_T\": T,\n",
        "                \"win_start\": 0,\n",
        "                \"win_end\": T,\n",
        "            })\n",
        "            continue\n",
        "\n",
        "        last_start = T - win_len\n",
        "        starts = list(range(0, last_start + 1, stride))\n",
        "\n",
        "        for st in starts:\n",
        "            ed = st + win_len\n",
        "            win_dur = win_len / float(fs)\n",
        "            windows.append({\n",
        "                \"data\": x[st:ed],\n",
        "                \"count\": rate_trial * win_dur,\n",
        "                \"meta\": f\"{meta}__win[{st}:{ed}]\",\n",
        "                \"parent_meta\": meta,\n",
        "                \"parent_T\": T,\n",
        "                \"win_start\": st,\n",
        "                \"win_end\": ed,\n",
        "            })\n",
        "\n",
        "        if not drop_last:\n",
        "            last_st = starts[-1] + stride\n",
        "            if last_st < T:\n",
        "                ed = T\n",
        "                win_dur = (ed - last_st) / float(fs)\n",
        "                windows.append({\n",
        "                    \"data\": x[last_st:ed],\n",
        "                    \"count\": rate_trial * win_dur,\n",
        "                    \"meta\": f\"{meta}__win[{last_st}:{ed}]\",\n",
        "                    \"parent_meta\": meta,\n",
        "                    \"parent_T\": T,\n",
        "                    \"win_start\": last_st,\n",
        "                    \"win_end\": ed,\n",
        "                })\n",
        "\n",
        "    return windows\n",
        "\n",
        "\n",
        "def predict_count_by_windowing(model, x_np, fs, win_sec, stride_sec, device, tau=1.0, batch_size=64):\n",
        "    win_len = int(round(win_sec * fs))\n",
        "    stride = int(round(stride_sec * fs))\n",
        "    T = x_np.shape[0]\n",
        "    total_dur = T / float(fs)\n",
        "\n",
        "    if T <= win_len:\n",
        "        x_tensor = torch.tensor(x_np, dtype=torch.float32).transpose(0, 1).unsqueeze(0).to(device)\n",
        "        with torch.no_grad():\n",
        "            rate_hat, _, _, _ = model(x_tensor, mask=None, tau=tau)\n",
        "        pred_count = float(rate_hat.item() * total_dur)\n",
        "        return pred_count, np.array([float(rate_hat.item())], dtype=np.float32)\n",
        "\n",
        "    starts = list(range(0, T - win_len + 1, stride))\n",
        "    windows = np.stack([x_np[st:st + win_len] for st in starts], axis=0)\n",
        "\n",
        "    xw = torch.tensor(windows, dtype=torch.float32).permute(0, 2, 1).to(device)\n",
        "\n",
        "    rates = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, xw.shape[0], batch_size):\n",
        "            xb = xw[i:i + batch_size]\n",
        "            r_hat, _, _, _ = model(xb, mask=None, tau=tau)\n",
        "            rates.append(r_hat.detach().cpu().numpy())\n",
        "\n",
        "    rates = np.concatenate(rates, axis=0)\n",
        "    rate_mean = float(rates.mean())\n",
        "    pred_count = rate_mean * total_dur\n",
        "    return float(pred_count), rates\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# ✅ Helpers for scenario-avg plots\n",
        "# ---------------------------------------------------------------------\n",
        "def get_window_rate_curve(model, x_np, fs, win_sec, stride_sec, device, tau=1.0, batch_size=64):\n",
        "    win_len = int(round(win_sec * fs))\n",
        "    stride = int(round(stride_sec * fs))\n",
        "    T = x_np.shape[0]\n",
        "\n",
        "    if T <= win_len:\n",
        "        pred_count, rates = predict_count_by_windowing(model, x_np, fs, win_sec, stride_sec, device, tau=tau, batch_size=batch_size)\n",
        "        t_cent = np.array([0.5 * (T / float(fs))], dtype=np.float32)\n",
        "        return t_cent, rates.astype(np.float32)\n",
        "\n",
        "    starts = list(range(0, T - win_len + 1, stride))\n",
        "    windows = np.stack([x_np[st:st + win_len] for st in starts], axis=0)\n",
        "\n",
        "    xw = torch.tensor(windows, dtype=torch.float32).permute(0, 2, 1).to(device)\n",
        "\n",
        "    rates = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, xw.shape[0], batch_size):\n",
        "            xb = xw[i:i + batch_size]\n",
        "            r_hat, _, _, _ = model(xb, mask=None, tau=tau)\n",
        "            rates.append(r_hat.detach().cpu().numpy())\n",
        "    rates = np.concatenate(rates, axis=0).astype(np.float32)\n",
        "\n",
        "    centers = np.array([(st + 0.5 * win_len) / float(fs) for st in starts], dtype=np.float32)\n",
        "    return centers, rates\n",
        "\n",
        "\n",
        "def resample_1d(y, new_len):\n",
        "    y = np.asarray(y, dtype=np.float32)\n",
        "    if y.size == 0:\n",
        "        return np.zeros((new_len,), dtype=np.float32)\n",
        "    if y.size == 1:\n",
        "        return np.full((new_len,), float(y[0]), dtype=np.float32)\n",
        "    x_old = np.linspace(0.0, 1.0, num=y.size, dtype=np.float32)\n",
        "    x_new = np.linspace(0.0, 1.0, num=new_len, dtype=np.float32)\n",
        "    return np.interp(x_new, x_old, y).astype(np.float32)\n",
        "\n",
        "\n",
        "def resample_2d(Y, new_len):\n",
        "    Y = np.asarray(Y, dtype=np.float32)\n",
        "    if Y.shape[0] == 0:\n",
        "        return np.zeros((new_len, Y.shape[1]), dtype=np.float32)\n",
        "    if Y.shape[0] == 1:\n",
        "        return np.repeat(Y, repeats=new_len, axis=0).astype(np.float32)\n",
        "    x_old = np.linspace(0.0, 1.0, num=Y.shape[0], dtype=np.float32)\n",
        "    x_new = np.linspace(0.0, 1.0, num=new_len, dtype=np.float32)\n",
        "    out = []\n",
        "    for d in range(Y.shape[1]):\n",
        "        out.append(np.interp(x_new, x_old, Y[:, d]))\n",
        "    return np.stack(out, axis=1).astype(np.float32)\n",
        "\n",
        "\n",
        "def global_pca2(Z_all):\n",
        "    \"\"\"\n",
        "    Z_all: (N,D)\n",
        "    returns: mean (D,), V2 (2,D)  such that proj = (Z-mean) @ V2.T\n",
        "    \"\"\"\n",
        "    Z_all = np.asarray(Z_all, dtype=np.float32)\n",
        "    mu = Z_all.mean(axis=0, keepdims=True)\n",
        "    Zc = Z_all - mu\n",
        "    _, _, Vt = np.linalg.svd(Zc, full_matrices=False)\n",
        "    V2 = Vt[:2].astype(np.float32)  # (2,D)\n",
        "    return mu.squeeze(0).astype(np.float32), V2\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# ✅ NEW: Two-plot visualization for ABAB\n",
        "#   - only (1) rep_rate mean±std + boundaries + shading\n",
        "#   - only (2) PCA2 scatter (no line, no boundary markers, no std circles)\n",
        "# ---------------------------------------------------------------------\n",
        "def plot_scenario_mean_std_two_plots(\n",
        "    save_dir,\n",
        "    scen_name,\n",
        "    win_t, win_mean, win_std,\n",
        "    lat_xy_mean, lat_xy_std,   # (std unused but keep signature)\n",
        "    n_subjects,\n",
        "    dpi=600,\n",
        "):\n",
        "    _ensure_dir(save_dir if save_dir is not None else \"scenario_viz\")\n",
        "    out_dir = save_dir if save_dir is not None else \"scenario_viz\"\n",
        "    scen_tag = _safe_fname(scen_name)\n",
        "\n",
        "    # -------------------------\n",
        "    # (1) Window-level rep_rate mean±std (+ boundaries + shading)\n",
        "    # - A/B 글자 없음\n",
        "    # - figsize/폰트 크게\n",
        "    # -------------------------\n",
        "    fig1 = plt.figure(figsize=(26.0, 9.2))\n",
        "    ax = fig1.gca()\n",
        "\n",
        "    # ABAB: 0~1,1~2,2~3,3~4\n",
        "    ax.axvspan(0.0, 1.0, alpha=0.06, color=\"k\", lw=0)\n",
        "    ax.axvspan(1.0, 2.0, alpha=0.03, color=\"k\", lw=0)\n",
        "    ax.axvspan(2.0, 3.0, alpha=0.06, color=\"k\", lw=0)\n",
        "    ax.axvspan(3.0, 4.0, alpha=0.03, color=\"k\", lw=0)\n",
        "\n",
        "    ax.plot(win_t, win_mean, linewidth=6.4, label=\"Window rep_rate (mean)\")\n",
        "    ax.fill_between(win_t, win_mean - win_std, win_mean + win_std, alpha=0.22, label=\"±1 std\")\n",
        "\n",
        "    # boundaries at 1,2,3 (label once)\n",
        "    ax.axvline(1.0, linestyle=\"--\", linewidth=6.0, label=\"Boundary\")\n",
        "    ax.axvline(2.0, linestyle=\"--\", linewidth=6.0, label=\"_nolegend_\")\n",
        "    ax.axvline(3.0, linestyle=\"--\", linewidth=6.0, label=\"_nolegend_\")\n",
        "\n",
        "    ax.set_xlabel(\"Normalized time\", fontsize=46, labelpad=10)\n",
        "    ax.set_ylabel(\"rep_rate (reps/s)\", fontsize=46, labelpad=10)\n",
        "    ax.tick_params(axis=\"both\", labelsize=37)\n",
        "\n",
        "    ax.legend(fontsize=38, frameon=True, loc=\"upper left\")\n",
        "    fig1.tight_layout()\n",
        "    fig1.savefig(os.path.join(out_dir, f\"{scen_tag}__1_win_rep_rate.png\"), dpi=dpi, bbox_inches=\"tight\")\n",
        "    plt.close(fig1)\n",
        "\n",
        "    # -------------------------\n",
        "    # (2) Latent trajectory PCA2\n",
        "    # - std circle 삭제\n",
        "    # - line 없음 (scatter only)\n",
        "    # - boundary/start/end 없음\n",
        "    # -------------------------\n",
        "    fig2 = plt.figure(figsize=(8.8, 8.8))\n",
        "    ax = fig2.gca()\n",
        "\n",
        "    T = lat_xy_mean.shape[0]\n",
        "    t_lat = np.linspace(0.0, 4.0, num=T, dtype=np.float32)\n",
        "\n",
        "    sc = ax.scatter(lat_xy_mean[:, 0], lat_xy_mean[:, 1], c=t_lat, s=22, alpha=0.95)\n",
        "\n",
        "    ax.set_xlabel(\"PC1\", fontsize=24, labelpad=10)\n",
        "    ax.set_ylabel(\"PC2\", fontsize=24, labelpad=10)\n",
        "    ax.tick_params(axis=\"both\", labelsize=18)\n",
        "    ax.set_aspect(\"equal\", adjustable=\"box\")\n",
        "\n",
        "    # square bounds 유지\n",
        "    x = lat_xy_mean[:, 0]; y = lat_xy_mean[:, 1]\n",
        "    xmin, xmax = float(np.min(x)), float(np.max(x))\n",
        "    ymin, ymax = float(np.min(y)), float(np.max(y))\n",
        "    cx = 0.5 * (xmin + xmax); cy = 0.5 * (ymin + ymax)\n",
        "    rx = 0.5 * (xmax - xmin); ry = 0.5 * (ymax - ymin)\n",
        "    r = max(rx, ry) * 1.12 + 1e-6\n",
        "    ax.set_xlim(cx - r, cx + r)\n",
        "    ax.set_ylim(cy - r, cy + r)\n",
        "\n",
        "    cbar = fig2.colorbar(sc, ax=ax, fraction=0.046, pad=0.04)\n",
        "    cbar.set_label(\"normalized time (0→4)\", fontsize=20, labelpad=10)\n",
        "    cbar.ax.tick_params(labelsize=16)\n",
        "\n",
        "    fig2.tight_layout()\n",
        "    fig2.savefig(os.path.join(out_dir, f\"{scen_tag}__2_latent_pca2.png\"), dpi=dpi, bbox_inches=\"tight\")\n",
        "    plt.close(fig2)\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 2.8) Dataset / Collate\n",
        "# ---------------------------------------------------------------------\n",
        "class TrialDataset(Dataset):\n",
        "    def __init__(self, trial_list):\n",
        "        self.trials = trial_list\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.trials)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.trials[idx]\n",
        "        data = torch.tensor(item['data'], dtype=torch.float32).transpose(0, 1)  # (C, T)\n",
        "        count = torch.tensor(item['count'], dtype=torch.float32)\n",
        "        return data, count, item['meta']\n",
        "\n",
        "\n",
        "def collate_variable_length(batch):\n",
        "    max_len = max([x[0].shape[1] for x in batch])\n",
        "    C = batch[0][0].shape[0]\n",
        "\n",
        "    padded_data, masks, counts, metas, lengths = [], [], [], [], []\n",
        "    for data, count, meta in batch:\n",
        "        T = data.shape[1]\n",
        "        lengths.append(T)\n",
        "\n",
        "        pad_size = max_len - T\n",
        "        if pad_size > 0:\n",
        "            pad = torch.zeros(C, pad_size)\n",
        "            d_padded = torch.cat([data, pad], dim=1)\n",
        "            mask = torch.cat([torch.ones(T), torch.zeros(pad_size)], dim=0)\n",
        "        else:\n",
        "            d_padded = data\n",
        "            mask = torch.ones(T)\n",
        "\n",
        "        padded_data.append(d_padded)\n",
        "        masks.append(mask)\n",
        "        counts.append(count)\n",
        "        metas.append(meta)\n",
        "\n",
        "    return {\n",
        "        \"data\": torch.stack(padded_data),\n",
        "        \"mask\": torch.stack(masks),\n",
        "        \"count\": torch.stack(counts),\n",
        "        \"length\": torch.tensor(lengths, dtype=torch.float32),\n",
        "        \"meta\": metas\n",
        "    }\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 3) Model\n",
        "# ---------------------------------------------------------------------\n",
        "class ManifoldEncoder(nn.Module):\n",
        "    def __init__(self, input_ch, hidden_dim=128, latent_dim=16):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv1d(input_ch, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, latent_dim, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        z = self.net(x)            # (B, D, T)\n",
        "        z = z.transpose(1, 2)      # (B, T, D)\n",
        "        return z\n",
        "\n",
        "\n",
        "class ManifoldDecoder(nn.Module):\n",
        "    def __init__(self, latent_dim, hidden_dim, out_ch):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv1d(latent_dim, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, out_ch, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        zt = z.transpose(1, 2)\n",
        "        x_hat = self.net(zt)\n",
        "        return x_hat\n",
        "\n",
        "\n",
        "class MultiRateHead(nn.Module):\n",
        "    def __init__(self, latent_dim=16, hidden=64, K_max=6):\n",
        "        super().__init__()\n",
        "        self.K_max = K_max\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(latent_dim, hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden, 1 + K_max)\n",
        "        )\n",
        "\n",
        "    def forward(self, z, tau=1.0):\n",
        "        out = self.net(z)\n",
        "        amp = F.softplus(out[..., 0])\n",
        "        phase_logits = out[..., 1:]\n",
        "        phase = F.softmax(phase_logits / tau, dim=-1)\n",
        "        return amp, phase, phase_logits\n",
        "\n",
        "\n",
        "class KAutoCountModel(nn.Module):\n",
        "    def __init__(self, input_ch, hidden_dim=128, latent_dim=16, K_max=6, k_hidden=64):\n",
        "        super().__init__()\n",
        "        self.encoder = ManifoldEncoder(input_ch, hidden_dim, latent_dim)\n",
        "        self.decoder = ManifoldDecoder(latent_dim, hidden_dim, input_ch)\n",
        "        self.rate_head = MultiRateHead(latent_dim, hidden=hidden_dim, K_max=K_max)\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, (nn.Conv1d, nn.Linear)):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            b = self.rate_head.net[-1].bias\n",
        "            b.zero_()\n",
        "            b[0].fill_(-2.0)\n",
        "\n",
        "    @staticmethod\n",
        "    def _masked_mean_time(x, mask=None, eps=1e-6):\n",
        "        if mask is None:\n",
        "            return x.mean(dim=1)\n",
        "        if x.dim() == 2:\n",
        "            m = mask.to(dtype=x.dtype, device=x.device)\n",
        "            return (x * m).sum(dim=1) / (m.sum(dim=1) + eps)\n",
        "        elif x.dim() == 3:\n",
        "            m = mask.to(dtype=x.dtype, device=x.device).unsqueeze(-1)\n",
        "            return (x * m).sum(dim=1) / (m.sum(dim=1) + eps)\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported dim for masked mean: {x.dim()}\")\n",
        "\n",
        "    def forward(self, x, mask=None, tau=1.0):\n",
        "        z = self.encoder(x)\n",
        "        x_hat = self.decoder(z)\n",
        "\n",
        "        amp_t, phase_p, phase_logits = self.rate_head(z, tau=tau)\n",
        "\n",
        "        micro_rate_t = amp_t\n",
        "\n",
        "        p_bar = self._masked_mean_time(phase_p, mask)\n",
        "        k_hat = 1.0 / (p_bar.pow(2).sum(dim=1) + 1e-6)\n",
        "\n",
        "        rep_rate_t = micro_rate_t / (k_hat.unsqueeze(1) + 1e-6)\n",
        "        if mask is not None:\n",
        "            rep_rate_t = rep_rate_t * mask\n",
        "\n",
        "        if mask is None:\n",
        "            avg_rep_rate = rep_rate_t.mean(dim=1)\n",
        "        else:\n",
        "            avg_rep_rate = (rep_rate_t * mask).sum(dim=1) / (mask.sum(dim=1) + 1e-6)\n",
        "\n",
        "        aux = {\n",
        "            \"phase_p\": phase_p,\n",
        "            \"phase_logits\": phase_logits,\n",
        "            \"micro_rate_t\": micro_rate_t,\n",
        "            \"rep_rate_t\": rep_rate_t,\n",
        "            \"k_hat\": k_hat,\n",
        "        }\n",
        "        return avg_rep_rate, z, x_hat, aux\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 4) Loss utils\n",
        "# ---------------------------------------------------------------------\n",
        "def masked_recon_mse(x_hat, x, mask, eps=1e-6):\n",
        "    mask = mask.to(dtype=x.dtype, device=x.device)\n",
        "    mask_bc = mask.unsqueeze(1)\n",
        "    se = (x_hat - x) ** 2\n",
        "    se = se * mask_bc\n",
        "    denom = (mask.sum() * x.shape[1]) + eps\n",
        "    return se.sum() / denom\n",
        "\n",
        "\n",
        "def temporal_smoothness(v, mask=None, eps=1e-6):\n",
        "    dv = torch.abs(v[:, 1:] - v[:, :-1])\n",
        "    if mask is None:\n",
        "        return dv.mean()\n",
        "    m = mask[:, 1:] * mask[:, :-1]\n",
        "    m = m.to(dtype=dv.dtype, device=dv.device)\n",
        "    return (dv * m).sum() / (m.sum() + eps)\n",
        "\n",
        "\n",
        "def phase_entropy_loss(phase_p, mask=None, eps=1e-8):\n",
        "    ent = -(phase_p * (phase_p + eps).log()).sum(dim=-1)\n",
        "    if mask is None:\n",
        "        return ent.mean()\n",
        "    ent = ent * mask\n",
        "    return ent.sum() / (mask.sum() + eps)\n",
        "\n",
        "\n",
        "def effK_usage_loss(phase_p, mask=None, eps=1e-6):\n",
        "    # kept for training loss\n",
        "    if mask is None:\n",
        "        p_bar = phase_p.mean(dim=1)\n",
        "    else:\n",
        "        m = mask.to(dtype=phase_p.dtype, device=phase_p.device).unsqueeze(-1)\n",
        "        p_bar = (phase_p * m).sum(dim=1) / (m.sum(dim=1) + eps)\n",
        "\n",
        "    effK = 1.0 / (p_bar.pow(2).sum(dim=1) + eps)\n",
        "    return effK.mean(), effK.detach()\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 5) Train\n",
        "# ---------------------------------------------------------------------\n",
        "def train_one_epoch(model, loader, optimizer, config, device):\n",
        "    model.train()\n",
        "    stats = {k: 0.0 for k in [\n",
        "        'loss', 'loss_rate', 'loss_recon', 'loss_smooth', 'loss_phase_ent', 'loss_effk',\n",
        "        'mae_count'\n",
        "    ]}\n",
        "\n",
        "    fs = config[\"fs\"]\n",
        "    tau = config.get(\"tau\", 1.0)\n",
        "\n",
        "    lam_recon = config.get(\"lambda_recon\", 1.0)\n",
        "    lam_smooth = config.get(\"lambda_smooth\", 0.05)\n",
        "    lam_phase_ent = config.get(\"lambda_phase_ent\", 0.01)\n",
        "    lam_effk = config.get(\"lambda_effk\", 0.005)\n",
        "\n",
        "    for batch in loader:\n",
        "        x = batch[\"data\"].to(device)\n",
        "        mask = batch[\"mask\"].to(device)\n",
        "        y_count = batch[\"count\"].to(device)\n",
        "        length = batch[\"length\"].to(device)\n",
        "\n",
        "        duration = torch.clamp(length / fs, min=1e-6)\n",
        "        y_rate = y_count / duration\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        rate_hat, z, x_hat, aux = model(x, mask, tau=tau)\n",
        "\n",
        "        loss_rate = F.mse_loss(rate_hat, y_rate)\n",
        "        loss_recon = masked_recon_mse(x_hat, x, mask)\n",
        "        loss_smooth = temporal_smoothness(aux[\"rep_rate_t\"], mask)\n",
        "        loss_phase_ent = phase_entropy_loss(aux[\"phase_p\"], mask)\n",
        "        loss_effk, _ = effK_usage_loss(aux[\"phase_p\"], mask)\n",
        "\n",
        "        loss = (loss_rate\n",
        "                + lam_recon * loss_recon\n",
        "                + lam_smooth * loss_smooth\n",
        "                + lam_phase_ent * loss_phase_ent\n",
        "                + lam_effk * loss_effk)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        count_hat = rate_hat * duration\n",
        "        stats['loss'] += loss.item()\n",
        "        stats['loss_rate'] += loss_rate.item()\n",
        "        stats['loss_recon'] += loss_recon.item()\n",
        "        stats['loss_smooth'] += loss_smooth.item()\n",
        "        stats['loss_phase_ent'] += loss_phase_ent.item()\n",
        "        stats['loss_effk'] += loss_effk.item()\n",
        "        stats['mae_count'] += torch.abs(count_hat - y_count).mean().item()\n",
        "\n",
        "    n = len(loader)\n",
        "    return {k: v / n for k, v in stats.items()}\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 6) Main (Scenario-wise LOSO)  - ABAB\n",
        "# ---------------------------------------------------------------------\n",
        "def main():\n",
        "    CONFIG = {\n",
        "        \"seed\": 42,\n",
        "        \"data_dir\": \"/content/drive/MyDrive/Colab Notebooks/HAR_data/MHEALTHDATASET\",\n",
        "\n",
        "        \"COLUMN_NAMES\": [\n",
        "            'acc_chest_x', 'acc_chest_y', 'acc_chest_z',\n",
        "            'ecg_1', 'ecg_2',\n",
        "            'acc_ankle_x', 'acc_ankle_y', 'acc_ankle_z',\n",
        "            'gyro_ankle_x', 'gyro_ankle_y', 'gyro_ankle_z',\n",
        "            'mag_ankle_x', 'mag_ankle_y', 'mag_ankle_z',\n",
        "            'acc_arm_x', 'acc_arm_y', 'acc_arm_z',\n",
        "            'gyro_arm_x', 'gyro_arm_y', 'gyro_arm_z',\n",
        "            'mag_arm_x', 'mag_arm_y', 'mag_arm_z',\n",
        "            'activity_id'\n",
        "        ],\n",
        "        \"TARGET_ACTIVITIES_MAP\": {\n",
        "            7: 'Frontal elevation of arms',\n",
        "            8: 'Knees bending',\n",
        "            10: 'Jogging',\n",
        "            11: 'Running',\n",
        "        },\n",
        "        \"ACT_FEATURE_MAP\": {\n",
        "            7: ['acc_chest_x', 'acc_chest_y', 'acc_chest_z',\n",
        "                'acc_ankle_x', 'acc_ankle_y', 'acc_ankle_z',\n",
        "                'gyro_ankle_x', 'gyro_ankle_y', 'gyro_ankle_z',\n",
        "                'acc_arm_x', 'acc_arm_y', 'acc_arm_z',\n",
        "                'gyro_arm_x', 'gyro_arm_y', 'gyro_arm_z'],\n",
        "            8: ['acc_chest_x', 'acc_chest_y', 'acc_chest_z',\n",
        "                'acc_ankle_x', 'acc_ankle_y', 'acc_ankle_z',\n",
        "                'gyro_ankle_x', 'gyro_ankle_y', 'gyro_ankle_z',\n",
        "                'acc_arm_x', 'acc_arm_y', 'acc_arm_z',\n",
        "                'gyro_arm_x', 'gyro_arm_y', 'gyro_arm_z'],\n",
        "            10: ['acc_chest_x', 'acc_chest_y', 'acc_chest_z',\n",
        "                 'acc_ankle_x', 'acc_ankle_y', 'acc_ankle_z',\n",
        "                 'gyro_ankle_x', 'gyro_ankle_y', 'gyro_ankle_z',\n",
        "                 'acc_arm_x', 'acc_arm_y', 'acc_arm_z',\n",
        "                 'gyro_arm_x', 'gyro_arm_y', 'gyro_arm_z'],\n",
        "            11: ['acc_chest_x', 'acc_chest_y', 'acc_chest_z',\n",
        "                 'acc_ankle_x', 'acc_ankle_y', 'acc_ankle_z',\n",
        "                 'gyro_ankle_x', 'gyro_ankle_y', 'gyro_ankle_z',\n",
        "                 'acc_arm_x', 'acc_arm_y', 'acc_arm_z',\n",
        "                 'gyro_arm_x', 'gyro_arm_y', 'gyro_arm_z'],\n",
        "        },\n",
        "\n",
        "        # Training Params\n",
        "        \"epochs\": 100,\n",
        "        \"lr\": 5e-4,\n",
        "        \"batch_size\": 64,\n",
        "        \"fs\": 50,\n",
        "\n",
        "        # Windowing Params\n",
        "        \"win_sec\": 8.0,\n",
        "        \"stride_sec\": 4.0,\n",
        "        \"drop_last\": True,\n",
        "\n",
        "        # Model\n",
        "        \"hidden_dim\": 128,\n",
        "        \"latent_dim\": 16,\n",
        "        \"K_max\": 6,\n",
        "\n",
        "        # Loss Weights\n",
        "        \"lambda_recon\": 1.0,\n",
        "        \"lambda_smooth\": 0.05,\n",
        "        \"lambda_phase_ent\": 0.01,\n",
        "        \"lambda_effk\": 0.0075,\n",
        "\n",
        "        \"tau\": 1.0,\n",
        "\n",
        "        \"GT_BY_ACT\": {\n",
        "            7: {\n",
        "                \"subject1\": 20, \"subject2\": 20, \"subject3\": 20, \"subject4\": 20, \"subject5\": 20,\n",
        "                \"subject6\": 20, \"subject7\": 20, \"subject8\": 19, \"subject9\": 19, \"subject10\": 20,\n",
        "            },\n",
        "            8: {\n",
        "                \"subject1\": 20, \"subject2\": 21, \"subject3\": 21, \"subject4\": 19, \"subject5\": 20,\n",
        "                \"subject6\": 20, \"subject7\": 21, \"subject8\": 21, \"subject9\": 21, \"subject10\": 21,\n",
        "            },\n",
        "            10: {\n",
        "                \"subject1\": 157, \"subject2\": 161, \"subject3\": 154, \"subject4\": 154, \"subject5\": 160,\n",
        "                \"subject6\": 156, \"subject7\": 153, \"subject8\": 160, \"subject9\": 166, \"subject10\": 156,\n",
        "            },\n",
        "            11: {\n",
        "                \"subject1\": 165, \"subject2\": 158, \"subject3\": 174, \"subject4\": 163, \"subject5\": 157,\n",
        "                \"subject6\": 172, \"subject7\": 149, \"subject8\": 166, \"subject9\": 174, \"subject10\": 172,\n",
        "            },\n",
        "        },\n",
        "\n",
        "        \"MIX_SCENARIOS\": [\n",
        "            (\"Frontal elevation of arms -> Knees bending (ABAB)\", 7, 8),\n",
        "            (\"Knees bending -> Frontal elevation of arms (ABAB)\", 8, 7),\n",
        "            (\"Jogging -> Running (ABAB)\", 10, 11),\n",
        "            (\"Running -> Jogging (ABAB)\", 11, 10),\n",
        "        ],\n",
        "    }\n",
        "\n",
        "    set_strict_seed(CONFIG[\"seed\"])\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Device: {device}\")\n",
        "\n",
        "    full_data = load_mhealth_dataset(CONFIG[\"data_dir\"], CONFIG[\"TARGET_ACTIVITIES_MAP\"], CONFIG[\"COLUMN_NAMES\"])\n",
        "    if not full_data:\n",
        "        return\n",
        "\n",
        "    subjects = [f\"subject{i}\" for i in range(1, 11)]\n",
        "\n",
        "    VIZ_DIR = \"scenario_viz\"\n",
        "    os.makedirs(VIZ_DIR, exist_ok=True)\n",
        "\n",
        "    # resample lengths (fixed for averaging)  # per-segment\n",
        "    NW = 60     # window-rate points per segment\n",
        "    NL = 200    # latent points per segment\n",
        "\n",
        "    for scen_idx, (scen_name, actA_id, actB_id) in enumerate(CONFIG.get(\"MIX_SCENARIOS\", []), start=1):\n",
        "        TRAIN_ACT_ID = actA_id\n",
        "\n",
        "        print(\"\\n\" + \"=\" * 100)\n",
        "        print(f\"1. 시나리오{scen_idx}: {scen_name}\")\n",
        "        print(\"=\" * 100)\n",
        "\n",
        "        loso_results = []\n",
        "        scenario_records = []\n",
        "        scenario_diffs = []\n",
        "\n",
        "        # buffers for scenario-avg plots (ABAB => 4 segments)\n",
        "        buf_win_concat = []    # (4*NW,)\n",
        "        buf_z_concat = []      # raw z(t) list for global PCA: (Tmix,D) per subject\n",
        "\n",
        "        # per-subject z splits for later projection & resample\n",
        "        per_subject_latent = []  # dicts with zA1,zB1,zA2,zB2\n",
        "\n",
        "        for fold_idx, test_subj in enumerate(subjects):\n",
        "            set_strict_seed(CONFIG[\"seed\"])\n",
        "\n",
        "            gt_train_map = CONFIG[\"GT_BY_ACT\"][TRAIN_ACT_ID]\n",
        "            train_labels = [(s, TRAIN_ACT_ID, gt_train_map[s]) for s in subjects if s != test_subj]\n",
        "            test_labels  = [(test_subj, TRAIN_ACT_ID, gt_train_map[test_subj])]\n",
        "\n",
        "            train_trials = prepare_trial_list(train_labels, full_data, CONFIG[\"TARGET_ACTIVITIES_MAP\"], CONFIG[\"ACT_FEATURE_MAP\"])\n",
        "            test_trials  = prepare_trial_list(test_labels,  full_data, CONFIG[\"TARGET_ACTIVITIES_MAP\"], CONFIG[\"ACT_FEATURE_MAP\"])\n",
        "            if not test_trials or not train_trials:\n",
        "                continue\n",
        "\n",
        "            train_data = trial_list_to_windows(\n",
        "                train_trials, fs=CONFIG[\"fs\"],\n",
        "                win_sec=CONFIG[\"win_sec\"], stride_sec=CONFIG[\"stride_sec\"],\n",
        "                drop_last=CONFIG[\"drop_last\"]\n",
        "            )\n",
        "\n",
        "            g = torch.Generator()\n",
        "            g.manual_seed(CONFIG[\"seed\"])\n",
        "\n",
        "            train_loader = DataLoader(\n",
        "                TrialDataset(train_data),\n",
        "                batch_size=CONFIG[\"batch_size\"],\n",
        "                shuffle=True,\n",
        "                collate_fn=collate_variable_length,\n",
        "                generator=g,\n",
        "                num_workers=0\n",
        "            )\n",
        "\n",
        "            input_ch = train_data[0]['data'].shape[1]\n",
        "            model = KAutoCountModel(\n",
        "                input_ch=input_ch,\n",
        "                hidden_dim=CONFIG[\"hidden_dim\"],\n",
        "                latent_dim=CONFIG[\"latent_dim\"],\n",
        "                K_max=CONFIG[\"K_max\"]\n",
        "            ).to(device)\n",
        "\n",
        "            optimizer = torch.optim.Adam(model.parameters(), lr=CONFIG[\"lr\"])\n",
        "            scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.5)\n",
        "\n",
        "            for epoch in range(CONFIG[\"epochs\"]):\n",
        "                _ = train_one_epoch(model, train_loader, optimizer, CONFIG, device)\n",
        "                scheduler.step()\n",
        "\n",
        "            model.eval()\n",
        "\n",
        "            # (A) LOSO on TRAIN_ACT_ID\n",
        "            item = test_trials[0]\n",
        "            x_np = item[\"data\"]\n",
        "            count_gt = float(item[\"count\"])\n",
        "            count_pred_win, _ = predict_count_by_windowing(\n",
        "                model, x_np=x_np,\n",
        "                fs=CONFIG[\"fs\"], win_sec=CONFIG[\"win_sec\"], stride_sec=CONFIG[\"stride_sec\"],\n",
        "                device=device, tau=CONFIG.get(\"tau\", 1.0),\n",
        "                batch_size=CONFIG.get(\"batch_size\", 64)\n",
        "            )\n",
        "            fold_mae = float(abs(count_pred_win - count_gt))\n",
        "            loso_results.append(fold_mae)\n",
        "\n",
        "            # (B) Scenario eval (ABAB)\n",
        "            mixed_item = build_mixed_abab_trial(test_subj, actA_id, actB_id, CONFIG, full_data)\n",
        "            if mixed_item is None:\n",
        "                continue\n",
        "\n",
        "            x_mix = mixed_item[\"data\"]\n",
        "            boundaries = mixed_item[\"boundaries\"]  # [b1,b2,b3]\n",
        "            gt_total = float(mixed_item[\"count\"])\n",
        "            md = mixed_item[\"meta_detail\"]\n",
        "\n",
        "            pred_total, _ = predict_count_by_windowing(\n",
        "                model, x_np=x_mix,\n",
        "                fs=CONFIG[\"fs\"], win_sec=CONFIG[\"win_sec\"], stride_sec=CONFIG[\"stride_sec\"],\n",
        "                device=device, tau=CONFIG.get(\"tau\", 1.0),\n",
        "                batch_size=CONFIG.get(\"batch_size\", 64)\n",
        "            )\n",
        "\n",
        "            diff = float(pred_total - gt_total)\n",
        "            mae = float(abs(diff))\n",
        "            scenario_diffs.append(diff)\n",
        "\n",
        "            # full forward for z only (no effK/entropy plots)\n",
        "            x_tensor = torch.tensor(x_mix, dtype=torch.float32).transpose(0, 1).unsqueeze(0).to(device)\n",
        "            with torch.no_grad():\n",
        "                _, z_m, _, _ = model(x_tensor, mask=None, tau=CONFIG.get(\"tau\", 1.0))\n",
        "\n",
        "            z_td = z_m.squeeze(0).detach().cpu().numpy()  # (Tmix,D)\n",
        "\n",
        "            b1, b2, b3 = boundaries\n",
        "            scenario_records.append({\n",
        "                \"subj\": test_subj,\n",
        "                \"line\": (\n",
        "                    f\"[Scenario-ABAB] {scen_name} | {test_subj} | \"\n",
        "                    f\"{md['actA_name']}({md['gtA']:.0f}) -> {md['actB_name']}({md['gtB']:.0f}) -> \"\n",
        "                    f\"{md['actA_name']}({md['gtA']:.0f}) -> {md['actB_name']}({md['gtB']:.0f}) | \"\n",
        "                    f\"GT_total={gt_total:.0f} | Pred(win)={pred_total:.2f} | Diff={diff:+.2f} | MAE={mae:.2f} | \"\n",
        "                    f\"boundaries={boundaries}\"\n",
        "                )\n",
        "            })\n",
        "\n",
        "            # ---------------------------\n",
        "            # buffers for scenario-avg plots (ABAB => 4 segments)\n",
        "            # ---------------------------\n",
        "            xA1 = mixed_item[\"data_A1\"]\n",
        "            xB1 = mixed_item[\"data_B1\"]\n",
        "            xA2 = mixed_item[\"data_A2\"]\n",
        "            xB2 = mixed_item[\"data_B2\"]\n",
        "\n",
        "            # 1) window-level rep_rate curves per segment -> resample -> concat (0~4)\n",
        "            _, rA1_w = get_window_rate_curve(\n",
        "                model, xA1, fs=CONFIG[\"fs\"],\n",
        "                win_sec=CONFIG[\"win_sec\"], stride_sec=CONFIG[\"stride_sec\"],\n",
        "                device=device, tau=CONFIG.get(\"tau\", 1.0),\n",
        "                batch_size=CONFIG.get(\"batch_size\", 64)\n",
        "            )\n",
        "            _, rB1_w = get_window_rate_curve(\n",
        "                model, xB1, fs=CONFIG[\"fs\"],\n",
        "                win_sec=CONFIG[\"win_sec\"], stride_sec=CONFIG[\"stride_sec\"],\n",
        "                device=device, tau=CONFIG.get(\"tau\", 1.0),\n",
        "                batch_size=CONFIG.get(\"batch_size\", 64)\n",
        "            )\n",
        "            _, rA2_w = get_window_rate_curve(\n",
        "                model, xA2, fs=CONFIG[\"fs\"],\n",
        "                win_sec=CONFIG[\"win_sec\"], stride_sec=CONFIG[\"stride_sec\"],\n",
        "                device=device, tau=CONFIG.get(\"tau\", 1.0),\n",
        "                batch_size=CONFIG.get(\"batch_size\", 64)\n",
        "            )\n",
        "            _, rB2_w = get_window_rate_curve(\n",
        "                model, xB2, fs=CONFIG[\"fs\"],\n",
        "                win_sec=CONFIG[\"win_sec\"], stride_sec=CONFIG[\"stride_sec\"],\n",
        "                device=device, tau=CONFIG.get(\"tau\", 1.0),\n",
        "                batch_size=CONFIG.get(\"batch_size\", 64)\n",
        "            )\n",
        "\n",
        "            rA1_rs = resample_1d(rA1_w, NW)\n",
        "            rB1_rs = resample_1d(rB1_w, NW)\n",
        "            rA2_rs = resample_1d(rA2_w, NW)\n",
        "            rB2_rs = resample_1d(rB2_w, NW)\n",
        "            buf_win_concat.append(np.concatenate([rA1_rs, rB1_rs, rA2_rs, rB2_rs], axis=0))  # (4*NW,)\n",
        "\n",
        "            # 2) latent: store raw z split for global PCA projection later\n",
        "            zA1 = z_td[:b1, :]\n",
        "            zB1 = z_td[b1:b2, :]\n",
        "            zA2 = z_td[b2:b3, :]\n",
        "            zB2 = z_td[b3:, :]\n",
        "            buf_z_concat.append(z_td)\n",
        "            per_subject_latent.append({\"zA1\": zA1, \"zB1\": zB1, \"zA2\": zA2, \"zB2\": zB2})\n",
        "\n",
        "        # ---------------------------------------------------------\n",
        "        # Printing\n",
        "        # ---------------------------------------------------------\n",
        "        print(\"-\" * 100)\n",
        "        if len(loso_results) > 0:\n",
        "            print(\"TRain 결과 ->\")\n",
        "            print(\"-\" * 100)\n",
        "            print(f\" >>> Final LOSO Result (Average MAE): {np.mean(loso_results):.3f}\")\n",
        "            print(f\" >>> Standard Deviation: {np.std(loso_results):.3f}\")\n",
        "            print(\"-\" * 100)\n",
        "        else:\n",
        "            print(\"TRain 결과 -> (no folds computed)\")\n",
        "            print(\"-\" * 100)\n",
        "\n",
        "        subj2line = {r[\"subj\"]: r[\"line\"] for r in scenario_records}\n",
        "        for s in subjects:\n",
        "            if s in subj2line:\n",
        "                print(subj2line[s])\n",
        "\n",
        "        print(\"\\n\" + \"-\" * 100)\n",
        "        print(f\"시나리오{scen_idx} 결과\")\n",
        "        print(\"-\" * 100)\n",
        "\n",
        "        if len(scenario_diffs) == 0:\n",
        "            print(f\"[{scen_name}] No samples (all skipped). Fill GT_BY_ACT for required activities.\")\n",
        "        else:\n",
        "            diffs = np.array(scenario_diffs, dtype=np.float32)\n",
        "            mae = float(np.mean(np.abs(diffs)))\n",
        "            rmse = float(np.sqrt(np.mean(diffs ** 2)))\n",
        "            print(f\"[{scen_name}] N={len(diffs):2d} | MAE={mae:.3f} | RMSE={rmse:.3f} | mean(diff)={diffs.mean():+.3f} | std(diff)={diffs.std():.3f}\")\n",
        "        print(\"-\" * 100)\n",
        "\n",
        "        # ---------------------------------------------------------\n",
        "        # ✅ Scenario-level mean±std plots (ABAB) — TWO plots only\n",
        "        # ---------------------------------------------------------\n",
        "        n_ok = len(buf_win_concat)\n",
        "        if n_ok == 0:\n",
        "            print(\"[Viz Skip] No scenario samples collected for averaging.\")\n",
        "            continue\n",
        "\n",
        "        # (1) window-level mean±std on normalized axis [0,4]\n",
        "        W = np.stack(buf_win_concat, axis=0)  # (N, 4*NW)\n",
        "        win_mean = W.mean(axis=0)\n",
        "        win_std  = W.std(axis=0)\n",
        "        win_t = np.linspace(0.0, 4.0, num=4 * NW, dtype=np.float32)\n",
        "\n",
        "        # (2) latent: global PCA basis from all z\n",
        "        Z_all = np.concatenate(buf_z_concat, axis=0)  # (sumT, D)\n",
        "        mu, V2 = global_pca2(Z_all)\n",
        "\n",
        "        # project each subject, resample each segment to NL then concat -> (4*NL,2)\n",
        "        lat_list = []\n",
        "        for d in per_subject_latent:\n",
        "            zA1 = d[\"zA1\"]; zB1 = d[\"zB1\"]; zA2 = d[\"zA2\"]; zB2 = d[\"zB2\"]\n",
        "            pcA1 = (zA1 - mu) @ V2.T\n",
        "            pcB1 = (zB1 - mu) @ V2.T\n",
        "            pcA2 = (zA2 - mu) @ V2.T\n",
        "            pcB2 = (zB2 - mu) @ V2.T\n",
        "            pcA1_rs = resample_2d(pcA1, NL)\n",
        "            pcB1_rs = resample_2d(pcB1, NL)\n",
        "            pcA2_rs = resample_2d(pcA2, NL)\n",
        "            pcB2_rs = resample_2d(pcB2, NL)\n",
        "            lat_list.append(np.concatenate([pcA1_rs, pcB1_rs, pcA2_rs, pcB2_rs], axis=0))  # (4*NL,2)\n",
        "\n",
        "        LAT = np.stack(lat_list, axis=0)  # (N,4*NL,2)\n",
        "        lat_xy_mean = LAT.mean(axis=0)    # (4*NL,2)\n",
        "        lat_xy_std  = LAT.std(axis=0)     # (4*NL,2) (unused)\n",
        "\n",
        "        plot_scenario_mean_std_two_plots(\n",
        "            save_dir=VIZ_DIR,\n",
        "            scen_name=scen_name,\n",
        "            win_t=win_t, win_mean=win_mean, win_std=win_std,\n",
        "            lat_xy_mean=lat_xy_mean, lat_xy_std=lat_xy_std,\n",
        "            n_subjects=n_ok,\n",
        "            dpi=600\n",
        "        )\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-NIbLydMtp-K",
        "outputId": "7796b9dc-963d-4d7d-c9cf-f8b1cda37c69"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n",
            "Loading 10 subjects from /content/drive/MyDrive/Colab Notebooks/HAR_data/MHEALTHDATASET...\n",
            "\n",
            "====================================================================================================\n",
            "1. 시나리오1: Frontal elevation of arms -> Knees bending (ABAB)\n",
            "====================================================================================================\n",
            "----------------------------------------------------------------------------------------------------\n",
            "TRain 결과 ->\n",
            "----------------------------------------------------------------------------------------------------\n",
            " >>> Final LOSO Result (Average MAE): 2.324\n",
            " >>> Standard Deviation: 1.910\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[Scenario-ABAB] Frontal elevation of arms -> Knees bending (ABAB) | subject1 | Frontal elevation of arms(20) -> Knees bending(20) -> Frontal elevation of arms(20) -> Knees bending(20) | GT_total=80 | Pred(win)=101.02 | Diff=+21.02 | MAE=21.02 | boundaries=[3072, 6451, 9523]\n",
            "[Scenario-ABAB] Frontal elevation of arms -> Knees bending (ABAB) | subject2 | Frontal elevation of arms(20) -> Knees bending(21) -> Frontal elevation of arms(20) -> Knees bending(21) | GT_total=82 | Pred(win)=108.08 | Diff=+26.08 | MAE=26.08 | boundaries=[3328, 6758, 10086]\n",
            "[Scenario-ABAB] Frontal elevation of arms -> Knees bending (ABAB) | subject3 | Frontal elevation of arms(20) -> Knees bending(21) -> Frontal elevation of arms(20) -> Knees bending(21) | GT_total=82 | Pred(win)=131.53 | Diff=+49.53 | MAE=49.53 | boundaries=[3379, 6554, 9933]\n",
            "[Scenario-ABAB] Frontal elevation of arms -> Knees bending (ABAB) | subject4 | Frontal elevation of arms(20) -> Knees bending(19) -> Frontal elevation of arms(20) -> Knees bending(19) | GT_total=78 | Pred(win)=132.73 | Diff=+54.73 | MAE=54.73 | boundaries=[3277, 6400, 9677]\n",
            "[Scenario-ABAB] Frontal elevation of arms -> Knees bending (ABAB) | subject5 | Frontal elevation of arms(20) -> Knees bending(20) -> Frontal elevation of arms(20) -> Knees bending(20) | GT_total=80 | Pred(win)=103.07 | Diff=+23.07 | MAE=23.07 | boundaries=[2868, 5582, 8450]\n",
            "[Scenario-ABAB] Frontal elevation of arms -> Knees bending (ABAB) | subject6 | Frontal elevation of arms(20) -> Knees bending(20) -> Frontal elevation of arms(20) -> Knees bending(20) | GT_total=80 | Pred(win)=80.47 | Diff=+0.47 | MAE=0.47 | boundaries=[2099, 4403, 6502]\n",
            "[Scenario-ABAB] Frontal elevation of arms -> Knees bending (ABAB) | subject7 | Frontal elevation of arms(20) -> Knees bending(21) -> Frontal elevation of arms(20) -> Knees bending(21) | GT_total=82 | Pred(win)=83.99 | Diff=+1.99 | MAE=1.99 | boundaries=[2765, 5581, 8346]\n",
            "[Scenario-ABAB] Frontal elevation of arms -> Knees bending (ABAB) | subject8 | Frontal elevation of arms(19) -> Knees bending(21) -> Frontal elevation of arms(19) -> Knees bending(21) | GT_total=80 | Pred(win)=107.49 | Diff=+27.49 | MAE=27.49 | boundaries=[3021, 5581, 8602]\n",
            "[Scenario-ABAB] Frontal elevation of arms -> Knees bending (ABAB) | subject9 | Frontal elevation of arms(19) -> Knees bending(21) -> Frontal elevation of arms(19) -> Knees bending(21) | GT_total=80 | Pred(win)=92.01 | Diff=+12.01 | MAE=12.01 | boundaries=[2867, 5836, 8703]\n",
            "[Scenario-ABAB] Frontal elevation of arms -> Knees bending (ABAB) | subject10 | Frontal elevation of arms(20) -> Knees bending(21) -> Frontal elevation of arms(20) -> Knees bending(21) | GT_total=82 | Pred(win)=75.19 | Diff=-6.81 | MAE=6.81 | boundaries=[2765, 5632, 8397]\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "시나리오1 결과\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[Frontal elevation of arms -> Knees bending (ABAB)] N=10 | MAE=22.320 | RMSE=28.379 | mean(diff)=+20.959 | std(diff)=19.133\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "====================================================================================================\n",
            "1. 시나리오2: Knees bending -> Frontal elevation of arms (ABAB)\n",
            "====================================================================================================\n",
            "----------------------------------------------------------------------------------------------------\n",
            "TRain 결과 ->\n",
            "----------------------------------------------------------------------------------------------------\n",
            " >>> Final LOSO Result (Average MAE): 4.274\n",
            " >>> Standard Deviation: 3.012\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[Scenario-ABAB] Knees bending -> Frontal elevation of arms (ABAB) | subject1 | Knees bending(20) -> Frontal elevation of arms(20) -> Knees bending(20) -> Frontal elevation of arms(20) | GT_total=80 | Pred(win)=132.62 | Diff=+52.62 | MAE=52.62 | boundaries=[3379, 6451, 9830]\n",
            "[Scenario-ABAB] Knees bending -> Frontal elevation of arms (ABAB) | subject2 | Knees bending(21) -> Frontal elevation of arms(20) -> Knees bending(21) -> Frontal elevation of arms(20) | GT_total=82 | Pred(win)=85.19 | Diff=+3.19 | MAE=3.19 | boundaries=[3430, 6758, 10188]\n",
            "[Scenario-ABAB] Knees bending -> Frontal elevation of arms (ABAB) | subject3 | Knees bending(21) -> Frontal elevation of arms(20) -> Knees bending(21) -> Frontal elevation of arms(20) | GT_total=82 | Pred(win)=60.19 | Diff=-21.81 | MAE=21.81 | boundaries=[3175, 6554, 9729]\n",
            "[Scenario-ABAB] Knees bending -> Frontal elevation of arms (ABAB) | subject4 | Knees bending(19) -> Frontal elevation of arms(20) -> Knees bending(19) -> Frontal elevation of arms(20) | GT_total=78 | Pred(win)=96.29 | Diff=+18.29 | MAE=18.29 | boundaries=[3123, 6400, 9523]\n",
            "[Scenario-ABAB] Knees bending -> Frontal elevation of arms (ABAB) | subject5 | Knees bending(20) -> Frontal elevation of arms(20) -> Knees bending(20) -> Frontal elevation of arms(20) | GT_total=80 | Pred(win)=64.38 | Diff=-15.62 | MAE=15.62 | boundaries=[2714, 5582, 8296]\n",
            "[Scenario-ABAB] Knees bending -> Frontal elevation of arms (ABAB) | subject6 | Knees bending(20) -> Frontal elevation of arms(20) -> Knees bending(20) -> Frontal elevation of arms(20) | GT_total=80 | Pred(win)=50.89 | Diff=-29.11 | MAE=29.11 | boundaries=[2304, 4403, 6707]\n",
            "[Scenario-ABAB] Knees bending -> Frontal elevation of arms (ABAB) | subject7 | Knees bending(21) -> Frontal elevation of arms(20) -> Knees bending(21) -> Frontal elevation of arms(20) | GT_total=82 | Pred(win)=72.33 | Diff=-9.67 | MAE=9.67 | boundaries=[2816, 5581, 8397]\n",
            "[Scenario-ABAB] Knees bending -> Frontal elevation of arms (ABAB) | subject8 | Knees bending(21) -> Frontal elevation of arms(19) -> Knees bending(21) -> Frontal elevation of arms(19) | GT_total=80 | Pred(win)=72.16 | Diff=-7.84 | MAE=7.84 | boundaries=[2560, 5581, 8141]\n",
            "[Scenario-ABAB] Knees bending -> Frontal elevation of arms (ABAB) | subject9 | Knees bending(21) -> Frontal elevation of arms(19) -> Knees bending(21) -> Frontal elevation of arms(19) | GT_total=80 | Pred(win)=73.58 | Diff=-6.42 | MAE=6.42 | boundaries=[2969, 5836, 8805]\n",
            "[Scenario-ABAB] Knees bending -> Frontal elevation of arms (ABAB) | subject10 | Knees bending(21) -> Frontal elevation of arms(20) -> Knees bending(21) -> Frontal elevation of arms(20) | GT_total=82 | Pred(win)=65.74 | Diff=-16.26 | MAE=16.26 | boundaries=[2867, 5632, 8499]\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "시나리오2 결과\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[Knees bending -> Frontal elevation of arms (ABAB)] N=10 | MAE=18.083 | RMSE=22.674 | mean(diff)=-3.263 | std(diff)=22.438\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "====================================================================================================\n",
            "1. 시나리오3: Jogging -> Running (ABAB)\n",
            "====================================================================================================\n",
            "----------------------------------------------------------------------------------------------------\n",
            "TRain 결과 ->\n",
            "----------------------------------------------------------------------------------------------------\n",
            " >>> Final LOSO Result (Average MAE): 10.421\n",
            " >>> Standard Deviation: 10.920\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[Scenario-ABAB] Jogging -> Running (ABAB) | subject1 | Jogging(157) -> Running(165) -> Jogging(157) -> Running(165) | GT_total=644 | Pred(win)=646.41 | Diff=+2.41 | MAE=2.41 | boundaries=[3072, 6144, 9216]\n",
            "[Scenario-ABAB] Jogging -> Running (ABAB) | subject2 | Jogging(161) -> Running(158) -> Jogging(161) -> Running(158) | GT_total=638 | Pred(win)=679.35 | Diff=+41.35 | MAE=41.35 | boundaries=[3072, 6144, 9216]\n",
            "[Scenario-ABAB] Jogging -> Running (ABAB) | subject3 | Jogging(154) -> Running(174) -> Jogging(154) -> Running(174) | GT_total=656 | Pred(win)=671.70 | Diff=+15.70 | MAE=15.70 | boundaries=[3072, 6144, 9216]\n",
            "[Scenario-ABAB] Jogging -> Running (ABAB) | subject4 | Jogging(154) -> Running(163) -> Jogging(154) -> Running(163) | GT_total=634 | Pred(win)=743.89 | Diff=+109.89 | MAE=109.89 | boundaries=[3072, 6144, 9216]\n",
            "[Scenario-ABAB] Jogging -> Running (ABAB) | subject5 | Jogging(160) -> Running(157) -> Jogging(160) -> Running(157) | GT_total=634 | Pred(win)=713.23 | Diff=+79.23 | MAE=79.23 | boundaries=[3072, 6144, 9216]\n",
            "[Scenario-ABAB] Jogging -> Running (ABAB) | subject6 | Jogging(156) -> Running(172) -> Jogging(156) -> Running(172) | GT_total=656 | Pred(win)=750.36 | Diff=+94.36 | MAE=94.36 | boundaries=[3072, 6144, 9216]\n",
            "[Scenario-ABAB] Jogging -> Running (ABAB) | subject7 | Jogging(153) -> Running(149) -> Jogging(153) -> Running(149) | GT_total=604 | Pred(win)=687.38 | Diff=+83.38 | MAE=83.38 | boundaries=[3072, 6144, 9216]\n",
            "[Scenario-ABAB] Jogging -> Running (ABAB) | subject8 | Jogging(160) -> Running(166) -> Jogging(160) -> Running(166) | GT_total=652 | Pred(win)=643.68 | Diff=-8.32 | MAE=8.32 | boundaries=[3072, 6144, 9216]\n",
            "[Scenario-ABAB] Jogging -> Running (ABAB) | subject9 | Jogging(166) -> Running(174) -> Jogging(166) -> Running(174) | GT_total=680 | Pred(win)=763.90 | Diff=+83.90 | MAE=83.90 | boundaries=[3072, 6144, 9216]\n",
            "[Scenario-ABAB] Jogging -> Running (ABAB) | subject10 | Jogging(156) -> Running(172) -> Jogging(156) -> Running(172) | GT_total=656 | Pred(win)=785.83 | Diff=+129.83 | MAE=129.83 | boundaries=[3072, 6144, 9216]\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "시나리오3 결과\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[Jogging -> Running (ABAB)] N=10 | MAE=64.837 | RMSE=77.546 | mean(diff)=+63.173 | std(diff)=44.973\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "====================================================================================================\n",
            "1. 시나리오4: Running -> Jogging (ABAB)\n",
            "====================================================================================================\n",
            "----------------------------------------------------------------------------------------------------\n",
            "TRain 결과 ->\n",
            "----------------------------------------------------------------------------------------------------\n",
            " >>> Final LOSO Result (Average MAE): 15.211\n",
            " >>> Standard Deviation: 7.371\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[Scenario-ABAB] Running -> Jogging (ABAB) | subject1 | Running(165) -> Jogging(157) -> Running(165) -> Jogging(157) | GT_total=644 | Pred(win)=634.68 | Diff=-9.32 | MAE=9.32 | boundaries=[3072, 6144, 9216]\n",
            "[Scenario-ABAB] Running -> Jogging (ABAB) | subject2 | Running(158) -> Jogging(161) -> Running(158) -> Jogging(161) | GT_total=638 | Pred(win)=688.71 | Diff=+50.71 | MAE=50.71 | boundaries=[3072, 6144, 9216]\n",
            "[Scenario-ABAB] Running -> Jogging (ABAB) | subject3 | Running(174) -> Jogging(154) -> Running(174) -> Jogging(154) | GT_total=656 | Pred(win)=732.15 | Diff=+76.15 | MAE=76.15 | boundaries=[3072, 6144, 9216]\n",
            "[Scenario-ABAB] Running -> Jogging (ABAB) | subject4 | Running(163) -> Jogging(154) -> Running(163) -> Jogging(154) | GT_total=634 | Pred(win)=823.96 | Diff=+189.96 | MAE=189.96 | boundaries=[3072, 6144, 9216]\n",
            "[Scenario-ABAB] Running -> Jogging (ABAB) | subject5 | Running(157) -> Jogging(160) -> Running(157) -> Jogging(160) | GT_total=634 | Pred(win)=755.12 | Diff=+121.12 | MAE=121.12 | boundaries=[3072, 6144, 9216]\n",
            "[Scenario-ABAB] Running -> Jogging (ABAB) | subject6 | Running(172) -> Jogging(156) -> Running(172) -> Jogging(156) | GT_total=656 | Pred(win)=773.30 | Diff=+117.30 | MAE=117.30 | boundaries=[3072, 6144, 9216]\n",
            "[Scenario-ABAB] Running -> Jogging (ABAB) | subject7 | Running(149) -> Jogging(153) -> Running(149) -> Jogging(153) | GT_total=604 | Pred(win)=758.65 | Diff=+154.65 | MAE=154.65 | boundaries=[3072, 6144, 9216]\n",
            "[Scenario-ABAB] Running -> Jogging (ABAB) | subject8 | Running(166) -> Jogging(160) -> Running(166) -> Jogging(160) | GT_total=652 | Pred(win)=653.89 | Diff=+1.89 | MAE=1.89 | boundaries=[3072, 6144, 9216]\n",
            "[Scenario-ABAB] Running -> Jogging (ABAB) | subject9 | Running(174) -> Jogging(166) -> Running(174) -> Jogging(166) | GT_total=680 | Pred(win)=781.08 | Diff=+101.08 | MAE=101.08 | boundaries=[3072, 6144, 9216]\n",
            "[Scenario-ABAB] Running -> Jogging (ABAB) | subject10 | Running(172) -> Jogging(156) -> Running(172) -> Jogging(156) | GT_total=656 | Pred(win)=785.52 | Diff=+129.52 | MAE=129.52 | boundaries=[3072, 6144, 9216]\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "시나리오4 결과\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[Running -> Jogging (ABAB)] N=10 | MAE=95.170 | RMSE=111.303 | mean(diff)=+93.306 | std(diff)=60.684\n",
            "----------------------------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import random\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns  # kept\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.ndimage import gaussian_filter1d  # kept\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# Small IO helpers\n",
        "# =============================================================================\n",
        "def _ensure_dir(d):\n",
        "    os.makedirs(d, exist_ok=True)\n",
        "\n",
        "\n",
        "def _safe_fname(s: str) -> str:\n",
        "    s = str(s)\n",
        "    s = re.sub(r\"[\\\\/:*?\\\"<>|]+\", \"_\", s)\n",
        "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
        "    s = s.replace(\" \", \"_\")\n",
        "    return s[:180]\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# 1) Strict Seeding\n",
        "# =============================================================================\n",
        "def set_strict_seed(seed: int):\n",
        "    random.seed(seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# 2) Data Loading\n",
        "# =============================================================================\n",
        "def load_mhealth_dataset(data_dir, target_activities_map, column_names):\n",
        "    full_dataset = {}\n",
        "    file_list = sorted(glob.glob(os.path.join(data_dir, \"mHealth_subject*.log\")))\n",
        "\n",
        "    if not file_list:\n",
        "        print(f\"[Warning] No mHealth logs found in {data_dir}\")\n",
        "        return {}\n",
        "\n",
        "    print(f\"Loading {len(file_list)} subjects from {data_dir}...\")\n",
        "\n",
        "    for file_path in file_list:\n",
        "        file_name = os.path.basename(file_path)\n",
        "        subj_part = file_name.split('.')[0]\n",
        "        try:\n",
        "            subj_id_num = int(''.join(filter(str.isdigit, subj_part)))\n",
        "            subj_key = f\"subject{subj_id_num}\"\n",
        "        except:\n",
        "            subj_key = subj_part\n",
        "\n",
        "        try:\n",
        "            df = pd.read_csv(file_path, sep=\"\\t\", header=None)\n",
        "            df = df.iloc[:, :len(column_names)]\n",
        "            df.columns = column_names\n",
        "\n",
        "            subj_data = {}\n",
        "            for label_code, activity_name in target_activities_map.items():\n",
        "                activity_df = df[df['activity_id'] == label_code].copy()\n",
        "                if not activity_df.empty:\n",
        "                    subj_data[activity_name] = activity_df.drop(columns=['activity_id'])\n",
        "\n",
        "            full_dataset[subj_key] = subj_data\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {file_name}: {e}\")\n",
        "            pass\n",
        "\n",
        "    return full_dataset\n",
        "\n",
        "\n",
        "def prepare_trial_list(label_config, full_data, target_map, feature_map):\n",
        "    trial_list = []\n",
        "    for subj, act_id, gt_count in label_config:\n",
        "        act_name = target_map.get(act_id)\n",
        "        feats = feature_map.get(act_id)\n",
        "\n",
        "        if subj in full_data and act_name in full_data[subj]:\n",
        "            raw_df = full_data[subj][act_name][feats]\n",
        "            raw_np = raw_df.values.astype(np.float32)\n",
        "\n",
        "            # Z-score 정규화 (표준화) 평균=0, std=1\n",
        "            mean = raw_np.mean(axis=0)\n",
        "            std = raw_np.std(axis=0) + 1e-6\n",
        "            norm_np = (raw_np - mean) / std\n",
        "\n",
        "            trial_list.append({\n",
        "                'data': norm_np,              # (T, C)\n",
        "                'count': float(gt_count),      # trial total count\n",
        "                'meta': f\"{subj}_{act_name}\"\n",
        "            })\n",
        "        else:\n",
        "            print(f\"[Skip] Missing data for {subj} - {act_name}\")\n",
        "\n",
        "    return trial_list\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# ✅ Mixed builder helpers\n",
        "# =============================================================================\n",
        "def get_single_trial_from_full_data(subj, act_id, gt_count, full_data, target_map, feature_map, normalize=True):\n",
        "    act_name = target_map.get(act_id)\n",
        "    feats = feature_map.get(act_id)\n",
        "\n",
        "    if subj not in full_data or act_name not in full_data[subj]:\n",
        "        return None\n",
        "\n",
        "    raw_df = full_data[subj][act_name][feats]\n",
        "    x = raw_df.values.astype(np.float32)\n",
        "\n",
        "    if normalize:\n",
        "        mean = x.mean(axis=0)\n",
        "        std = x.std(axis=0) + 1e-6\n",
        "        x = (x - mean) / std\n",
        "\n",
        "    return {\n",
        "        \"data\": x,\n",
        "        \"count\": float(gt_count),\n",
        "        \"meta\": f\"{subj}_{act_name}\"\n",
        "    }\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# ✅ Mixed A-B builder (kept; not used in pattern runs but left intact)\n",
        "# =============================================================================\n",
        "def build_mixed_ab_trial(subj, actA_id, actB_id, config, full_data):\n",
        "    gt_map = config.get(\"GT_BY_ACT\", {})\n",
        "    if actA_id not in gt_map or actB_id not in gt_map:\n",
        "        return None\n",
        "    if subj not in gt_map[actA_id] or subj not in gt_map[actB_id]:\n",
        "        return None\n",
        "\n",
        "    gtA = float(gt_map[actA_id][subj])\n",
        "    gtB = float(gt_map[actB_id][subj])\n",
        "    gt_total = gtA + gtB\n",
        "\n",
        "    A = get_single_trial_from_full_data(\n",
        "        subj, actA_id, gtA, full_data,\n",
        "        config[\"TARGET_ACTIVITIES_MAP\"], config[\"ACT_FEATURE_MAP\"],\n",
        "        normalize=False\n",
        "    )\n",
        "    B = get_single_trial_from_full_data(\n",
        "        subj, actB_id, gtB, full_data,\n",
        "        config[\"TARGET_ACTIVITIES_MAP\"], config[\"ACT_FEATURE_MAP\"],\n",
        "        normalize=False\n",
        "    )\n",
        "    if A is None or B is None:\n",
        "        return None\n",
        "\n",
        "    xA_raw = A[\"data\"]\n",
        "    xB_raw = B[\"data\"]\n",
        "    boundary = int(xA_raw.shape[0])\n",
        "\n",
        "    x_mix_raw = np.concatenate([xA_raw, xB_raw], axis=0).astype(np.float32)\n",
        "\n",
        "    mean = x_mix_raw.mean(axis=0)\n",
        "    std = x_mix_raw.std(axis=0) + 1e-6\n",
        "    x_mix = (x_mix_raw - mean) / std\n",
        "\n",
        "    xA = (xA_raw - mean) / std\n",
        "    xB = (xB_raw - mean) / std\n",
        "\n",
        "    actA_name = config[\"TARGET_ACTIVITIES_MAP\"].get(actA_id, str(actA_id))\n",
        "    actB_name = config[\"TARGET_ACTIVITIES_MAP\"].get(actB_id, str(actB_id))\n",
        "\n",
        "    return {\n",
        "        \"data\": x_mix,\n",
        "        \"count\": float(gt_total),\n",
        "        \"meta\": f\"{subj}__{actA_name}__TO__{actB_name}\",\n",
        "        \"boundary\": boundary,\n",
        "        \"meta_detail\": {\n",
        "            \"subj\": subj,\n",
        "            \"actA_id\": actA_id, \"actB_id\": actB_id,\n",
        "            \"actA_name\": actA_name, \"actB_name\": actB_name,\n",
        "            \"gtA\": gtA, \"gtB\": gtB,\n",
        "            \"gt_total\": gt_total,\n",
        "        },\n",
        "        \"data_A\": xA,\n",
        "        \"data_B\": xB,\n",
        "        \"T_A\": int(xA.shape[0]),\n",
        "        \"T_B\": int(xB.shape[0]),\n",
        "    }\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# ✅ Mixed builder with arbitrary pattern (e.g., \"ABAB\", \"AABB\", \"ABBA\", ... / now AAAAB used)\n",
        "# =============================================================================\n",
        "def build_mixed_pattern_trial(subj, actA_id, actB_id, pattern, config, full_data):\n",
        "    gt_map = config.get(\"GT_BY_ACT\", {})\n",
        "    if actA_id not in gt_map or actB_id not in gt_map:\n",
        "        return None\n",
        "    if subj not in gt_map[actA_id] or subj not in gt_map[actB_id]:\n",
        "        return None\n",
        "\n",
        "    pattern = pattern.strip().upper()\n",
        "    if (len(pattern) < 2) or (not set(pattern).issubset({\"A\", \"B\"})):\n",
        "        raise ValueError(f\"pattern must be a string of A/B with len>=2, got: {pattern}\")\n",
        "\n",
        "    gtA = float(gt_map[actA_id][subj])\n",
        "    gtB = float(gt_map[actB_id][subj])\n",
        "    gt_total = (pattern.count(\"A\") * gtA) + (pattern.count(\"B\") * gtB)\n",
        "\n",
        "    A = get_single_trial_from_full_data(\n",
        "        subj, actA_id, gtA, full_data,\n",
        "        config[\"TARGET_ACTIVITIES_MAP\"], config[\"ACT_FEATURE_MAP\"],\n",
        "        normalize=False\n",
        "    )\n",
        "    B = get_single_trial_from_full_data(\n",
        "        subj, actB_id, gtB, full_data,\n",
        "        config[\"TARGET_ACTIVITIES_MAP\"], config[\"ACT_FEATURE_MAP\"],\n",
        "        normalize=False\n",
        "    )\n",
        "    if A is None or B is None:\n",
        "        return None\n",
        "\n",
        "    xA_raw = A[\"data\"]\n",
        "    xB_raw = B[\"data\"]\n",
        "    TA = int(xA_raw.shape[0])\n",
        "    TB = int(xB_raw.shape[0])\n",
        "\n",
        "    seg_raw = []\n",
        "    seg_lens = []\n",
        "    for ch in pattern:\n",
        "        if ch == \"A\":\n",
        "            seg_raw.append(xA_raw)\n",
        "            seg_lens.append(TA)\n",
        "        else:\n",
        "            seg_raw.append(xB_raw)\n",
        "            seg_lens.append(TB)\n",
        "\n",
        "    x_mix_raw = np.concatenate(seg_raw, axis=0).astype(np.float32)\n",
        "\n",
        "    # single z-score on mixed\n",
        "    mean = x_mix_raw.mean(axis=0)\n",
        "    std = x_mix_raw.std(axis=0) + 1e-6\n",
        "    x_mix = (x_mix_raw - mean) / std\n",
        "\n",
        "    # boundaries: cumulative sums except last\n",
        "    boundaries = []\n",
        "    cs = 0\n",
        "    for L in seg_lens[:-1]:\n",
        "        cs += int(L)\n",
        "        boundaries.append(cs)\n",
        "\n",
        "    # per-segment normalized views (same mean/std)\n",
        "    segments = []\n",
        "    for ch in pattern:\n",
        "        if ch == \"A\":\n",
        "            segments.append((xA_raw - mean) / std)\n",
        "        else:\n",
        "            segments.append((xB_raw - mean) / std)\n",
        "\n",
        "    actA_name = config[\"TARGET_ACTIVITIES_MAP\"].get(actA_id, str(actA_id))\n",
        "    actB_name = config[\"TARGET_ACTIVITIES_MAP\"].get(actB_id, str(actB_id))\n",
        "\n",
        "    # readable meta\n",
        "    name_seq = [(actA_name if ch == \"A\" else actB_name) for ch in pattern]\n",
        "    meta = f\"{subj}__\" + \"__TO__\".join(name_seq)\n",
        "\n",
        "    return {\n",
        "        \"data\": x_mix,\n",
        "        \"count\": float(gt_total),\n",
        "        \"meta\": meta,\n",
        "        \"pattern\": pattern,\n",
        "        \"boundaries\": boundaries,\n",
        "        \"meta_detail\": {\n",
        "            \"subj\": subj,\n",
        "            \"actA_id\": actA_id, \"actB_id\": actB_id,\n",
        "            \"actA_name\": actA_name, \"actB_name\": actB_name,\n",
        "            \"gtA\": gtA, \"gtB\": gtB,\n",
        "            \"gt_total\": gt_total,\n",
        "            \"pattern\": pattern,\n",
        "        },\n",
        "        \"segments\": segments,     # list of np arrays, len = len(pattern)\n",
        "        \"seg_lens\": seg_lens,     # list of ints\n",
        "        \"T_A\": TA,\n",
        "        \"T_B\": TB,\n",
        "    }\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# 2.5) ✅ Windowing\n",
        "# =============================================================================\n",
        "def trial_list_to_windows(trial_list, fs, win_sec=8.0, stride_sec=4.0, drop_last=True):\n",
        "    win_len = int(round(win_sec * fs))\n",
        "    stride = int(round(stride_sec * fs))\n",
        "    assert win_len > 0 and stride > 0\n",
        "\n",
        "    windows = []\n",
        "    for item in trial_list:\n",
        "        x = item[\"data\"]\n",
        "        T = x.shape[0]\n",
        "        total_count = float(item[\"count\"])\n",
        "        meta = item[\"meta\"]\n",
        "\n",
        "        total_dur = max(T / float(fs), 1e-6)\n",
        "        rate_trial = total_count / total_dur\n",
        "\n",
        "        if T < win_len:\n",
        "            win_dur = T / float(fs)\n",
        "            windows.append({\n",
        "                \"data\": x,\n",
        "                \"count\": rate_trial * win_dur,\n",
        "                \"meta\": f\"{meta}__win[0:{T}]\",\n",
        "                \"parent_meta\": meta,\n",
        "                \"parent_T\": T,\n",
        "                \"win_start\": 0,\n",
        "                \"win_end\": T,\n",
        "            })\n",
        "            continue\n",
        "\n",
        "        last_start = T - win_len\n",
        "        starts = list(range(0, last_start + 1, stride))\n",
        "\n",
        "        for st in starts:\n",
        "            ed = st + win_len\n",
        "            win_dur = win_len / float(fs)\n",
        "            windows.append({\n",
        "                \"data\": x[st:ed],\n",
        "                \"count\": rate_trial * win_dur,\n",
        "                \"meta\": f\"{meta}__win[{st}:{ed}]\",\n",
        "                \"parent_meta\": meta,\n",
        "                \"parent_T\": T,\n",
        "                \"win_start\": st,\n",
        "                \"win_end\": ed,\n",
        "            })\n",
        "\n",
        "        if not drop_last:\n",
        "            last_st = starts[-1] + stride\n",
        "            if last_st < T:\n",
        "                ed = T\n",
        "                win_dur = (ed - last_st) / float(fs)\n",
        "                windows.append({\n",
        "                    \"data\": x[last_st:ed],\n",
        "                    \"count\": rate_trial * win_dur,\n",
        "                    \"meta\": f\"{meta}__win[{last_st}:{ed}]\",\n",
        "                    \"parent_meta\": meta,\n",
        "                    \"parent_T\": T,\n",
        "                    \"win_start\": last_st,\n",
        "                    \"win_end\": ed,\n",
        "                })\n",
        "\n",
        "    return windows\n",
        "\n",
        "\n",
        "def predict_count_by_windowing(model, x_np, fs, win_sec, stride_sec, device, tau=1.0, batch_size=64):\n",
        "    win_len = int(round(win_sec * fs))\n",
        "    stride = int(round(stride_sec * fs))\n",
        "    T = x_np.shape[0]\n",
        "    total_dur = T / float(fs)\n",
        "\n",
        "    if T <= win_len:\n",
        "        x_tensor = torch.tensor(x_np, dtype=torch.float32).transpose(0, 1).unsqueeze(0).to(device)\n",
        "        with torch.no_grad():\n",
        "            rate_hat, _, _, _ = model(x_tensor, mask=None, tau=tau)\n",
        "        pred_count = float(rate_hat.item() * total_dur)\n",
        "        return pred_count, np.array([float(rate_hat.item())], dtype=np.float32)\n",
        "\n",
        "    starts = list(range(0, T - win_len + 1, stride))\n",
        "    windows = np.stack([x_np[st:st + win_len] for st in starts], axis=0)\n",
        "\n",
        "    xw = torch.tensor(windows, dtype=torch.float32).permute(0, 2, 1).to(device)\n",
        "\n",
        "    rates = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, xw.shape[0], batch_size):\n",
        "            xb = xw[i:i + batch_size]\n",
        "            r_hat, _, _, _ = model(xb, mask=None, tau=tau)\n",
        "            rates.append(r_hat.detach().cpu().numpy())\n",
        "\n",
        "    rates = np.concatenate(rates, axis=0)\n",
        "    rate_mean = float(rates.mean())\n",
        "    pred_count = rate_mean * total_dur\n",
        "    return float(pred_count), rates\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# ✅ Helpers for scenario-avg plots\n",
        "# =============================================================================\n",
        "def get_window_rate_curve(model, x_np, fs, win_sec, stride_sec, device, tau=1.0, batch_size=64):\n",
        "    win_len = int(round(win_sec * fs))\n",
        "    stride = int(round(stride_sec * fs))\n",
        "    T = x_np.shape[0]\n",
        "\n",
        "    if T <= win_len:\n",
        "        pred_count, rates = predict_count_by_windowing(\n",
        "            model, x_np, fs, win_sec, stride_sec, device, tau=tau, batch_size=batch_size\n",
        "        )\n",
        "        t_cent = np.array([0.5 * (T / float(fs))], dtype=np.float32)\n",
        "        return t_cent, rates.astype(np.float32)\n",
        "\n",
        "    starts = list(range(0, T - win_len + 1, stride))\n",
        "    windows = np.stack([x_np[st:st + win_len] for st in starts], axis=0)\n",
        "\n",
        "    xw = torch.tensor(windows, dtype=torch.float32).permute(0, 2, 1).to(device)\n",
        "\n",
        "    rates = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, xw.shape[0], batch_size):\n",
        "            xb = xw[i:i + batch_size]\n",
        "            r_hat, _, _, _ = model(xb, mask=None, tau=tau)\n",
        "            rates.append(r_hat.detach().cpu().numpy())\n",
        "    rates = np.concatenate(rates, axis=0).astype(np.float32)\n",
        "\n",
        "    centers = np.array([(st + 0.5 * win_len) / float(fs) for st in starts], dtype=np.float32)\n",
        "    return centers, rates\n",
        "\n",
        "\n",
        "def resample_1d(y, new_len):\n",
        "    y = np.asarray(y, dtype=np.float32)\n",
        "    if y.size == 0:\n",
        "        return np.zeros((new_len,), dtype=np.float32)\n",
        "    if y.size == 1:\n",
        "        return np.full((new_len,), float(y[0]), dtype=np.float32)\n",
        "    x_old = np.linspace(0.0, 1.0, num=y.size, dtype=np.float32)\n",
        "    x_new = np.linspace(0.0, 1.0, num=new_len, dtype=np.float32)\n",
        "    return np.interp(x_new, x_old, y).astype(np.float32)\n",
        "\n",
        "\n",
        "def resample_2d(Y, new_len):\n",
        "    Y = np.asarray(Y, dtype=np.float32)\n",
        "    if Y.shape[0] == 0:\n",
        "        return np.zeros((new_len, Y.shape[1]), dtype=np.float32)\n",
        "    if Y.shape[0] == 1:\n",
        "        return np.repeat(Y, repeats=new_len, axis=0).astype(np.float32)\n",
        "    x_old = np.linspace(0.0, 1.0, num=Y.shape[0], dtype=np.float32)\n",
        "    x_new = np.linspace(0.0, 1.0, num=new_len, dtype=np.float32)\n",
        "    out = []\n",
        "    for d in range(Y.shape[1]):\n",
        "        out.append(np.interp(x_new, x_old, Y[:, d]))\n",
        "    return np.stack(out, axis=1).astype(np.float32)\n",
        "\n",
        "\n",
        "def global_pca2(Z_all):\n",
        "    \"\"\"\n",
        "    Z_all: (N,D)\n",
        "    returns: mean (D,), V2 (2,D)  such that proj = (Z-mean) @ V2.T\n",
        "    \"\"\"\n",
        "    Z_all = np.asarray(Z_all, dtype=np.float32)\n",
        "    mu = Z_all.mean(axis=0, keepdims=True)\n",
        "    Zc = Z_all - mu\n",
        "    _, _, Vt = np.linalg.svd(Zc, full_matrices=False)\n",
        "    V2 = Vt[:2].astype(np.float32)  # (2,D)\n",
        "    return mu.squeeze(0).astype(np.float32), V2\n",
        "\n",
        "\n",
        "def compute_phase_entropy_mean(phase_p_np, eps=1e-8):\n",
        "    phase_p_np = np.asarray(phase_p_np, dtype=np.float32)\n",
        "    ent_t = -(phase_p_np * np.log(phase_p_np + eps)).sum(axis=1)\n",
        "    return float(ent_t.mean())\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# ✅ NEW: Scenario plot (TWO PLOTS) — \"two_plots\" style 그대로\n",
        "# =============================================================================\n",
        "def plot_scenario_mean_std_two_plots(\n",
        "    save_dir,\n",
        "    scen_name,\n",
        "    win_t, win_mean, win_std,\n",
        "    lat_xy_mean, lat_xy_std,   # (std는 지금 안 쓰지만 signature 유지)\n",
        "    n_subjects,\n",
        "    pattern_len,\n",
        "    dpi=600,\n",
        "):\n",
        "    _ensure_dir(save_dir if save_dir is not None else \"scenario_viz\")\n",
        "    out_dir = save_dir if save_dir is not None else \"scenario_viz\"\n",
        "    scen_tag = _safe_fname(scen_name)\n",
        "\n",
        "    # -------------------------\n",
        "    # (1) Window-level rep_rate mean±std (+ boundary + segment shading)\n",
        "    # - A/B 글자 없음\n",
        "    # - figsize/폰트 크게\n",
        "    # -------------------------\n",
        "    fig1 = plt.figure(figsize=(26.0, 9.2))\n",
        "    ax = fig1.gca()\n",
        "\n",
        "    # segment shading (alternating alpha, subtle)\n",
        "    for i in range(pattern_len):\n",
        "        a0 = float(i)\n",
        "        a1 = float(i + 1)\n",
        "        alpha = 0.06 if (i % 2 == 0) else 0.03\n",
        "        ax.axvspan(a0, a1, alpha=alpha, color=\"k\", lw=0)\n",
        "\n",
        "    ax.plot(win_t, win_mean, linewidth=6.4, label=\"Window rep_rate (mean)\")\n",
        "    ax.fill_between(win_t, win_mean - win_std, win_mean + win_std, alpha=0.22, label=\"±1 std\")\n",
        "\n",
        "    # boundaries at integers\n",
        "    for xline in range(1, pattern_len):\n",
        "        ax.axvline(float(xline), linestyle=\"--\", linewidth=6.0, alpha=0.95, label=(\"Boundary\" if xline == 1 else None))\n",
        "\n",
        "    ax.set_xlabel(f\"Normalized time (0→{pattern_len})\", fontsize=46, labelpad=10)\n",
        "    ax.set_ylabel(\"rep_rate (reps/s)\", fontsize=46, labelpad=10)\n",
        "    ax.tick_params(axis=\"both\", labelsize=37)\n",
        "\n",
        "    ax.legend(fontsize=38, frameon=True, loc=\"upper left\")\n",
        "    fig1.tight_layout()\n",
        "    fig1.savefig(os.path.join(out_dir, f\"{scen_tag}__1_win_rep_rate.png\"), dpi=dpi, bbox_inches=\"tight\")\n",
        "    plt.close(fig1)\n",
        "\n",
        "    # -------------------------\n",
        "    # (2) Latent trajectory PCA2\n",
        "    # - std circle 없음\n",
        "    # - trajectory line 없음 (scatter only)\n",
        "    # - square bounds 유지\n",
        "    # -------------------------\n",
        "    fig2 = plt.figure(figsize=(8.8, 8.8))\n",
        "    ax = fig2.gca()\n",
        "\n",
        "    T = lat_xy_mean.shape[0]\n",
        "    t_lat = np.linspace(0.0, float(pattern_len), num=T, dtype=np.float32)\n",
        "    sc = ax.scatter(lat_xy_mean[:, 0], lat_xy_mean[:, 1], c=t_lat, s=22, alpha=0.95)\n",
        "\n",
        "    ax.set_xlabel(\"PC1\", fontsize=24, labelpad=10)\n",
        "    ax.set_ylabel(\"PC2\", fontsize=24, labelpad=10)\n",
        "    ax.tick_params(axis=\"both\", labelsize=18)\n",
        "    ax.set_aspect(\"equal\", adjustable=\"box\")\n",
        "\n",
        "    x = lat_xy_mean[:, 0]\n",
        "    y = lat_xy_mean[:, 1]\n",
        "    xmin, xmax = float(np.min(x)), float(np.max(x))\n",
        "    ymin, ymax = float(np.min(y)), float(np.max(y))\n",
        "    cx = 0.5 * (xmin + xmax)\n",
        "    cy = 0.5 * (ymin + ymax)\n",
        "    rx = 0.5 * (xmax - xmin)\n",
        "    ry = 0.5 * (ymax - ymin)\n",
        "    r = max(rx, ry) * 1.12 + 1e-6\n",
        "    ax.set_xlim(cx - r, cx + r)\n",
        "    ax.set_ylim(cy - r, cy + r)\n",
        "\n",
        "    cbar = fig2.colorbar(sc, ax=ax, fraction=0.046, pad=0.04)\n",
        "    cbar.set_label(f\"normalized time (0→{pattern_len})\", fontsize=20, labelpad=10)\n",
        "    cbar.ax.tick_params(labelsize=16)\n",
        "\n",
        "    fig2.tight_layout()\n",
        "    fig2.savefig(os.path.join(out_dir, f\"{scen_tag}__2_latent_pca2.png\"), dpi=dpi, bbox_inches=\"tight\")\n",
        "    plt.close(fig2)\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# 2.8) Dataset / Collate\n",
        "# =============================================================================\n",
        "class TrialDataset(Dataset):\n",
        "    def __init__(self, trial_list):\n",
        "        self.trials = trial_list\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.trials)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.trials[idx]\n",
        "        data = torch.tensor(item['data'], dtype=torch.float32).transpose(0, 1)  # (C, T)\n",
        "        count = torch.tensor(item['count'], dtype=torch.float32)\n",
        "        return data, count, item['meta']\n",
        "\n",
        "\n",
        "def collate_variable_length(batch):\n",
        "    max_len = max([x[0].shape[1] for x in batch])\n",
        "    C = batch[0][0].shape[0]\n",
        "\n",
        "    padded_data, masks, counts, metas, lengths = [], [], [], [], []\n",
        "    for data, count, meta in batch:\n",
        "        T = data.shape[1]\n",
        "        lengths.append(T)\n",
        "\n",
        "        pad_size = max_len - T\n",
        "        if pad_size > 0:\n",
        "            pad = torch.zeros(C, pad_size)\n",
        "            d_padded = torch.cat([data, pad], dim=1)\n",
        "            mask = torch.cat([torch.ones(T), torch.zeros(pad_size)], dim=0)\n",
        "        else:\n",
        "            d_padded = data\n",
        "            mask = torch.ones(T)\n",
        "\n",
        "        padded_data.append(d_padded)\n",
        "        masks.append(mask)\n",
        "        counts.append(count)\n",
        "        metas.append(meta)\n",
        "\n",
        "    return {\n",
        "        \"data\": torch.stack(padded_data),\n",
        "        \"mask\": torch.stack(masks),\n",
        "        \"count\": torch.stack(counts),\n",
        "        \"length\": torch.tensor(lengths, dtype=torch.float32),\n",
        "        \"meta\": metas\n",
        "    }\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# 3) Model\n",
        "# =============================================================================\n",
        "class ManifoldEncoder(nn.Module):\n",
        "    def __init__(self, input_ch, hidden_dim=128, latent_dim=16):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv1d(input_ch, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, latent_dim, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        z = self.net(x)            # (B, D, T)\n",
        "        z = z.transpose(1, 2)      # (B, T, D)\n",
        "        return z\n",
        "\n",
        "\n",
        "class ManifoldDecoder(nn.Module):\n",
        "    def __init__(self, latent_dim, hidden_dim, out_ch):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv1d(latent_dim, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, out_ch, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        zt = z.transpose(1, 2)\n",
        "        x_hat = self.net(zt)\n",
        "        return x_hat\n",
        "\n",
        "\n",
        "class MultiRateHead(nn.Module):\n",
        "    def __init__(self, latent_dim=16, hidden=64, K_max=6):\n",
        "        super().__init__()\n",
        "        self.K_max = K_max\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(latent_dim, hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden, 1 + K_max)\n",
        "        )\n",
        "\n",
        "    def forward(self, z, tau=1.0):\n",
        "        out = self.net(z)\n",
        "        amp = F.softplus(out[..., 0])\n",
        "        phase_logits = out[..., 1:]\n",
        "        phase = F.softmax(phase_logits / tau, dim=-1)\n",
        "        return amp, phase, phase_logits\n",
        "\n",
        "\n",
        "class KAutoCountModel(nn.Module):\n",
        "    def __init__(self, input_ch, hidden_dim=128, latent_dim=16, K_max=6, k_hidden=64):\n",
        "        super().__init__()\n",
        "        self.encoder = ManifoldEncoder(input_ch, hidden_dim, latent_dim)\n",
        "        self.decoder = ManifoldDecoder(latent_dim, hidden_dim, input_ch)\n",
        "        self.rate_head = MultiRateHead(latent_dim, hidden=hidden_dim, K_max=K_max)\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, (nn.Conv1d, nn.Linear)):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            b = self.rate_head.net[-1].bias\n",
        "            b.zero_()\n",
        "            b[0].fill_(-2.0)\n",
        "\n",
        "    @staticmethod\n",
        "    def _masked_mean_time(x, mask=None, eps=1e-6):\n",
        "        if mask is None:\n",
        "            return x.mean(dim=1)\n",
        "        if x.dim() == 2:\n",
        "            m = mask.to(dtype=x.dtype, device=x.device)\n",
        "            return (x * m).sum(dim=1) / (m.sum(dim=1) + eps)\n",
        "        elif x.dim() == 3:\n",
        "            m = mask.to(dtype=x.dtype, device=x.device).unsqueeze(-1)\n",
        "            return (x * m).sum(dim=1) / (m.sum(dim=1) + eps)\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported dim for masked mean: {x.dim()}\")\n",
        "\n",
        "    def forward(self, x, mask=None, tau=1.0):\n",
        "        z = self.encoder(x)\n",
        "        x_hat = self.decoder(z)\n",
        "\n",
        "        amp_t, phase_p, phase_logits = self.rate_head(z, tau=tau)\n",
        "        rates_k_t = amp_t.unsqueeze(-1) * phase_p\n",
        "\n",
        "        micro_rate_t = amp_t\n",
        "\n",
        "        p_bar = self._masked_mean_time(phase_p, mask)\n",
        "        k_hat = 1.0 / (p_bar.pow(2).sum(dim=1) + 1e-6)\n",
        "\n",
        "        rep_rate_t = micro_rate_t / (k_hat.unsqueeze(1) + 1e-6)\n",
        "        if mask is not None:\n",
        "            rep_rate_t = rep_rate_t * mask\n",
        "\n",
        "        if mask is None:\n",
        "            avg_rep_rate = rep_rate_t.mean(dim=1)\n",
        "        else:\n",
        "            avg_rep_rate = (rep_rate_t * mask).sum(dim=1) / (mask.sum(dim=1) + 1e-6)\n",
        "\n",
        "        aux = {\n",
        "            \"rates_k_t\": rates_k_t,\n",
        "            \"phase_p\": phase_p,\n",
        "            \"phase_logits\": phase_logits,\n",
        "            \"micro_rate_t\": micro_rate_t,\n",
        "            \"rep_rate_t\": rep_rate_t,\n",
        "            \"k_hat\": k_hat,\n",
        "        }\n",
        "        return avg_rep_rate, z, x_hat, aux\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# 4) Loss utils\n",
        "# =============================================================================\n",
        "def masked_recon_mse(x_hat, x, mask, eps=1e-6):\n",
        "    mask = mask.to(dtype=x.dtype, device=x.device)\n",
        "    mask_bc = mask.unsqueeze(1)\n",
        "    se = (x_hat - x) ** 2\n",
        "    se = se * mask_bc\n",
        "    denom = (mask.sum() * x.shape[1]) + eps\n",
        "    return se.sum() / denom\n",
        "\n",
        "\n",
        "def temporal_smoothness(v, mask=None, eps=1e-6):\n",
        "    dv = torch.abs(v[:, 1:] - v[:, :-1])\n",
        "    if mask is None:\n",
        "        return dv.mean()\n",
        "    m = mask[:, 1:] * mask[:, :-1]\n",
        "    m = m.to(dtype=dv.dtype, device=dv.device)\n",
        "    return (dv * m).sum() / (m.sum() + eps)\n",
        "\n",
        "\n",
        "def phase_entropy_loss(phase_p, mask=None, eps=1e-8):\n",
        "    ent = -(phase_p * (phase_p + eps).log()).sum(dim=-1)\n",
        "    if mask is None:\n",
        "        return ent.mean()\n",
        "    ent = ent * mask\n",
        "    return ent.sum() / (mask.sum() + eps)\n",
        "\n",
        "\n",
        "def effK_usage_loss(phase_p, mask=None, eps=1e-6):\n",
        "    if mask is None:\n",
        "        p_bar = phase_p.mean(dim=1)\n",
        "    else:\n",
        "        m = mask.to(dtype=phase_p.dtype, device=phase_p.device).unsqueeze(-1)\n",
        "        p_bar = (phase_p * m).sum(dim=1) / (m.sum(dim=1) + eps)\n",
        "\n",
        "    effK = 1.0 / (p_bar.pow(2).sum(dim=1) + eps)\n",
        "    return effK.mean(), effK.detach()\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# 5) Train\n",
        "# =============================================================================\n",
        "def train_one_epoch(model, loader, optimizer, config, device):\n",
        "    model.train()\n",
        "    stats = {k: 0.0 for k in [\n",
        "        'loss', 'loss_rate', 'loss_recon', 'loss_smooth', 'loss_phase_ent', 'loss_effk',\n",
        "        'mae_count'\n",
        "    ]}\n",
        "\n",
        "    fs = config[\"fs\"]\n",
        "    tau = config.get(\"tau\", 1.0)\n",
        "\n",
        "    lam_recon = config.get(\"lambda_recon\", 1.0)\n",
        "    lam_smooth = config.get(\"lambda_smooth\", 0.05)\n",
        "    lam_phase_ent = config.get(\"lambda_phase_ent\", 0.01)\n",
        "    lam_effk = config.get(\"lambda_effk\", 0.005)\n",
        "\n",
        "    for batch in loader:\n",
        "        x = batch[\"data\"].to(device)\n",
        "        mask = batch[\"mask\"].to(device)\n",
        "        y_count = batch[\"count\"].to(device)\n",
        "        length = batch[\"length\"].to(device)\n",
        "\n",
        "        duration = torch.clamp(length / fs, min=1e-6)\n",
        "        y_rate = y_count / duration\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        rate_hat, z, x_hat, aux = model(x, mask, tau=tau)\n",
        "\n",
        "        loss_rate = F.mse_loss(rate_hat, y_rate)\n",
        "        loss_recon = masked_recon_mse(x_hat, x, mask)\n",
        "        loss_smooth = temporal_smoothness(aux[\"rep_rate_t\"], mask)\n",
        "        loss_phase_ent = phase_entropy_loss(aux[\"phase_p\"], mask)\n",
        "        loss_effk, _ = effK_usage_loss(aux[\"phase_p\"], mask)\n",
        "\n",
        "        loss = (loss_rate\n",
        "                + lam_recon * loss_recon\n",
        "                + lam_smooth * loss_smooth\n",
        "                + lam_phase_ent * loss_phase_ent\n",
        "                + lam_effk * loss_effk)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        count_hat = rate_hat * duration\n",
        "        stats['loss'] += loss.item()\n",
        "        stats['loss_rate'] += loss_rate.item()\n",
        "        stats['loss_recon'] += loss_recon.item()\n",
        "        stats['loss_smooth'] += loss_smooth.item()\n",
        "        stats['loss_phase_ent'] += loss_phase_ent.item()\n",
        "        stats['loss_effk'] += loss_effk.item()\n",
        "        stats['mae_count'] += torch.abs(count_hat - y_count).mean().item()\n",
        "\n",
        "    n = len(loader)\n",
        "    return {k: v / n for k, v in stats.items()}\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# 6) Main (Scenario-wise LOSO)  - Pattern runs (AAAAB)\n",
        "# =============================================================================\n",
        "def main():\n",
        "    CONFIG = {\n",
        "        \"seed\": 42,\n",
        "        \"data_dir\": \"/content/drive/MyDrive/Colab Notebooks/HAR_data/MHEALTHDATASET\",\n",
        "\n",
        "        \"COLUMN_NAMES\": [\n",
        "            'acc_chest_x', 'acc_chest_y', 'acc_chest_z',\n",
        "            'ecg_1', 'ecg_2',\n",
        "            'acc_ankle_x', 'acc_ankle_y', 'acc_ankle_z',\n",
        "            'gyro_ankle_x', 'gyro_ankle_y', 'gyro_ankle_z',\n",
        "            'mag_ankle_x', 'mag_ankle_y', 'mag_ankle_z',\n",
        "            'acc_arm_x', 'acc_arm_y', 'acc_arm_z',\n",
        "            'gyro_arm_x', 'gyro_arm_y', 'gyro_arm_z',\n",
        "            'mag_arm_x', 'mag_arm_y', 'mag_arm_z',\n",
        "            'activity_id'\n",
        "        ],\n",
        "\n",
        "        \"TARGET_ACTIVITIES_MAP\": {\n",
        "            6: 'Waist bends forward',\n",
        "            7: 'Frontal elevation of arms',\n",
        "            8: 'Knees bending',\n",
        "            10: 'Jogging',\n",
        "            11: 'Running',\n",
        "        },\n",
        "\n",
        "        \"ACT_FEATURE_MAP\": {\n",
        "            6: ['acc_chest_x', 'acc_chest_y', 'acc_chest_z',\n",
        "                'acc_ankle_x', 'acc_ankle_y', 'acc_ankle_z',\n",
        "                'gyro_ankle_x', 'gyro_ankle_y', 'gyro_ankle_z',\n",
        "                'acc_arm_x', 'acc_arm_y', 'acc_arm_z',\n",
        "                'gyro_arm_x', 'gyro_arm_y', 'gyro_arm_z'\n",
        "                ],\n",
        "            7: ['acc_chest_x', 'acc_chest_y', 'acc_chest_z',\n",
        "                'acc_ankle_x', 'acc_ankle_y', 'acc_ankle_z',\n",
        "                'gyro_ankle_x', 'gyro_ankle_y', 'gyro_ankle_z',\n",
        "                'acc_arm_x', 'acc_arm_y', 'acc_arm_z',\n",
        "                'gyro_arm_x', 'gyro_arm_y', 'gyro_arm_z'\n",
        "                ],\n",
        "            8: ['acc_chest_x', 'acc_chest_y', 'acc_chest_z',\n",
        "                'acc_ankle_x', 'acc_ankle_y', 'acc_ankle_z',\n",
        "                'gyro_ankle_x', 'gyro_ankle_y', 'gyro_ankle_z',\n",
        "                'acc_arm_x', 'acc_arm_y', 'acc_arm_z',\n",
        "                'gyro_arm_x', 'gyro_arm_y', 'gyro_arm_z'\n",
        "                ],\n",
        "            10: ['acc_chest_x', 'acc_chest_y', 'acc_chest_z',\n",
        "                 'acc_ankle_x', 'acc_ankle_y', 'acc_ankle_z',\n",
        "                 'gyro_ankle_x', 'gyro_ankle_y', 'gyro_ankle_z',\n",
        "                 'acc_arm_x', 'acc_arm_y', 'acc_arm_z',\n",
        "                 'gyro_arm_x', 'gyro_arm_y', 'gyro_arm_z'\n",
        "                 ],\n",
        "            11: ['acc_chest_x', 'acc_chest_y', 'acc_chest_z',\n",
        "                 'acc_ankle_x', 'acc_ankle_y', 'acc_ankle_z',\n",
        "                 'gyro_ankle_x', 'gyro_ankle_y', 'gyro_ankle_z',\n",
        "                 'acc_arm_x', 'acc_arm_y', 'acc_arm_z',\n",
        "                 'gyro_arm_x', 'gyro_arm_y', 'gyro_arm_z'\n",
        "                 ],\n",
        "        },\n",
        "\n",
        "        # Training Params\n",
        "        \"epochs\": 100,\n",
        "        \"lr\": 5e-4,\n",
        "        \"batch_size\": 64,\n",
        "        \"fs\": 50,\n",
        "\n",
        "        # Windowing Params\n",
        "        \"win_sec\": 8.0,\n",
        "        \"stride_sec\": 4.0,\n",
        "        \"drop_last\": True,\n",
        "\n",
        "        # Model\n",
        "        \"hidden_dim\": 128,\n",
        "        \"latent_dim\": 16,\n",
        "        \"K_max\": 6,\n",
        "\n",
        "        # Loss Weights\n",
        "        \"lambda_recon\": 1.0,\n",
        "        \"lambda_smooth\": 0.05,\n",
        "        \"lambda_phase_ent\": 0.01,\n",
        "        \"lambda_effk\": 0.0075,\n",
        "\n",
        "        \"tau\": 1.0,\n",
        "\n",
        "        \"GT_BY_ACT\": {\n",
        "            6: {\n",
        "                \"subject1\": 21, \"subject2\": 19, \"subject3\": 21, \"subject4\": 20, \"subject5\": 20,\n",
        "                \"subject6\": 20, \"subject7\": 20, \"subject8\": 21, \"subject9\": 21, \"subject10\": 20,\n",
        "            },\n",
        "            7: {\n",
        "                \"subject1\": 20, \"subject2\": 20, \"subject3\": 20, \"subject4\": 20, \"subject5\": 20,\n",
        "                \"subject6\": 20, \"subject7\": 20, \"subject8\": 19, \"subject9\": 19, \"subject10\": 20,\n",
        "            },\n",
        "            8: {\n",
        "                \"subject1\": 20, \"subject2\": 21, \"subject3\": 21, \"subject4\": 19, \"subject5\": 20,\n",
        "                \"subject6\": 20, \"subject7\": 21, \"subject8\": 21, \"subject9\": 21, \"subject10\": 21,\n",
        "            },\n",
        "            10: {\n",
        "                \"subject1\": 157, \"subject2\": 161, \"subject3\": 154, \"subject4\": 154, \"subject5\": 160,\n",
        "                \"subject6\": 156, \"subject7\": 153, \"subject8\": 160, \"subject9\": 166, \"subject10\": 156,\n",
        "            },\n",
        "            11: {\n",
        "                \"subject1\": 165, \"subject2\": 158, \"subject3\": 174, \"subject4\": 163, \"subject5\": 157,\n",
        "                \"subject6\": 172, \"subject7\": 149, \"subject8\": 166, \"subject9\": 174, \"subject10\": 172,\n",
        "            }\n",
        "        },\n",
        "\n",
        "        \"PATTERNS\": [\"AAAAB\"],\n",
        "\n",
        "        \"MIX_SCENARIOS\": [\n",
        "            (\"Running -> Jogging\", 11, 10),\n",
        "            (\"Jogging -> Running\", 10, 11),\n",
        "\n",
        "            (\"Running -> Knees bending\", 11, 8),\n",
        "            (\"Knees bending -> Running\", 8, 11),\n",
        "\n",
        "            (\"Frontal elevation of arms -> Waist bends forward\", 7, 6),\n",
        "            (\"Waist bends forward -> Frontal elevation of arms\", 6, 7),\n",
        "        ],\n",
        "    }\n",
        "\n",
        "    def build_labels_for_act(act_id, subjects_list, gt_by_act, exclude_subj=None):\n",
        "        gt_map = gt_by_act.get(act_id, {})\n",
        "        labels = []\n",
        "        for s in subjects_list:\n",
        "            if exclude_subj is not None and s == exclude_subj:\n",
        "                continue\n",
        "            if s in gt_map:\n",
        "                labels.append((s, act_id, gt_map[s]))\n",
        "        return labels\n",
        "\n",
        "    set_strict_seed(CONFIG[\"seed\"])\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Device: {device}\")\n",
        "\n",
        "    full_data = load_mhealth_dataset(CONFIG[\"data_dir\"], CONFIG[\"TARGET_ACTIVITIES_MAP\"], CONFIG[\"COLUMN_NAMES\"])\n",
        "    if not full_data:\n",
        "        return\n",
        "\n",
        "    subjects = [f\"subject{i}\" for i in range(1, 11)]\n",
        "\n",
        "    VIZ_DIR = \"scenario_viz\"\n",
        "    os.makedirs(VIZ_DIR, exist_ok=True)\n",
        "\n",
        "    # resample lengths (fixed for averaging)  # per-segment\n",
        "    NW = 60     # window-rate points per segment\n",
        "    NL = 200    # latent points per segment\n",
        "\n",
        "    # =========================================================\n",
        "    # Scenario-wise execution (scenario x pattern)\n",
        "    # =========================================================\n",
        "    scen_global_idx = 0\n",
        "    for base_idx, (base_name, actA_id, actB_id) in enumerate(CONFIG.get(\"MIX_SCENARIOS\", []), start=1):\n",
        "        for pattern in CONFIG.get(\"PATTERNS\", [\"AAAAB\"]):\n",
        "            scen_global_idx += 1\n",
        "            scen_name = f\"{base_name} | pattern={pattern}\"\n",
        "            TRAIN_ACT_ID = actA_id  # ✅ always train on A\n",
        "\n",
        "            print(\"\\n\" + \"=\" * 100)\n",
        "            print(f\"1. 시나리오{scen_global_idx}: {scen_name}\")\n",
        "            print(\"=\" * 100)\n",
        "\n",
        "            loso_results = []\n",
        "            scenario_records = []\n",
        "            scenario_diffs = []\n",
        "\n",
        "            pattern_len = len(pattern)\n",
        "\n",
        "            # buffers for scenario-avg plots\n",
        "            buf_win_concat = []    # (pattern_len*NW,)\n",
        "            buf_z_concat = []      # raw z(t): (Tmix,D) per subject\n",
        "            per_subject_latent = []  # {\"z_segs\": [...]}\n",
        "\n",
        "            for fold_idx, test_subj in enumerate(subjects):\n",
        "                set_strict_seed(CONFIG[\"seed\"])\n",
        "\n",
        "                train_labels = build_labels_for_act(TRAIN_ACT_ID, subjects, CONFIG[\"GT_BY_ACT\"], exclude_subj=test_subj)\n",
        "                test_labels  = build_labels_for_act(TRAIN_ACT_ID, subjects, CONFIG[\"GT_BY_ACT\"], exclude_subj=None)\n",
        "                test_labels  = [t for t in test_labels if t[0] == test_subj]\n",
        "\n",
        "                if len(train_labels) == 0 or len(test_labels) == 0:\n",
        "                    print(f\"[Skip Fold] Missing GT for TRAIN_ACT_ID={TRAIN_ACT_ID} ({CONFIG['TARGET_ACTIVITIES_MAP'].get(TRAIN_ACT_ID)}) at {test_subj}\")\n",
        "                    continue\n",
        "\n",
        "                train_trials = prepare_trial_list(train_labels, full_data, CONFIG[\"TARGET_ACTIVITIES_MAP\"], CONFIG[\"ACT_FEATURE_MAP\"])\n",
        "                test_trials  = prepare_trial_list(test_labels,  full_data, CONFIG[\"TARGET_ACTIVITIES_MAP\"], CONFIG[\"ACT_FEATURE_MAP\"])\n",
        "                if not test_trials or not train_trials:\n",
        "                    continue\n",
        "\n",
        "                train_data = trial_list_to_windows(\n",
        "                    train_trials, fs=CONFIG[\"fs\"],\n",
        "                    win_sec=CONFIG[\"win_sec\"], stride_sec=CONFIG[\"stride_sec\"],\n",
        "                    drop_last=CONFIG[\"drop_last\"]\n",
        "                )\n",
        "                if len(train_data) == 0:\n",
        "                    continue\n",
        "\n",
        "                g = torch.Generator()\n",
        "                g.manual_seed(CONFIG[\"seed\"])\n",
        "\n",
        "                train_loader = DataLoader(\n",
        "                    TrialDataset(train_data),\n",
        "                    batch_size=CONFIG[\"batch_size\"],\n",
        "                    shuffle=True,\n",
        "                    collate_fn=collate_variable_length,\n",
        "                    generator=g,\n",
        "                    num_workers=0\n",
        "                )\n",
        "\n",
        "                input_ch = train_data[0]['data'].shape[1]\n",
        "                model = KAutoCountModel(\n",
        "                    input_ch=input_ch,\n",
        "                    hidden_dim=CONFIG[\"hidden_dim\"],\n",
        "                    latent_dim=CONFIG[\"latent_dim\"],\n",
        "                    K_max=CONFIG[\"K_max\"]\n",
        "                ).to(device)\n",
        "\n",
        "                optimizer = torch.optim.Adam(model.parameters(), lr=CONFIG[\"lr\"])\n",
        "                scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.5)\n",
        "\n",
        "                for epoch in range(CONFIG[\"epochs\"]):\n",
        "                    _ = train_one_epoch(model, train_loader, optimizer, CONFIG, device)\n",
        "                    scheduler.step()\n",
        "\n",
        "                model.eval()\n",
        "\n",
        "                # (A) LOSO on TRAIN_ACT_ID\n",
        "                item = test_trials[0]\n",
        "                x_np = item[\"data\"]\n",
        "                count_gt = float(item[\"count\"])\n",
        "                count_pred_win, _ = predict_count_by_windowing(\n",
        "                    model, x_np=x_np,\n",
        "                    fs=CONFIG[\"fs\"], win_sec=CONFIG[\"win_sec\"], stride_sec=CONFIG[\"stride_sec\"],\n",
        "                    device=device, tau=CONFIG.get(\"tau\", 1.0),\n",
        "                    batch_size=CONFIG.get(\"batch_size\", 64)\n",
        "                )\n",
        "                fold_mae = float(abs(count_pred_win - count_gt))\n",
        "                loso_results.append(fold_mae)\n",
        "\n",
        "                # (B) Scenario eval (pattern)\n",
        "                mixed_item = build_mixed_pattern_trial(test_subj, actA_id, actB_id, pattern, CONFIG, full_data)\n",
        "                if mixed_item is None:\n",
        "                    print(f\"[Skip Mix] Missing GT or data for mix: subj={test_subj}, A={actA_id}, B={actB_id}\")\n",
        "                    continue\n",
        "\n",
        "                x_mix = mixed_item[\"data\"]\n",
        "                boundaries = mixed_item[\"boundaries\"]  # length = pattern_len-1\n",
        "                segments = mixed_item[\"segments\"]      # list length = pattern_len\n",
        "                gt_total = float(mixed_item[\"count\"])\n",
        "                md = mixed_item[\"meta_detail\"]\n",
        "\n",
        "                pred_total, _ = predict_count_by_windowing(\n",
        "                    model, x_np=x_mix,\n",
        "                    fs=CONFIG[\"fs\"], win_sec=CONFIG[\"win_sec\"], stride_sec=CONFIG[\"stride_sec\"],\n",
        "                    device=device, tau=CONFIG.get(\"tau\", 1.0),\n",
        "                    batch_size=CONFIG.get(\"batch_size\", 64)\n",
        "                )\n",
        "\n",
        "                diff = float(pred_total - gt_total)\n",
        "                mae = float(abs(diff))\n",
        "                scenario_diffs.append(diff)\n",
        "\n",
        "                # optional per-segment A/B sanity (first seg vs last seg)\n",
        "                predA_seg, _ = predict_count_by_windowing(\n",
        "                    model, x_np=segments[0],\n",
        "                    fs=CONFIG[\"fs\"], win_sec=CONFIG[\"win_sec\"], stride_sec=CONFIG[\"stride_sec\"],\n",
        "                    device=device, tau=CONFIG.get(\"tau\", 1.0),\n",
        "                    batch_size=CONFIG.get(\"batch_size\", 64)\n",
        "                )\n",
        "                predB_seg, _ = predict_count_by_windowing(\n",
        "                    model, x_np=segments[-1],\n",
        "                    fs=CONFIG[\"fs\"], win_sec=CONFIG[\"win_sec\"], stride_sec=CONFIG[\"stride_sec\"],\n",
        "                    device=device, tau=CONFIG.get(\"tau\", 1.0),\n",
        "                    batch_size=CONFIG.get(\"batch_size\", 64)\n",
        "                )\n",
        "\n",
        "                # full forward for phase/z\n",
        "                x_tensor = torch.tensor(x_mix, dtype=torch.float32).transpose(0, 1).unsqueeze(0).to(device)\n",
        "                with torch.no_grad():\n",
        "                    _, z_m, _, aux_m = model(x_tensor, mask=None, tau=CONFIG.get(\"tau\", 1.0))\n",
        "\n",
        "                phase_p_m = aux_m[\"phase_p\"].squeeze(0).detach().cpu().numpy()  # (Tmix,K)\n",
        "                k_hat_m = float(aux_m[\"k_hat\"].item())\n",
        "                ent_m = compute_phase_entropy_mean(phase_p_m)\n",
        "                z_td = z_m.squeeze(0).detach().cpu().numpy()  # (Tmix,D)\n",
        "\n",
        "                # pretty sequence string with GT per segment\n",
        "                seq_names = []\n",
        "                seq_gts = []\n",
        "                for ch in pattern:\n",
        "                    if ch == \"A\":\n",
        "                        seq_names.append(md[\"actA_name\"])\n",
        "                        seq_gts.append(md[\"gtA\"])\n",
        "                    else:\n",
        "                        seq_names.append(md[\"actB_name\"])\n",
        "                        seq_gts.append(md[\"gtB\"])\n",
        "                seq_str = \" -> \".join([f\"{n}({g:.0f})\" for n, g in zip(seq_names, seq_gts)])\n",
        "\n",
        "                scenario_records.append({\n",
        "                    \"subj\": test_subj,\n",
        "                    \"line\": (\n",
        "                        f\"[Scenario-PAT] {base_name} | pattern={pattern} | {test_subj} | \"\n",
        "                        f\"{seq_str} | \"\n",
        "                        f\"GT_total={gt_total:.0f} | Pred(win)={pred_total:.2f} | Diff={diff:+.2f} | \"\n",
        "                        f\"MAE={mae:.2f} | \"\n",
        "                        f\"A_GT={md['gtA']:.0f} | A_pred(seg)={predA_seg:.2f} | \"\n",
        "                        f\"B_GT={md['gtB']:.0f} | B_pred(seg)={predB_seg:.2f} | \"\n",
        "                        f\"k_hat(full)={k_hat_m:.2f} | ent(full)={ent_m:.3f} | \"\n",
        "                        f\"boundaries={boundaries}\"\n",
        "                    )\n",
        "                })\n",
        "\n",
        "                # ===========================\n",
        "                # buffers for scenario-avg plots\n",
        "                # ===========================\n",
        "\n",
        "                # 1) window-level rep_rate curves per segment -> resample -> concat (0~pattern_len)\n",
        "                seg_rate_rs = []\n",
        "                for seg_np in segments:\n",
        "                    _, r_w = get_window_rate_curve(\n",
        "                        model, seg_np, fs=CONFIG[\"fs\"],\n",
        "                        win_sec=CONFIG[\"win_sec\"], stride_sec=CONFIG[\"stride_sec\"],\n",
        "                        device=device, tau=CONFIG.get(\"tau\", 1.0),\n",
        "                        batch_size=CONFIG.get(\"batch_size\", 64)\n",
        "                    )\n",
        "                    seg_rate_rs.append(resample_1d(r_w, NW))\n",
        "                buf_win_concat.append(np.concatenate(seg_rate_rs, axis=0))  # (pattern_len*NW,)\n",
        "\n",
        "                # 2) latent: store raw z split for global PCA projection later\n",
        "                z_cuts = [0] + boundaries + [z_td.shape[0]]\n",
        "                z_segs = [z_td[z_cuts[i]:z_cuts[i + 1], :] for i in range(len(z_cuts) - 1)]\n",
        "                buf_z_concat.append(z_td)\n",
        "                per_subject_latent.append({\"z_segs\": z_segs})\n",
        "\n",
        "            # ---------------------------------------------------------\n",
        "            # Printing: Train summary -> subject lines -> scenario summary\n",
        "            # ---------------------------------------------------------\n",
        "            print(\"-\" * 100)\n",
        "            if len(loso_results) > 0:\n",
        "                print(\"TRain 결과 ->\")\n",
        "                print(\"-\" * 100)\n",
        "                print(f\" >>> Final LOSO Result (Average MAE): {np.mean(loso_results):.3f}\")\n",
        "                print(f\" >>> Standard Deviation: {np.std(loso_results):.3f}\")\n",
        "                print(\"-\" * 100)\n",
        "            else:\n",
        "                print(\"TRain 결과 -> (no folds computed)\")\n",
        "                print(\"-\" * 100)\n",
        "\n",
        "            subj2line = {r[\"subj\"]: r[\"line\"] for r in scenario_records}\n",
        "            for s in subjects:\n",
        "                if s in subj2line:\n",
        "                    print(subj2line[s])\n",
        "\n",
        "            print(\"\\n\" + \"-\" * 100)\n",
        "            print(f\"시나리오{scen_global_idx} 결과\")\n",
        "            print(\"-\" * 100)\n",
        "\n",
        "            if len(scenario_diffs) == 0:\n",
        "                print(f\"[{scen_name}] No samples (all skipped). Fill GT_BY_ACT for required activities.\")\n",
        "            else:\n",
        "                diffs = np.array(scenario_diffs, dtype=np.float32)\n",
        "                mae_v = float(np.mean(np.abs(diffs)))\n",
        "                rmse = float(np.sqrt(np.mean(diffs ** 2)))\n",
        "                print(f\"[{scen_name}] N={len(diffs):2d} | MAE={mae_v:.3f} | RMSE={rmse:.3f} | mean(diff)={diffs.mean():+.3f} | std(diff)={diffs.std():.3f}\")\n",
        "            print(\"-\" * 100)\n",
        "\n",
        "            # ---------------------------------------------------------\n",
        "            # Scenario-level mean±std plots (two_plots style)\n",
        "            # ---------------------------------------------------------\n",
        "            n_ok = len(buf_win_concat)\n",
        "            if n_ok == 0:\n",
        "                print(\"[Viz Skip] No scenario samples collected for averaging.\")\n",
        "                continue\n",
        "\n",
        "            # (1) window-level mean±std on normalized axis [0, pattern_len]\n",
        "            W = np.stack(buf_win_concat, axis=0)  # (N, pattern_len*NW)\n",
        "            win_mean = W.mean(axis=0)\n",
        "            win_std  = W.std(axis=0)\n",
        "            win_t = np.linspace(0.0, float(pattern_len), num=pattern_len * NW, dtype=np.float32)\n",
        "\n",
        "            # (2) latent: global PCA basis from all z\n",
        "            Z_all = np.concatenate(buf_z_concat, axis=0)  # (sumT, D)\n",
        "            mu, V2 = global_pca2(Z_all)\n",
        "\n",
        "            # project each subject, resample each segment to NL then concat -> (pattern_len*NL,2)\n",
        "            lat_list = []\n",
        "            for d in per_subject_latent:\n",
        "                pcs = []\n",
        "                for zseg in d[\"z_segs\"]:\n",
        "                    pc = (zseg - mu) @ V2.T\n",
        "                    pcs.append(resample_2d(pc, NL))\n",
        "                lat_list.append(np.concatenate(pcs, axis=0))  # (pattern_len*NL,2)\n",
        "\n",
        "            LAT = np.stack(lat_list, axis=0)  # (N,pattern_len*NL,2)\n",
        "            lat_xy_mean = LAT.mean(axis=0)    # (pattern_len*NL,2)\n",
        "            lat_xy_std  = LAT.std(axis=0)     # (pattern_len*NL,2)\n",
        "\n",
        "            plot_scenario_mean_std_two_plots(\n",
        "                save_dir=VIZ_DIR,\n",
        "                scen_name=scen_name,\n",
        "                win_t=win_t, win_mean=win_mean, win_std=win_std,\n",
        "                lat_xy_mean=lat_xy_mean, lat_xy_std=lat_xy_std,\n",
        "                n_subjects=n_ok,\n",
        "                pattern_len=pattern_len,\n",
        "                dpi=600\n",
        "            )\n",
        "\n",
        "            print(f\"[Saved] {VIZ_DIR}/{_safe_fname(scen_name)}__1_win_rep_rate.png\")\n",
        "            print(f\"[Saved] {VIZ_DIR}/{_safe_fname(scen_name)}__2_latent_pca2.png\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8CxBSb4YvsEX",
        "outputId": "d70571c7-2117-4338-b20c-5f1f1673e6c5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n",
            "Loading 10 subjects from /content/drive/MyDrive/Colab Notebooks/HAR_data/MHEALTHDATASET...\n",
            "\n",
            "====================================================================================================\n",
            "1. 시나리오1: Running -> Jogging | pattern=AAAAB\n",
            "====================================================================================================\n",
            "----------------------------------------------------------------------------------------------------\n",
            "TRain 결과 ->\n",
            "----------------------------------------------------------------------------------------------------\n",
            " >>> Final LOSO Result (Average MAE): 15.211\n",
            " >>> Standard Deviation: 7.371\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[Scenario-PAT] Running -> Jogging | pattern=AAAAB | subject1 | Running(165) -> Running(165) -> Running(165) -> Running(165) -> Jogging(157) | GT_total=817 | Pred(win)=812.40 | Diff=-4.60 | MAE=4.60 | A_GT=165 | A_pred(seg)=165.02 | B_GT=157 | B_pred(seg)=148.30 | k_hat(full)=1.62 | ent(full)=0.317 | boundaries=[3072, 6144, 9216, 12288]\n",
            "[Scenario-PAT] Running -> Jogging | pattern=AAAAB | subject2 | Running(158) -> Running(158) -> Running(158) -> Running(158) -> Jogging(161) | GT_total=793 | Pred(win)=864.65 | Diff=+71.65 | MAE=71.65 | A_GT=158 | A_pred(seg)=175.31 | B_GT=161 | B_pred(seg)=160.43 | k_hat(full)=1.97 | ent(full)=0.320 | boundaries=[3072, 6144, 9216, 12288]\n",
            "[Scenario-PAT] Running -> Jogging | pattern=AAAAB | subject3 | Running(174) -> Running(174) -> Running(174) -> Running(174) -> Jogging(154) | GT_total=850 | Pred(win)=838.45 | Diff=-11.55 | MAE=11.55 | A_GT=174 | A_pred(seg)=157.06 | B_GT=154 | B_pred(seg)=213.50 | k_hat(full)=1.33 | ent(full)=0.307 | boundaries=[3072, 6144, 9216, 12288]\n",
            "[Scenario-PAT] Running -> Jogging | pattern=AAAAB | subject4 | Running(163) -> Running(163) -> Running(163) -> Running(163) -> Jogging(154) | GT_total=806 | Pred(win)=974.33 | Diff=+168.33 | MAE=168.33 | A_GT=163 | A_pred(seg)=174.76 | B_GT=154 | B_pred(seg)=280.55 | k_hat(full)=1.26 | ent(full)=0.258 | boundaries=[3072, 6144, 9216, 12288]\n",
            "[Scenario-PAT] Running -> Jogging | pattern=AAAAB | subject5 | Running(157) -> Running(157) -> Running(157) -> Running(157) -> Jogging(160) | GT_total=788 | Pred(win)=880.70 | Diff=+92.70 | MAE=92.70 | A_GT=157 | A_pred(seg)=151.78 | B_GT=160 | B_pred(seg)=283.59 | k_hat(full)=1.42 | ent(full)=0.284 | boundaries=[3072, 6144, 9216, 12288]\n",
            "[Scenario-PAT] Running -> Jogging | pattern=AAAAB | subject6 | Running(172) -> Running(172) -> Running(172) -> Running(172) -> Jogging(156) | GT_total=844 | Pred(win)=907.36 | Diff=+63.36 | MAE=63.36 | A_GT=172 | A_pred(seg)=154.58 | B_GT=156 | B_pred(seg)=296.37 | k_hat(full)=1.78 | ent(full)=0.379 | boundaries=[3072, 6144, 9216, 12288]\n",
            "[Scenario-PAT] Running -> Jogging | pattern=AAAAB | subject7 | Running(149) -> Running(149) -> Running(149) -> Running(149) -> Jogging(153) | GT_total=749 | Pred(win)=898.63 | Diff=+149.63 | MAE=149.63 | A_GT=149 | A_pred(seg)=152.11 | B_GT=153 | B_pred(seg)=297.98 | k_hat(full)=1.35 | ent(full)=0.236 | boundaries=[3072, 6144, 9216, 12288]\n",
            "[Scenario-PAT] Running -> Jogging | pattern=AAAAB | subject8 | Running(166) -> Running(166) -> Running(166) -> Running(166) -> Jogging(160) | GT_total=824 | Pred(win)=829.36 | Diff=+5.36 | MAE=5.36 | A_GT=166 | A_pred(seg)=160.45 | B_GT=160 | B_pred(seg)=183.39 | k_hat(full)=1.41 | ent(full)=0.335 | boundaries=[3072, 6144, 9216, 12288]\n",
            "[Scenario-PAT] Running -> Jogging | pattern=AAAAB | subject9 | Running(174) -> Running(174) -> Running(174) -> Running(174) -> Jogging(166) | GT_total=862 | Pred(win)=965.96 | Diff=+103.96 | MAE=103.96 | A_GT=174 | A_pred(seg)=172.75 | B_GT=166 | B_pred(seg)=282.45 | k_hat(full)=1.59 | ent(full)=0.291 | boundaries=[3072, 6144, 9216, 12288]\n",
            "[Scenario-PAT] Running -> Jogging | pattern=AAAAB | subject10 | Running(172) -> Running(172) -> Running(172) -> Running(172) -> Jogging(156) | GT_total=844 | Pred(win)=901.70 | Diff=+57.70 | MAE=57.70 | A_GT=172 | A_pred(seg)=152.52 | B_GT=156 | B_pred(seg)=301.53 | k_hat(full)=1.42 | ent(full)=0.290 | boundaries=[3072, 6144, 9216, 12288]\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "시나리오1 결과\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[Running -> Jogging | pattern=AAAAB] N=10 | MAE=72.884 | RMSE=90.986 | mean(diff)=+69.652 | std(diff)=58.541\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[Saved] scenario_viz/Running_-__Jogging___pattern=AAAAB__1_win_rep_rate.png\n",
            "[Saved] scenario_viz/Running_-__Jogging___pattern=AAAAB__2_latent_pca2.png\n",
            "\n",
            "====================================================================================================\n",
            "1. 시나리오2: Jogging -> Running | pattern=AAAAB\n",
            "====================================================================================================\n",
            "----------------------------------------------------------------------------------------------------\n",
            "TRain 결과 ->\n",
            "----------------------------------------------------------------------------------------------------\n",
            " >>> Final LOSO Result (Average MAE): 10.421\n",
            " >>> Standard Deviation: 10.920\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[Scenario-PAT] Jogging -> Running | pattern=AAAAB | subject1 | Jogging(157) -> Jogging(157) -> Jogging(157) -> Jogging(157) -> Running(165) | GT_total=793 | Pred(win)=772.09 | Diff=-20.91 | MAE=20.91 | A_GT=157 | A_pred(seg)=130.92 | B_GT=165 | B_pred(seg)=262.26 | k_hat(full)=1.26 | ent(full)=0.344 | boundaries=[3072, 6144, 9216, 12288]\n",
            "[Scenario-PAT] Jogging -> Running | pattern=AAAAB | subject2 | Jogging(161) -> Jogging(161) -> Jogging(161) -> Jogging(161) -> Running(158) | GT_total=802 | Pred(win)=838.69 | Diff=+36.69 | MAE=36.69 | A_GT=161 | A_pred(seg)=157.06 | B_GT=158 | B_pred(seg)=206.96 | k_hat(full)=1.73 | ent(full)=0.497 | boundaries=[3072, 6144, 9216, 12288]\n",
            "[Scenario-PAT] Jogging -> Running | pattern=AAAAB | subject3 | Jogging(154) -> Jogging(154) -> Jogging(154) -> Jogging(154) -> Running(174) | GT_total=790 | Pred(win)=783.93 | Diff=-6.07 | MAE=6.07 | A_GT=154 | A_pred(seg)=126.77 | B_GT=174 | B_pred(seg)=289.84 | k_hat(full)=1.45 | ent(full)=0.344 | boundaries=[3072, 6144, 9216, 12288]\n",
            "[Scenario-PAT] Jogging -> Running | pattern=AAAAB | subject4 | Jogging(154) -> Jogging(154) -> Jogging(154) -> Jogging(154) -> Running(163) | GT_total=779 | Pred(win)=876.64 | Diff=+97.64 | MAE=97.64 | A_GT=154 | A_pred(seg)=145.43 | B_GT=163 | B_pred(seg)=306.36 | k_hat(full)=1.37 | ent(full)=0.291 | boundaries=[3072, 6144, 9216, 12288]\n",
            "[Scenario-PAT] Jogging -> Running | pattern=AAAAB | subject5 | Jogging(160) -> Jogging(160) -> Jogging(160) -> Jogging(160) -> Running(157) | GT_total=797 | Pred(win)=851.05 | Diff=+54.05 | MAE=54.05 | A_GT=160 | A_pred(seg)=142.09 | B_GT=157 | B_pred(seg)=294.84 | k_hat(full)=1.48 | ent(full)=0.337 | boundaries=[3072, 6144, 9216, 12288]\n",
            "[Scenario-PAT] Jogging -> Running | pattern=AAAAB | subject6 | Jogging(156) -> Jogging(156) -> Jogging(156) -> Jogging(156) -> Running(172) | GT_total=796 | Pred(win)=891.01 | Diff=+95.01 | MAE=95.01 | A_GT=156 | A_pred(seg)=133.24 | B_GT=172 | B_pred(seg)=375.14 | k_hat(full)=1.53 | ent(full)=0.431 | boundaries=[3072, 6144, 9216, 12288]\n",
            "[Scenario-PAT] Jogging -> Running | pattern=AAAAB | subject7 | Jogging(153) -> Jogging(153) -> Jogging(153) -> Jogging(153) -> Running(149) | GT_total=761 | Pred(win)=805.26 | Diff=+44.26 | MAE=44.26 | A_GT=153 | A_pred(seg)=125.56 | B_GT=149 | B_pred(seg)=315.30 | k_hat(full)=1.58 | ent(full)=0.377 | boundaries=[3072, 6144, 9216, 12288]\n",
            "[Scenario-PAT] Jogging -> Running | pattern=AAAAB | subject8 | Jogging(160) -> Jogging(160) -> Jogging(160) -> Jogging(160) -> Running(166) | GT_total=806 | Pred(win)=718.91 | Diff=-87.09 | MAE=87.09 | A_GT=160 | A_pred(seg)=108.64 | B_GT=166 | B_pred(seg)=296.57 | k_hat(full)=1.53 | ent(full)=0.469 | boundaries=[3072, 6144, 9216, 12288]\n",
            "[Scenario-PAT] Jogging -> Running | pattern=AAAAB | subject9 | Jogging(166) -> Jogging(166) -> Jogging(166) -> Jogging(166) -> Running(174) | GT_total=838 | Pred(win)=912.50 | Diff=+74.50 | MAE=74.50 | A_GT=166 | A_pred(seg)=153.15 | B_GT=174 | B_pred(seg)=309.61 | k_hat(full)=1.64 | ent(full)=0.329 | boundaries=[3072, 6144, 9216, 12288]\n",
            "[Scenario-PAT] Jogging -> Running | pattern=AAAAB | subject10 | Jogging(156) -> Jogging(156) -> Jogging(156) -> Jogging(156) -> Running(172) | GT_total=796 | Pred(win)=930.85 | Diff=+134.85 | MAE=134.85 | A_GT=156 | A_pred(seg)=148.80 | B_GT=172 | B_pred(seg)=349.88 | k_hat(full)=1.46 | ent(full)=0.341 | boundaries=[3072, 6144, 9216, 12288]\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "시나리오2 결과\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[Jogging -> Running | pattern=AAAAB] N=10 | MAE=65.108 | RMSE=75.220 | mean(diff)=+42.292 | std(diff)=62.205\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[Saved] scenario_viz/Jogging_-__Running___pattern=AAAAB__1_win_rep_rate.png\n",
            "[Saved] scenario_viz/Jogging_-__Running___pattern=AAAAB__2_latent_pca2.png\n",
            "\n",
            "====================================================================================================\n",
            "1. 시나리오3: Running -> Knees bending | pattern=AAAAB\n",
            "====================================================================================================\n",
            "----------------------------------------------------------------------------------------------------\n",
            "TRain 결과 ->\n",
            "----------------------------------------------------------------------------------------------------\n",
            " >>> Final LOSO Result (Average MAE): 15.211\n",
            " >>> Standard Deviation: 7.371\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[Scenario-PAT] Running -> Knees bending | pattern=AAAAB | subject1 | Running(165) -> Running(165) -> Running(165) -> Running(165) -> Knees bending(20) | GT_total=680 | Pred(win)=919.16 | Diff=+239.16 | MAE=239.16 | A_GT=165 | A_pred(seg)=168.33 | B_GT=20 | B_pred(seg)=246.90 | k_hat(full)=1.49 | ent(full)=0.253 | boundaries=[3072, 6144, 9216, 12288]\n",
            "[Scenario-PAT] Running -> Knees bending | pattern=AAAAB | subject2 | Running(158) -> Running(158) -> Running(158) -> Running(158) -> Knees bending(21) | GT_total=653 | Pred(win)=1030.22 | Diff=+377.22 | MAE=377.22 | A_GT=158 | A_pred(seg)=181.37 | B_GT=21 | B_pred(seg)=314.20 | k_hat(full)=1.78 | ent(full)=0.245 | boundaries=[3072, 6144, 9216, 12288]\n",
            "[Scenario-PAT] Running -> Knees bending | pattern=AAAAB | subject3 | Running(174) -> Running(174) -> Running(174) -> Running(174) -> Knees bending(21) | GT_total=717 | Pred(win)=837.76 | Diff=+120.76 | MAE=120.76 | A_GT=174 | A_pred(seg)=164.01 | B_GT=21 | B_pred(seg)=181.93 | k_hat(full)=1.30 | ent(full)=0.271 | boundaries=[3072, 6144, 9216, 12288]\n",
            "[Scenario-PAT] Running -> Knees bending | pattern=AAAAB | subject4 | Running(163) -> Running(163) -> Running(163) -> Running(163) -> Knees bending(19) | GT_total=671 | Pred(win)=1000.54 | Diff=+329.54 | MAE=329.54 | A_GT=163 | A_pred(seg)=197.79 | B_GT=19 | B_pred(seg)=208.60 | k_hat(full)=1.22 | ent(full)=0.218 | boundaries=[3072, 6144, 9216, 12288]\n",
            "[Scenario-PAT] Running -> Knees bending | pattern=AAAAB | subject5 | Running(157) -> Running(157) -> Running(157) -> Running(157) -> Knees bending(20) | GT_total=648 | Pred(win)=873.64 | Diff=+225.64 | MAE=225.64 | A_GT=157 | A_pred(seg)=164.78 | B_GT=20 | B_pred(seg)=218.33 | k_hat(full)=1.40 | ent(full)=0.240 | boundaries=[3072, 6144, 9216, 12288]\n",
            "[Scenario-PAT] Running -> Knees bending | pattern=AAAAB | subject6 | Running(172) -> Running(172) -> Running(172) -> Running(172) -> Knees bending(20) | GT_total=708 | Pred(win)=707.96 | Diff=-0.04 | MAE=0.04 | A_GT=172 | A_pred(seg)=161.85 | B_GT=20 | B_pred(seg)=52.87 | k_hat(full)=1.94 | ent(full)=0.416 | boundaries=[3072, 6144, 9216, 12288]\n",
            "[Scenario-PAT] Running -> Knees bending | pattern=AAAAB | subject7 | Running(149) -> Running(149) -> Running(149) -> Running(149) -> Knees bending(21) | GT_total=617 | Pred(win)=945.00 | Diff=+328.00 | MAE=328.00 | A_GT=149 | A_pred(seg)=185.79 | B_GT=21 | B_pred(seg)=201.96 | k_hat(full)=1.24 | ent(full)=0.178 | boundaries=[3072, 6144, 9216, 12288]\n",
            "[Scenario-PAT] Running -> Knees bending | pattern=AAAAB | subject8 | Running(166) -> Running(166) -> Running(166) -> Running(166) -> Knees bending(21) | GT_total=685 | Pred(win)=776.92 | Diff=+91.92 | MAE=91.92 | A_GT=166 | A_pred(seg)=164.68 | B_GT=21 | B_pred(seg)=113.16 | k_hat(full)=1.43 | ent(full)=0.349 | boundaries=[3072, 6144, 9216, 12288]\n",
            "[Scenario-PAT] Running -> Knees bending | pattern=AAAAB | subject9 | Running(174) -> Running(174) -> Running(174) -> Running(174) -> Knees bending(21) | GT_total=717 | Pred(win)=1032.90 | Diff=+315.90 | MAE=315.90 | A_GT=174 | A_pred(seg)=211.88 | B_GT=21 | B_pred(seg)=183.90 | k_hat(full)=1.44 | ent(full)=0.225 | boundaries=[3072, 6144, 9216, 12288]\n",
            "[Scenario-PAT] Running -> Knees bending | pattern=AAAAB | subject10 | Running(172) -> Running(172) -> Running(172) -> Running(172) -> Knees bending(21) | GT_total=709 | Pred(win)=870.20 | Diff=+161.20 | MAE=161.20 | A_GT=172 | A_pred(seg)=179.44 | B_GT=21 | B_pred(seg)=151.10 | k_hat(full)=1.33 | ent(full)=0.232 | boundaries=[3072, 6144, 9216, 12288]\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "시나리오3 결과\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[Running -> Knees bending | pattern=AAAAB] N=10 | MAE=218.938 | RMSE=248.072 | mean(diff)=+218.930 | std(diff)=116.659\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[Saved] scenario_viz/Running_-__Knees_bending___pattern=AAAAB__1_win_rep_rate.png\n",
            "[Saved] scenario_viz/Running_-__Knees_bending___pattern=AAAAB__2_latent_pca2.png\n",
            "\n",
            "====================================================================================================\n",
            "1. 시나리오4: Knees bending -> Running | pattern=AAAAB\n",
            "====================================================================================================\n",
            "----------------------------------------------------------------------------------------------------\n",
            "TRain 결과 ->\n",
            "----------------------------------------------------------------------------------------------------\n",
            " >>> Final LOSO Result (Average MAE): 4.274\n",
            " >>> Standard Deviation: 3.012\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[Scenario-PAT] Knees bending -> Running | pattern=AAAAB | subject1 | Knees bending(20) -> Knees bending(20) -> Knees bending(20) -> Knees bending(20) -> Running(165) | GT_total=245 | Pred(win)=151.14 | Diff=-93.86 | MAE=93.86 | A_GT=20 | A_pred(seg)=18.33 | B_GT=165 | B_pred(seg)=82.08 | k_hat(full)=1.09 | ent(full)=0.211 | boundaries=[3379, 6758, 10137, 13516]\n",
            "[Scenario-PAT] Knees bending -> Running | pattern=AAAAB | subject2 | Knees bending(21) -> Knees bending(21) -> Knees bending(21) -> Knees bending(21) -> Running(158) | GT_total=242 | Pred(win)=112.00 | Diff=-130.00 | MAE=130.00 | A_GT=21 | A_pred(seg)=15.18 | B_GT=158 | B_pred(seg)=55.23 | k_hat(full)=1.12 | ent(full)=0.252 | boundaries=[3430, 6860, 10290, 13720]\n",
            "[Scenario-PAT] Knees bending -> Running | pattern=AAAAB | subject3 | Knees bending(21) -> Knees bending(21) -> Knees bending(21) -> Knees bending(21) -> Running(174) | GT_total=258 | Pred(win)=102.91 | Diff=-155.09 | MAE=155.09 | A_GT=21 | A_pred(seg)=11.35 | B_GT=174 | B_pred(seg)=60.39 | k_hat(full)=1.09 | ent(full)=0.213 | boundaries=[3175, 6350, 9525, 12700]\n",
            "[Scenario-PAT] Knees bending -> Running | pattern=AAAAB | subject4 | Knees bending(19) -> Knees bending(19) -> Knees bending(19) -> Knees bending(19) -> Running(163) | GT_total=239 | Pred(win)=123.83 | Diff=-115.17 | MAE=115.17 | A_GT=19 | A_pred(seg)=10.96 | B_GT=163 | B_pred(seg)=86.72 | k_hat(full)=2.12 | ent(full)=0.401 | boundaries=[3123, 6246, 9369, 12492]\n",
            "[Scenario-PAT] Knees bending -> Running | pattern=AAAAB | subject5 | Knees bending(20) -> Knees bending(20) -> Knees bending(20) -> Knees bending(20) -> Running(157) | GT_total=237 | Pred(win)=105.55 | Diff=-131.45 | MAE=131.45 | A_GT=20 | A_pred(seg)=12.05 | B_GT=157 | B_pred(seg)=59.73 | k_hat(full)=1.06 | ent(full)=0.135 | boundaries=[2714, 5428, 8142, 10856]\n",
            "[Scenario-PAT] Knees bending -> Running | pattern=AAAAB | subject6 | Knees bending(20) -> Knees bending(20) -> Knees bending(20) -> Knees bending(20) -> Running(172) | GT_total=252 | Pred(win)=105.70 | Diff=-146.30 | MAE=146.30 | A_GT=20 | A_pred(seg)=9.54 | B_GT=172 | B_pred(seg)=70.47 | k_hat(full)=1.09 | ent(full)=0.179 | boundaries=[2304, 4608, 6912, 9216]\n",
            "[Scenario-PAT] Knees bending -> Running | pattern=AAAAB | subject7 | Knees bending(21) -> Knees bending(21) -> Knees bending(21) -> Knees bending(21) -> Running(149) | GT_total=233 | Pred(win)=89.70 | Diff=-143.30 | MAE=143.30 | A_GT=21 | A_pred(seg)=12.23 | B_GT=149 | B_pred(seg)=42.56 | k_hat(full)=1.03 | ent(full)=0.094 | boundaries=[2816, 5632, 8448, 11264]\n",
            "[Scenario-PAT] Knees bending -> Running | pattern=AAAAB | subject8 | Knees bending(21) -> Knees bending(21) -> Knees bending(21) -> Knees bending(21) -> Running(166) | GT_total=250 | Pred(win)=92.11 | Diff=-157.89 | MAE=157.89 | A_GT=21 | A_pred(seg)=8.63 | B_GT=166 | B_pred(seg)=59.80 | k_hat(full)=1.09 | ent(full)=0.204 | boundaries=[2560, 5120, 7680, 10240]\n",
            "[Scenario-PAT] Knees bending -> Running | pattern=AAAAB | subject9 | Knees bending(21) -> Knees bending(21) -> Knees bending(21) -> Knees bending(21) -> Running(174) | GT_total=258 | Pred(win)=109.87 | Diff=-148.13 | MAE=148.13 | A_GT=21 | A_pred(seg)=11.46 | B_GT=174 | B_pred(seg)=67.02 | k_hat(full)=1.05 | ent(full)=0.124 | boundaries=[2969, 5938, 8907, 11876]\n",
            "[Scenario-PAT] Knees bending -> Running | pattern=AAAAB | subject10 | Knees bending(21) -> Knees bending(21) -> Knees bending(21) -> Knees bending(21) -> Running(172) | GT_total=256 | Pred(win)=104.26 | Diff=-151.74 | MAE=151.74 | A_GT=21 | A_pred(seg)=10.21 | B_GT=172 | B_pred(seg)=66.55 | k_hat(full)=1.06 | ent(full)=0.156 | boundaries=[2867, 5734, 8601, 11468]\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "시나리오4 결과\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[Knees bending -> Running | pattern=AAAAB] N=10 | MAE=137.293 | RMSE=138.615 | mean(diff)=-137.293 | std(diff)=19.098\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[Saved] scenario_viz/Knees_bending_-__Running___pattern=AAAAB__1_win_rep_rate.png\n",
            "[Saved] scenario_viz/Knees_bending_-__Running___pattern=AAAAB__2_latent_pca2.png\n",
            "\n",
            "====================================================================================================\n",
            "1. 시나리오5: Frontal elevation of arms -> Waist bends forward | pattern=AAAAB\n",
            "====================================================================================================\n",
            "----------------------------------------------------------------------------------------------------\n",
            "TRain 결과 ->\n",
            "----------------------------------------------------------------------------------------------------\n",
            " >>> Final LOSO Result (Average MAE): 2.324\n",
            " >>> Standard Deviation: 1.910\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[Scenario-PAT] Frontal elevation of arms -> Waist bends forward | pattern=AAAAB | subject1 | Frontal elevation of arms(20) -> Frontal elevation of arms(20) -> Frontal elevation of arms(20) -> Frontal elevation of arms(20) -> Waist bends forward(21) | GT_total=101 | Pred(win)=80.38 | Diff=-20.62 | MAE=20.62 | A_GT=20 | A_pred(seg)=13.92 | B_GT=21 | B_pred(seg)=25.44 | k_hat(full)=1.04 | ent(full)=0.095 | boundaries=[3072, 6144, 9216, 12288]\n",
            "[Scenario-PAT] Frontal elevation of arms -> Waist bends forward | pattern=AAAAB | subject2 | Frontal elevation of arms(20) -> Frontal elevation of arms(20) -> Frontal elevation of arms(20) -> Frontal elevation of arms(20) -> Waist bends forward(19) | GT_total=99 | Pred(win)=115.07 | Diff=+16.07 | MAE=16.07 | A_GT=20 | A_pred(seg)=17.11 | B_GT=19 | B_pred(seg)=48.02 | k_hat(full)=1.01 | ent(full)=0.040 | boundaries=[3328, 6656, 9984, 13312]\n",
            "[Scenario-PAT] Frontal elevation of arms -> Waist bends forward | pattern=AAAAB | subject3 | Frontal elevation of arms(20) -> Frontal elevation of arms(20) -> Frontal elevation of arms(20) -> Frontal elevation of arms(20) -> Waist bends forward(21) | GT_total=101 | Pred(win)=118.09 | Diff=+17.09 | MAE=17.09 | A_GT=20 | A_pred(seg)=18.85 | B_GT=21 | B_pred(seg)=44.28 | k_hat(full)=1.01 | ent(full)=0.037 | boundaries=[3379, 6758, 10137, 13516]\n",
            "[Scenario-PAT] Frontal elevation of arms -> Waist bends forward | pattern=AAAAB | subject4 | Frontal elevation of arms(20) -> Frontal elevation of arms(20) -> Frontal elevation of arms(20) -> Frontal elevation of arms(20) -> Waist bends forward(20) | GT_total=100 | Pred(win)=130.92 | Diff=+30.92 | MAE=30.92 | A_GT=20 | A_pred(seg)=16.13 | B_GT=20 | B_pred(seg)=67.24 | k_hat(full)=1.03 | ent(full)=0.076 | boundaries=[3277, 6554, 9831, 13108]\n",
            "[Scenario-PAT] Frontal elevation of arms -> Waist bends forward | pattern=AAAAB | subject5 | Frontal elevation of arms(20) -> Frontal elevation of arms(20) -> Frontal elevation of arms(20) -> Frontal elevation of arms(20) -> Waist bends forward(20) | GT_total=100 | Pred(win)=89.18 | Diff=-10.82 | MAE=10.82 | A_GT=20 | A_pred(seg)=13.72 | B_GT=20 | B_pred(seg)=34.80 | k_hat(full)=1.02 | ent(full)=0.060 | boundaries=[2868, 5736, 8604, 11472]\n",
            "[Scenario-PAT] Frontal elevation of arms -> Waist bends forward | pattern=AAAAB | subject6 | Frontal elevation of arms(20) -> Frontal elevation of arms(20) -> Frontal elevation of arms(20) -> Frontal elevation of arms(20) -> Waist bends forward(20) | GT_total=100 | Pred(win)=65.83 | Diff=-34.17 | MAE=34.17 | A_GT=20 | A_pred(seg)=8.05 | B_GT=20 | B_pred(seg)=35.86 | k_hat(full)=1.01 | ent(full)=0.035 | boundaries=[2099, 4198, 6297, 8396]\n",
            "[Scenario-PAT] Frontal elevation of arms -> Waist bends forward | pattern=AAAAB | subject7 | Frontal elevation of arms(20) -> Frontal elevation of arms(20) -> Frontal elevation of arms(20) -> Frontal elevation of arms(20) -> Waist bends forward(20) | GT_total=100 | Pred(win)=109.95 | Diff=+9.95 | MAE=9.95 | A_GT=20 | A_pred(seg)=14.05 | B_GT=20 | B_pred(seg)=55.63 | k_hat(full)=1.02 | ent(full)=0.053 | boundaries=[2765, 5530, 8295, 11060]\n",
            "[Scenario-PAT] Frontal elevation of arms -> Waist bends forward | pattern=AAAAB | subject8 | Frontal elevation of arms(19) -> Frontal elevation of arms(19) -> Frontal elevation of arms(19) -> Frontal elevation of arms(19) -> Waist bends forward(21) | GT_total=97 | Pred(win)=110.13 | Diff=+13.13 | MAE=13.13 | A_GT=19 | A_pred(seg)=15.76 | B_GT=21 | B_pred(seg)=49.50 | k_hat(full)=1.02 | ent(full)=0.054 | boundaries=[3021, 6042, 9063, 12084]\n",
            "[Scenario-PAT] Frontal elevation of arms -> Waist bends forward | pattern=AAAAB | subject9 | Frontal elevation of arms(19) -> Frontal elevation of arms(19) -> Frontal elevation of arms(19) -> Frontal elevation of arms(19) -> Waist bends forward(21) | GT_total=97 | Pred(win)=107.32 | Diff=+10.32 | MAE=10.32 | A_GT=19 | A_pred(seg)=13.12 | B_GT=21 | B_pred(seg)=57.70 | k_hat(full)=1.04 | ent(full)=0.085 | boundaries=[2867, 5734, 8601, 11468]\n",
            "[Scenario-PAT] Frontal elevation of arms -> Waist bends forward | pattern=AAAAB | subject10 | Frontal elevation of arms(20) -> Frontal elevation of arms(20) -> Frontal elevation of arms(20) -> Frontal elevation of arms(20) -> Waist bends forward(20) | GT_total=100 | Pred(win)=100.54 | Diff=+0.54 | MAE=0.54 | A_GT=20 | A_pred(seg)=12.97 | B_GT=20 | B_pred(seg)=51.56 | k_hat(full)=1.03 | ent(full)=0.079 | boundaries=[2765, 5530, 8295, 11060]\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "시나리오5 결과\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[Frontal elevation of arms -> Waist bends forward | pattern=AAAAB] N=10 | MAE=16.363 | RMSE=18.959 | mean(diff)=+3.242 | std(diff)=18.680\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[Saved] scenario_viz/Frontal_elevation_of_arms_-__Waist_bends_forward___pattern=AAAAB__1_win_rep_rate.png\n",
            "[Saved] scenario_viz/Frontal_elevation_of_arms_-__Waist_bends_forward___pattern=AAAAB__2_latent_pca2.png\n",
            "\n",
            "====================================================================================================\n",
            "1. 시나리오6: Waist bends forward -> Frontal elevation of arms | pattern=AAAAB\n",
            "====================================================================================================\n",
            "----------------------------------------------------------------------------------------------------\n",
            "TRain 결과 ->\n",
            "----------------------------------------------------------------------------------------------------\n",
            " >>> Final LOSO Result (Average MAE): 5.261\n",
            " >>> Standard Deviation: 3.006\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[Scenario-PAT] Waist bends forward -> Frontal elevation of arms | pattern=AAAAB | subject1 | Waist bends forward(21) -> Waist bends forward(21) -> Waist bends forward(21) -> Waist bends forward(21) -> Frontal elevation of arms(20) | GT_total=104 | Pred(win)=107.14 | Diff=+3.14 | MAE=3.14 | A_GT=21 | A_pred(seg)=21.91 | B_GT=20 | B_pred(seg)=19.37 | k_hat(full)=1.03 | ent(full)=0.080 | boundaries=[3072, 6144, 9216, 12288]\n",
            "[Scenario-PAT] Waist bends forward -> Frontal elevation of arms | pattern=AAAAB | subject2 | Waist bends forward(19) -> Waist bends forward(19) -> Waist bends forward(19) -> Waist bends forward(19) -> Frontal elevation of arms(20) | GT_total=96 | Pred(win)=106.86 | Diff=+10.86 | MAE=10.86 | A_GT=19 | A_pred(seg)=20.16 | B_GT=20 | B_pred(seg)=24.86 | k_hat(full)=1.06 | ent(full)=0.116 | boundaries=[3174, 6348, 9522, 12696]\n",
            "[Scenario-PAT] Waist bends forward -> Frontal elevation of arms | pattern=AAAAB | subject3 | Waist bends forward(21) -> Waist bends forward(21) -> Waist bends forward(21) -> Waist bends forward(21) -> Frontal elevation of arms(20) | GT_total=104 | Pred(win)=115.88 | Diff=+11.88 | MAE=11.88 | A_GT=21 | A_pred(seg)=21.97 | B_GT=20 | B_pred(seg)=27.89 | k_hat(full)=1.02 | ent(full)=0.055 | boundaries=[3226, 6452, 9678, 12904]\n",
            "[Scenario-PAT] Waist bends forward -> Frontal elevation of arms | pattern=AAAAB | subject4 | Waist bends forward(20) -> Waist bends forward(20) -> Waist bends forward(20) -> Waist bends forward(20) -> Frontal elevation of arms(20) | GT_total=100 | Pred(win)=139.07 | Diff=+39.07 | MAE=39.07 | A_GT=20 | A_pred(seg)=19.22 | B_GT=20 | B_pred(seg)=64.96 | k_hat(full)=1.02 | ent(full)=0.048 | boundaries=[3328, 6656, 9984, 13312]\n",
            "[Scenario-PAT] Waist bends forward -> Frontal elevation of arms | pattern=AAAAB | subject5 | Waist bends forward(20) -> Waist bends forward(20) -> Waist bends forward(20) -> Waist bends forward(20) -> Frontal elevation of arms(20) | GT_total=100 | Pred(win)=152.08 | Diff=+52.08 | MAE=52.08 | A_GT=20 | A_pred(seg)=28.17 | B_GT=20 | B_pred(seg)=38.82 | k_hat(full)=1.02 | ent(full)=0.055 | boundaries=[2765, 5530, 8295, 11060]\n",
            "[Scenario-PAT] Waist bends forward -> Frontal elevation of arms | pattern=AAAAB | subject6 | Waist bends forward(20) -> Waist bends forward(20) -> Waist bends forward(20) -> Waist bends forward(20) -> Frontal elevation of arms(20) | GT_total=100 | Pred(win)=62.60 | Diff=-37.40 | MAE=37.40 | A_GT=20 | A_pred(seg)=11.58 | B_GT=20 | B_pred(seg)=16.85 | k_hat(full)=1.01 | ent(full)=0.021 | boundaries=[2202, 4404, 6606, 8808]\n",
            "[Scenario-PAT] Waist bends forward -> Frontal elevation of arms | pattern=AAAAB | subject7 | Waist bends forward(20) -> Waist bends forward(20) -> Waist bends forward(20) -> Waist bends forward(20) -> Frontal elevation of arms(20) | GT_total=100 | Pred(win)=171.72 | Diff=+71.72 | MAE=71.72 | A_GT=20 | A_pred(seg)=20.22 | B_GT=20 | B_pred(seg)=92.04 | k_hat(full)=1.05 | ent(full)=0.112 | boundaries=[3072, 6144, 9216, 12288]\n",
            "[Scenario-PAT] Waist bends forward -> Frontal elevation of arms | pattern=AAAAB | subject8 | Waist bends forward(21) -> Waist bends forward(21) -> Waist bends forward(21) -> Waist bends forward(21) -> Frontal elevation of arms(19) | GT_total=103 | Pred(win)=117.66 | Diff=+14.66 | MAE=14.66 | A_GT=21 | A_pred(seg)=19.17 | B_GT=19 | B_pred(seg)=42.46 | k_hat(full)=1.01 | ent(full)=0.036 | boundaries=[2151, 4302, 6453, 8604]\n",
            "[Scenario-PAT] Waist bends forward -> Frontal elevation of arms | pattern=AAAAB | subject9 | Waist bends forward(21) -> Waist bends forward(21) -> Waist bends forward(21) -> Waist bends forward(21) -> Frontal elevation of arms(19) | GT_total=103 | Pred(win)=153.34 | Diff=+50.34 | MAE=50.34 | A_GT=21 | A_pred(seg)=27.75 | B_GT=19 | B_pred(seg)=44.11 | k_hat(full)=1.01 | ent(full)=0.037 | boundaries=[2867, 5734, 8601, 11468]\n",
            "[Scenario-PAT] Waist bends forward -> Frontal elevation of arms | pattern=AAAAB | subject10 | Waist bends forward(20) -> Waist bends forward(20) -> Waist bends forward(20) -> Waist bends forward(20) -> Frontal elevation of arms(20) | GT_total=100 | Pred(win)=98.34 | Diff=-1.66 | MAE=1.66 | A_GT=20 | A_pred(seg)=17.18 | B_GT=20 | B_pred(seg)=30.37 | k_hat(full)=1.02 | ent(full)=0.058 | boundaries=[2458, 4916, 7374, 9832]\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "시나리오6 결과\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[Waist bends forward -> Frontal elevation of arms | pattern=AAAAB] N=10 | MAE=29.280 | RMSE=37.151 | mean(diff)=+21.469 | std(diff)=30.319\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[Saved] scenario_viz/Waist_bends_forward_-__Frontal_elevation_of_arms___pattern=AAAAB__1_win_rep_rate.png\n",
            "[Saved] scenario_viz/Waist_bends_forward_-__Frontal_elevation_of_arms___pattern=AAAAB__2_latent_pca2.png\n"
          ]
        }
      ]
    }
  ]
}