{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ZACfDuuOtq-U",
        "outputId": "23c52fd9-fb3b-4d16-feed-99da232e38b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n",
            "Loading 10 subjects from /content/drive/MyDrive/Colab Notebooks/HAR_data/MHEALTHDATASET...\n",
            "\n",
            "==========================================================================================\n",
            ">>> Robustness Evaluation: Clean-train → Corrupted-test (test-only)\n",
            "==========================================================================================\n",
            "\n",
            "------------------------------------------------------------------------------------------\n",
            "[Activity] 6 | Waist bends forward\n",
            "------------------------------------------------------------------------------------------\n",
            "[Done] Fold  1 | Test: subject1\n",
            "[Done] Fold  2 | Test: subject2\n",
            "[Done] Fold  3 | Test: subject3\n",
            "[Done] Fold  4 | Test: subject4\n",
            "[Done] Fold  5 | Test: subject5\n",
            "[Done] Fold  6 | Test: subject6\n",
            "[Done] Fold  7 | Test: subject7\n",
            "[Done] Fold  8 | Test: subject8\n",
            "[Done] Fold  9 | Test: subject9\n",
            "[Done] Fold 10 | Test: subject10\n",
            "\n",
            "------------------------------------------------------------------------------------------\n",
            "[Activity] 7 | Frontal elevation of arms\n",
            "------------------------------------------------------------------------------------------\n",
            "[Done] Fold  1 | Test: subject1\n",
            "[Done] Fold  2 | Test: subject2\n",
            "[Done] Fold  3 | Test: subject3\n",
            "[Done] Fold  4 | Test: subject4\n",
            "[Done] Fold  5 | Test: subject5\n",
            "[Done] Fold  6 | Test: subject6\n",
            "[Done] Fold  7 | Test: subject7\n",
            "[Done] Fold  8 | Test: subject8\n",
            "[Done] Fold  9 | Test: subject9\n",
            "[Done] Fold 10 | Test: subject10\n",
            "\n",
            "------------------------------------------------------------------------------------------\n",
            "[Activity] 8 | Knees bending\n",
            "------------------------------------------------------------------------------------------\n",
            "[Done] Fold  1 | Test: subject1\n",
            "[Done] Fold  2 | Test: subject2\n",
            "[Done] Fold  3 | Test: subject3\n",
            "[Done] Fold  4 | Test: subject4\n",
            "[Done] Fold  5 | Test: subject5\n",
            "[Done] Fold  6 | Test: subject6\n",
            "[Done] Fold  7 | Test: subject7\n",
            "[Done] Fold  8 | Test: subject8\n",
            "[Done] Fold  9 | Test: subject9\n",
            "[Done] Fold 10 | Test: subject10\n",
            "\n",
            "------------------------------------------------------------------------------------------\n",
            "[Activity] 10 | Jogging\n",
            "------------------------------------------------------------------------------------------\n",
            "[Done] Fold  1 | Test: subject1\n",
            "[Done] Fold  2 | Test: subject2\n",
            "[Done] Fold  3 | Test: subject3\n",
            "[Done] Fold  4 | Test: subject4\n",
            "[Done] Fold  5 | Test: subject5\n",
            "[Done] Fold  6 | Test: subject6\n",
            "[Done] Fold  7 | Test: subject7\n",
            "[Done] Fold  8 | Test: subject8\n",
            "[Done] Fold  9 | Test: subject9\n",
            "[Done] Fold 10 | Test: subject10\n",
            "\n",
            "------------------------------------------------------------------------------------------\n",
            "[Activity] 11 | Running\n",
            "------------------------------------------------------------------------------------------\n",
            "[Done] Fold  1 | Test: subject1\n",
            "[Done] Fold  2 | Test: subject2\n",
            "[Done] Fold  3 | Test: subject3\n",
            "[Done] Fold  4 | Test: subject4\n",
            "[Done] Fold  5 | Test: subject5\n",
            "[Done] Fold  6 | Test: subject6\n",
            "[Done] Fold  7 | Test: subject7\n",
            "[Done] Fold  8 | Test: subject8\n",
            "[Done] Fold  9 | Test: subject9\n",
            "[Done] Fold 10 | Test: subject10\n",
            "\n",
            "------------------------------------------------------------------------------------------\n",
            "[Activity] 12 | Jump front & back\n",
            "------------------------------------------------------------------------------------------\n",
            "[Done] Fold  1 | Test: subject1\n",
            "[Done] Fold  2 | Test: subject2\n",
            "[Done] Fold  3 | Test: subject3\n",
            "[Done] Fold  4 | Test: subject4\n",
            "[Done] Fold  5 | Test: subject5\n",
            "[Done] Fold  6 | Test: subject6\n",
            "[Done] Fold  7 | Test: subject7\n",
            "[Done] Fold  8 | Test: subject8\n",
            "[Done] Fold  9 | Test: subject9\n",
            "[Done] Fold 10 | Test: subject10\n",
            "\n",
            "Saved: ./robustness_results/robustness_raw.csv\n",
            "Saved: ./robustness_results/robustness_summary.csv\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'level_num'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3805\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3806\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine._get_loc_duplicates\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine._maybe_get_bool_indexer\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index._unpack_bool_indexer\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'level_num'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3267957838.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1173\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1174\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-3267957838.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1158\u001b[0m         \u001b[0mout_png\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplot_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"act{act_id}_{stressor}.png\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1159\u001b[0m         \u001b[0mg2_plot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mg2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"level_num\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"level\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1160\u001b[0;31m         save_degradation_plot(\n\u001b[0m\u001b[1;32m   1161\u001b[0m             \u001b[0mg2_plot\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1162\u001b[0m             \u001b[0mout_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout_png\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3267957838.py\u001b[0m in \u001b[0;36msave_degradation_plot\u001b[0;34m(df_agg, out_path, title, x_col, y_col, yerr_col)\u001b[0m\n\u001b[1;32m    663\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    664\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msave_degradation_plot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_agg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"level\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"mae_mean\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myerr_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"mae_std\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 665\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_agg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx_col\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    666\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_agg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my_col\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    667\u001b[0m     \u001b[0myerr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_agg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0myerr_col\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0myerr_col\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf_agg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4101\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4102\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4103\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4104\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3810\u001b[0m             ):\n\u001b[1;32m   3811\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mInvalidIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3812\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3813\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3814\u001b[0m             \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'level_num'"
          ]
        }
      ],
      "source": [
        "# =========================\n",
        "# Count-only K-auto (Multi-event) + Windowing version\n",
        "# + ✅ Robustness: Clean-train → Corrupted-test (test only)\n",
        "#\n",
        "# Protocol (reflected in code):\n",
        "# - TRAIN: always clean (trial -> sliding windows, window label from trial-average rate)\n",
        "# - TEST : trial 그대로 두고, \"정규화된(clean) 신호\"에 corruption 적용 후 평가\n",
        "# - TEST pred: windowing inference (window rate 평균 → trial count)\n",
        "# - clean→clean baseline은 측정하지 않도록 옵션(INCLUDE_BASELINE_LEVELS=False) 제공\n",
        "#\n",
        "# Stressors:\n",
        "# 1) Noise: Gaussian / Bias drift / Spike\n",
        "# 2) Sampling rate: fs 50→25→12.5 (anti-aliasing low-pass → decimate)\n",
        "# 3) Axis dropout cases: Acc z-only / Gyro 제거 / Acc only / Gyro only (차원 유지: 0-masking)\n",
        "# 4) Window mismatch: test win_sec ∈ {2,3,4,6,8}, stride=win/2 (50% overlap)\n",
        "#\n",
        "# Metrics:\n",
        "# - MAE, MAPE (+ Worst-case)\n",
        "# - Degradation curves saved as PNG\n",
        "# =========================\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.ndimage import gaussian_filter1d\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# scipy signal (for LPF)\n",
        "try:\n",
        "    from scipy.signal import butter, filtfilt\n",
        "    _SCIPY_SIGNAL_OK = True\n",
        "except Exception:\n",
        "    _SCIPY_SIGNAL_OK = False\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 1) Strict Seeding\n",
        "# ---------------------------------------------------------------------\n",
        "def set_strict_seed(seed: int):\n",
        "    random.seed(seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 1.5) Build ALL_LABELS from COUNT_TABLE (minimal add)\n",
        "# ---------------------------------------------------------------------\n",
        "def build_all_labels_from_count_table(count_table, subjects, target_map, feature_map, allow_only_mapped=True):\n",
        "    \"\"\"\n",
        "    COUNT_TABLE: {act_id: {subjectX: gt_count}}\n",
        "    Returns ALL_LABELS list of (subj, act_id, gt)\n",
        "    - allow_only_mapped=True: target_map & feature_map에 존재하는 act_id만 포함 (10/11 같은 미등록 act 자동 제외)\n",
        "    \"\"\"\n",
        "    labels = []\n",
        "    for act_id in sorted(count_table.keys()):\n",
        "        if allow_only_mapped and ((act_id not in target_map) or (act_id not in feature_map)):\n",
        "            continue\n",
        "        subj_dict = count_table.get(act_id, {})\n",
        "        for subj in subjects:\n",
        "            if subj in subj_dict:\n",
        "                labels.append((subj, int(act_id), int(subj_dict[subj])))\n",
        "    return labels\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 2) Data Loading\n",
        "# ---------------------------------------------------------------------\n",
        "def load_mhealth_dataset(data_dir, target_activities_map, column_names):\n",
        "    full_dataset = {}\n",
        "    file_list = sorted(glob.glob(os.path.join(data_dir, \"mHealth_subject*.log\")))\n",
        "\n",
        "    if not file_list:\n",
        "        print(f\"[Warning] No mHealth logs found in {data_dir}\")\n",
        "        return {}\n",
        "\n",
        "    print(f\"Loading {len(file_list)} subjects from {data_dir}...\")\n",
        "\n",
        "    for file_path in file_list:\n",
        "        file_name = os.path.basename(file_path)\n",
        "        subj_part = file_name.split('.')[0]\n",
        "        try:\n",
        "            subj_id_num = int(''.join(filter(str.isdigit, subj_part)))\n",
        "            subj_key = f\"subject{subj_id_num}\"\n",
        "        except:\n",
        "            subj_key = subj_part\n",
        "\n",
        "        try:\n",
        "            df = pd.read_csv(file_path, sep=\"\\t\", header=None)\n",
        "            df = df.iloc[:, :len(column_names)]\n",
        "            df.columns = column_names\n",
        "\n",
        "            subj_data = {}\n",
        "            for label_code, activity_name in target_activities_map.items():\n",
        "                activity_df = df[df['activity_id'] == label_code].copy()\n",
        "                if not activity_df.empty:\n",
        "                    subj_data[activity_name] = activity_df.drop(columns=['activity_id'])\n",
        "\n",
        "            full_dataset[subj_key] = subj_data\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {file_name}: {e}\")\n",
        "            pass\n",
        "\n",
        "    return full_dataset\n",
        "\n",
        "\n",
        "def prepare_trial_list(label_config, full_data, target_map, feature_map):\n",
        "    \"\"\"\n",
        "    NOTE: per-trial z-score normalization (clean 기준은 각 trial 자체)\n",
        "    \"\"\"\n",
        "    trial_list = []\n",
        "    for subj, act_id, gt_count in label_config:\n",
        "        act_name = target_map.get(act_id)\n",
        "        feats = feature_map.get(act_id)\n",
        "\n",
        "        if subj in full_data and act_name in full_data[subj] and feats is not None:\n",
        "            raw_df = full_data[subj][act_name][feats]\n",
        "            raw_np = raw_df.values.astype(np.float32)\n",
        "\n",
        "            # Z-score 정규화 (표준화) 평균=0, std=1  (clean signal로 계산)\n",
        "            mean = raw_np.mean(axis=0)\n",
        "            std = raw_np.std(axis=0) + 1e-6\n",
        "            norm_np = (raw_np - mean) / std\n",
        "\n",
        "            trial_list.append({\n",
        "                'data': norm_np,              # (T, C) normalized\n",
        "                'count': float(gt_count),      # trial total count\n",
        "                'meta': f\"{subj}_{act_name}\"\n",
        "            })\n",
        "        else:\n",
        "            print(f\"[Skip] Missing data for {subj} - {act_name}\")\n",
        "\n",
        "    return trial_list\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 2.5) ✅ Windowing (TRAIN)\n",
        "# ---------------------------------------------------------------------\n",
        "def trial_list_to_windows(trial_list, fs, win_sec=8.0, stride_sec=4.0, drop_last=True):\n",
        "    \"\"\"\n",
        "    TRAIN only: trial -> sliding windows\n",
        "    window label from trial-average rate:\n",
        "      rate_trial = count_total / total_duration\n",
        "      count_window = rate_trial * window_duration\n",
        "    \"\"\"\n",
        "    win_len = int(round(win_sec * fs))\n",
        "    stride = int(round(stride_sec * fs))\n",
        "    assert win_len > 0 and stride > 0\n",
        "\n",
        "    windows = []\n",
        "    for item in trial_list:\n",
        "        x = item[\"data\"]  # (T,C)\n",
        "        T = x.shape[0]\n",
        "        total_count = float(item[\"count\"])\n",
        "        meta = item[\"meta\"]\n",
        "\n",
        "        total_dur = max(T / float(fs), 1e-6)\n",
        "        rate_trial = total_count / total_dur  # reps/s\n",
        "\n",
        "        if T < win_len:\n",
        "            win_dur = T / float(fs)\n",
        "            windows.append({\n",
        "                \"data\": x,\n",
        "                \"count\": rate_trial * win_dur,\n",
        "                \"meta\": f\"{meta}__win[0:{T}]\",\n",
        "                \"parent_meta\": meta,\n",
        "                \"parent_T\": T,\n",
        "                \"win_start\": 0,\n",
        "                \"win_end\": T,\n",
        "            })\n",
        "            continue\n",
        "\n",
        "        last_start = T - win_len\n",
        "        starts = list(range(0, last_start + 1, stride))\n",
        "\n",
        "        for st in starts:\n",
        "            ed = st + win_len\n",
        "            win_dur = win_len / float(fs)\n",
        "            windows.append({\n",
        "                \"data\": x[st:ed],\n",
        "                \"count\": rate_trial * win_dur,\n",
        "                \"meta\": f\"{meta}__win[{st}:{ed}]\",\n",
        "                \"parent_meta\": meta,\n",
        "                \"parent_T\": T,\n",
        "                \"win_start\": st,\n",
        "                \"win_end\": ed,\n",
        "            })\n",
        "\n",
        "        if not drop_last:\n",
        "            last_st = starts[-1] + stride\n",
        "            if last_st < T:\n",
        "                ed = T\n",
        "                win_dur = (ed - last_st) / float(fs)\n",
        "                windows.append({\n",
        "                    \"data\": x[last_st:ed],\n",
        "                    \"count\": rate_trial * win_dur,\n",
        "                    \"meta\": f\"{meta}__win[{last_st}:{ed}]\",\n",
        "                    \"parent_meta\": meta,\n",
        "                    \"parent_T\": T,\n",
        "                    \"win_start\": last_st,\n",
        "                    \"win_end\": ed,\n",
        "                })\n",
        "\n",
        "    return windows\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 2.6) ✅ Windowing inference (TEST)\n",
        "# ---------------------------------------------------------------------\n",
        "def predict_count_by_windowing(model, x_np, fs, win_sec, stride_sec, device, tau=1.0, batch_size=64):\n",
        "    \"\"\"\n",
        "    TEST: trial -> sliding windows inference -> window rate mean -> total count\n",
        "    x_np: (T,C) numpy (already normalized, then corrupted)\n",
        "    return: pred_count(float), window_rates(np.ndarray)\n",
        "    \"\"\"\n",
        "    win_len = int(round(win_sec * fs))\n",
        "    stride = int(round(stride_sec * fs))\n",
        "    T = x_np.shape[0]\n",
        "    total_dur = T / float(fs)\n",
        "\n",
        "    # short trial -> 1 forward\n",
        "    if T <= win_len:\n",
        "        x_tensor = torch.tensor(x_np, dtype=torch.float32).transpose(0, 1).unsqueeze(0).to(device)  # (1,C,T)\n",
        "        with torch.no_grad():\n",
        "            rate_hat, _, _, _ = model(x_tensor, mask=None, tau=tau)\n",
        "        pred_count = float(rate_hat.item() * total_dur)\n",
        "        return pred_count, np.array([float(rate_hat.item())], dtype=np.float32)\n",
        "\n",
        "    starts = list(range(0, T - win_len + 1, stride))\n",
        "    windows = np.stack([x_np[st:st + win_len] for st in starts], axis=0)  # (N, win_len, C)\n",
        "    xw = torch.tensor(windows, dtype=torch.float32).permute(0, 2, 1).to(device)  # (N, C, win_len)\n",
        "\n",
        "    rates = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, xw.shape[0], batch_size):\n",
        "            xb = xw[i:i + batch_size]\n",
        "            r_hat, _, _, _ = model(xb, mask=None, tau=tau)  # (B,)\n",
        "            rates.append(r_hat.detach().cpu().numpy())\n",
        "\n",
        "    rates = np.concatenate(rates, axis=0)  # (N,)\n",
        "    rate_mean = float(rates.mean())\n",
        "    pred_count = rate_mean * total_dur\n",
        "    return float(pred_count), rates\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 2.8) Dataset / Collate\n",
        "# ---------------------------------------------------------------------\n",
        "class TrialDataset(Dataset):\n",
        "    def __init__(self, trial_list):\n",
        "        self.trials = trial_list\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.trials)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.trials[idx]\n",
        "        data = torch.tensor(item['data'], dtype=torch.float32).transpose(0, 1)  # (C, T)\n",
        "        count = torch.tensor(item['count'], dtype=torch.float32)\n",
        "        return data, count, item['meta']\n",
        "\n",
        "\n",
        "def collate_variable_length(batch):\n",
        "    max_len = max([x[0].shape[1] for x in batch])\n",
        "    C = batch[0][0].shape[0]\n",
        "\n",
        "    padded_data, masks, counts, metas, lengths = [], [], [], [], []\n",
        "    for data, count, meta in batch:\n",
        "        T = data.shape[1]\n",
        "        lengths.append(T)\n",
        "\n",
        "        pad_size = max_len - T\n",
        "        if pad_size > 0:\n",
        "            pad = torch.zeros(C, pad_size)\n",
        "            d_padded = torch.cat([data, pad], dim=1)\n",
        "            mask = torch.cat([torch.ones(T), torch.zeros(pad_size)], dim=0)\n",
        "        else:\n",
        "            d_padded = data\n",
        "            mask = torch.ones(T)\n",
        "\n",
        "        padded_data.append(d_padded)\n",
        "        masks.append(mask)\n",
        "        counts.append(count)\n",
        "        metas.append(meta)\n",
        "\n",
        "    return {\n",
        "        \"data\": torch.stack(padded_data),         # (B, C, T_max)\n",
        "        \"mask\": torch.stack(masks),               # (B, T_max)\n",
        "        \"count\": torch.stack(counts),             # (B,)\n",
        "        \"length\": torch.tensor(lengths, dtype=torch.float32),  # (B,)\n",
        "        \"meta\": metas\n",
        "    }\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 3) Model\n",
        "# ---------------------------------------------------------------------\n",
        "class ManifoldEncoder(nn.Module):\n",
        "    def __init__(self, input_ch, hidden_dim=128, latent_dim=16):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv1d(input_ch, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, latent_dim, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        z = self.net(x)            # (B, D, T)\n",
        "        z = z.transpose(1, 2)      # (B, T, D)\n",
        "        return z\n",
        "\n",
        "\n",
        "class ManifoldDecoder(nn.Module):\n",
        "    def __init__(self, latent_dim, hidden_dim, out_ch):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv1d(latent_dim, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, out_ch, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        zt = z.transpose(1, 2)     # (B, D, T)\n",
        "        x_hat = self.net(zt)       # (B, C, T)\n",
        "        return x_hat\n",
        "\n",
        "\n",
        "class MultiRateHead(nn.Module):\n",
        "    def __init__(self, latent_dim=16, hidden=64, K_max=6):\n",
        "        super().__init__()\n",
        "        self.K_max = K_max\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(latent_dim, hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden, 1 + K_max)  # [amp_logit | phase_logits...]\n",
        "        )\n",
        "\n",
        "    def forward(self, z, tau=1.0):\n",
        "        out = self.net(z)                     # (B,T,1+K)\n",
        "        amp = F.softplus(out[..., 0])         # (B,T) >=0\n",
        "        phase_logits = out[..., 1:]           # (B,T,K)\n",
        "        phase = F.softmax(phase_logits / tau, dim=-1)  # (B,T,K)\n",
        "        return amp, phase, phase_logits\n",
        "\n",
        "\n",
        "class KAutoCountModel(nn.Module):\n",
        "    def __init__(self, input_ch, hidden_dim=128, latent_dim=16, K_max=6):\n",
        "        super().__init__()\n",
        "        self.encoder = ManifoldEncoder(input_ch, hidden_dim, latent_dim)\n",
        "        self.decoder = ManifoldDecoder(latent_dim, hidden_dim, input_ch)\n",
        "        self.rate_head = MultiRateHead(latent_dim, hidden=hidden_dim, K_max=K_max)\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, (nn.Conv1d, nn.Linear)):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            b = self.rate_head.net[-1].bias\n",
        "            b.zero_()\n",
        "            b[0].fill_(-2.0)  # amp bias만 -2\n",
        "\n",
        "    @staticmethod\n",
        "    def _masked_mean_time(x, mask=None, eps=1e-6):\n",
        "        if mask is None:\n",
        "            return x.mean(dim=1)\n",
        "        if x.dim() == 2:\n",
        "            m = mask.to(dtype=x.dtype, device=x.device)\n",
        "            return (x * m).sum(dim=1) / (m.sum(dim=1) + eps)\n",
        "        elif x.dim() == 3:\n",
        "            m = mask.to(dtype=x.dtype, device=x.device).unsqueeze(-1)\n",
        "            return (x * m).sum(dim=1) / (m.sum(dim=1) + eps)\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported dim for masked mean: {x.dim()}\")\n",
        "\n",
        "    def forward(self, x, mask=None, tau=1.0):\n",
        "        z = self.encoder(x)              # (B,T,D)\n",
        "        x_hat = self.decoder(z)          # (B,C,T)\n",
        "\n",
        "        amp_t, phase_p, phase_logits = self.rate_head(z, tau=tau)\n",
        "        rates_k_t = amp_t.unsqueeze(-1) * phase_p  # (B,T,K)\n",
        "\n",
        "        micro_rate_t = amp_t  # (B,T)\n",
        "        p_bar = self._masked_mean_time(phase_p, mask)           # (B,K)\n",
        "        k_hat = 1.0 / (p_bar.pow(2).sum(dim=1) + 1e-6)          # (B,) in [1,K]\n",
        "\n",
        "        rep_rate_t = micro_rate_t / (k_hat.unsqueeze(1) + 1e-6) # (B,T)\n",
        "        if mask is not None:\n",
        "            rep_rate_t = rep_rate_t * mask\n",
        "\n",
        "        if mask is None:\n",
        "            avg_rep_rate = rep_rate_t.mean(dim=1)\n",
        "        else:\n",
        "            avg_rep_rate = (rep_rate_t * mask).sum(dim=1) / (mask.sum(dim=1) + 1e-6)\n",
        "\n",
        "        aux = {\n",
        "            \"rates_k_t\": rates_k_t,\n",
        "            \"phase_p\": phase_p,\n",
        "            \"phase_logits\": phase_logits,\n",
        "            \"micro_rate_t\": micro_rate_t,\n",
        "            \"rep_rate_t\": rep_rate_t,\n",
        "            \"k_hat\": k_hat,\n",
        "        }\n",
        "        return avg_rep_rate, z, x_hat, aux\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 4) Loss utils\n",
        "# ---------------------------------------------------------------------\n",
        "def masked_recon_mse(x_hat, x, mask, eps=1e-6):\n",
        "    mask = mask.to(dtype=x.dtype, device=x.device)\n",
        "    mask_bc = mask.unsqueeze(1)              # (B,1,T)\n",
        "    se = (x_hat - x) ** 2                    # (B,C,T)\n",
        "    se = se * mask_bc\n",
        "    denom = (mask.sum() * x.shape[1]) + eps  # valid(B*T)*C\n",
        "    return se.sum() / denom\n",
        "\n",
        "\n",
        "def temporal_smoothness(v, mask=None, eps=1e-6):\n",
        "    dv = torch.abs(v[:, 1:] - v[:, :-1])  # (B,T-1)\n",
        "    if mask is None:\n",
        "        return dv.mean()\n",
        "    m = mask[:, 1:] * mask[:, :-1]\n",
        "    m = m.to(dtype=dv.dtype, device=dv.device)\n",
        "    return (dv * m).sum() / (m.sum() + eps)\n",
        "\n",
        "\n",
        "def phase_entropy_loss(phase_p, mask=None, eps=1e-8):\n",
        "    ent = -(phase_p * (phase_p + eps).log()).sum(dim=-1)  # (B,T)\n",
        "    if mask is None:\n",
        "        return ent.mean()\n",
        "    ent = ent * mask\n",
        "    return ent.sum() / (mask.sum() + eps)\n",
        "\n",
        "\n",
        "def effK_usage_loss(phase_p, mask=None, eps=1e-6):\n",
        "    if mask is None:\n",
        "        p_bar = phase_p.mean(dim=1)  # (B,K)\n",
        "    else:\n",
        "        m = mask.to(dtype=phase_p.dtype, device=phase_p.device).unsqueeze(-1)  # (B,T,1)\n",
        "        p_bar = (phase_p * m).sum(dim=1) / (m.sum(dim=1) + eps)\n",
        "\n",
        "    effK = 1.0 / (p_bar.pow(2).sum(dim=1) + eps)\n",
        "    return effK.mean(), effK.detach()\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 5) Train\n",
        "# ---------------------------------------------------------------------\n",
        "def train_one_epoch(model, loader, optimizer, config, device):\n",
        "    model.train()\n",
        "    stats = {k: 0.0 for k in [\n",
        "        'loss', 'loss_rate', 'loss_recon', 'loss_smooth', 'loss_phase_ent', 'loss_effk',\n",
        "        'mae_count'\n",
        "    ]}\n",
        "\n",
        "    fs = config[\"fs\"]\n",
        "    tau = config.get(\"tau\", 1.0)\n",
        "\n",
        "    lam_recon = config.get(\"lambda_recon\", 1.0)\n",
        "    lam_smooth = config.get(\"lambda_smooth\", 0.05)\n",
        "    lam_phase_ent = config.get(\"lambda_phase_ent\", 0.01)\n",
        "    lam_effk = config.get(\"lambda_effk\", 0.005)\n",
        "\n",
        "    for batch in loader:\n",
        "        x = batch[\"data\"].to(device)         # (B,C,T)\n",
        "        mask = batch[\"mask\"].to(device)      # (B,T)\n",
        "        y_count = batch[\"count\"].to(device)  # (B,)\n",
        "        length = batch[\"length\"].to(device)  # (B,)\n",
        "\n",
        "        duration = torch.clamp(length / fs, min=1e-6)  # sec\n",
        "        y_rate = y_count / duration                    # reps/s\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        rate_hat, z, x_hat, aux = model(x, mask, tau=tau)\n",
        "\n",
        "        loss_rate = F.mse_loss(rate_hat, y_rate)\n",
        "        loss_recon = masked_recon_mse(x_hat, x, mask)\n",
        "        loss_smooth = temporal_smoothness(aux[\"rep_rate_t\"], mask)\n",
        "        loss_phase_ent = phase_entropy_loss(aux[\"phase_p\"], mask)\n",
        "        loss_effk, _ = effK_usage_loss(aux[\"phase_p\"], mask)\n",
        "\n",
        "        loss = (loss_rate\n",
        "                + lam_recon * loss_recon\n",
        "                + lam_smooth * loss_smooth\n",
        "                + lam_phase_ent * loss_phase_ent\n",
        "                + lam_effk * loss_effk)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        count_hat = rate_hat * duration\n",
        "        stats['loss'] += loss.item()\n",
        "        stats['loss_rate'] += loss_rate.item()\n",
        "        stats['loss_recon'] += loss_recon.item()\n",
        "        stats['loss_smooth'] += loss_smooth.item()\n",
        "        stats['loss_phase_ent'] += loss_phase_ent.item()\n",
        "        stats['loss_effk'] += loss_effk.item()\n",
        "        stats['mae_count'] += torch.abs(count_hat - y_count).mean().item()\n",
        "\n",
        "    n = max(len(loader), 1)\n",
        "    return {k: v / n for k, v in stats.items()}\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 6) Robustness: Corruption functions (apply AFTER normalization)\n",
        "# ---------------------------------------------------------------------\n",
        "def _rng_from_meta(seed: int, meta: str):\n",
        "    # deterministic per (seed, meta)\n",
        "    h = abs(hash((seed, meta))) % (2**32 - 1)\n",
        "    return np.random.default_rng(h)\n",
        "\n",
        "\n",
        "def add_gaussian_noise(x, sigma, rng):\n",
        "    return (x + rng.normal(0.0, sigma, size=x.shape)).astype(np.float32)\n",
        "\n",
        "\n",
        "def add_bias_drift(x, fs, amplitude, rng, freq_hz=0.2):\n",
        "    # common low-freq drift\n",
        "    T, C = x.shape\n",
        "    t = np.arange(T, dtype=np.float32) / float(fs)\n",
        "    phase0 = float(rng.uniform(0, 2*np.pi))\n",
        "    drift = amplitude * np.sin(2*np.pi*freq_hz*t + phase0)  # (T,)\n",
        "    drift = drift[:, None]  # (T,1)\n",
        "    return (x + drift).astype(np.float32)\n",
        "\n",
        "\n",
        "def add_spikes(x, p, A, rng):\n",
        "    T, C = x.shape\n",
        "    n_spike = int(round(p * T))\n",
        "    if n_spike <= 0:\n",
        "        return x.astype(np.float32)\n",
        "\n",
        "    idx = rng.choice(T, size=min(n_spike, T), replace=False)\n",
        "    spike = np.zeros_like(x, dtype=np.float32)\n",
        "    for i in idx:\n",
        "        signs = rng.choice([-1.0, 1.0], size=C).astype(np.float32)\n",
        "        spike[i, :] += (A * signs)\n",
        "    return (x + spike).astype(np.float32)\n",
        "\n",
        "\n",
        "def lowpass_then_decimate(x, fs_orig, fs_new, order=4):\n",
        "    \"\"\"\n",
        "    Anti-aliasing low-pass → decimate (integer factor only)\n",
        "    Operates on normalized signal (per protocol).\n",
        "    \"\"\"\n",
        "    if not _SCIPY_SIGNAL_OK:\n",
        "        raise RuntimeError(\"scipy.signal not available. Please install SciPy or switch to an FIR fallback.\")\n",
        "\n",
        "    ratio = fs_orig / fs_new\n",
        "    if abs(ratio - round(ratio)) > 1e-6:\n",
        "        raise ValueError(f\"fs_orig/fs_new must be integer. Got {fs_orig}/{fs_new}={ratio}\")\n",
        "    factor = int(round(ratio))\n",
        "    if factor < 1:\n",
        "        raise ValueError(\"Decimation factor must be >=1\")\n",
        "\n",
        "    if factor == 1:\n",
        "        return x.astype(np.float32)\n",
        "\n",
        "    # cutoff slightly below new Nyquist\n",
        "    nyq_orig = fs_orig / 2.0\n",
        "    cutoff_hz = 0.45 * fs_new  # 0.9 * (fs_new/2)\n",
        "    Wn = cutoff_hz / nyq_orig\n",
        "    Wn = min(max(Wn, 1e-4), 0.99)\n",
        "\n",
        "    b, a = butter(order, Wn, btype='low')\n",
        "\n",
        "    x_f = np.zeros_like(x, dtype=np.float32)\n",
        "    for c in range(x.shape[1]):\n",
        "        x_f[:, c] = filtfilt(b, a, x[:, c].astype(np.float64)).astype(np.float32)\n",
        "\n",
        "    return x_f[::factor].astype(np.float32)\n",
        "\n",
        "\n",
        "def apply_axis_dropout_case(x, case_name):\n",
        "    \"\"\"\n",
        "    Keep channel dimension (0-masking).\n",
        "    Assumes ACT_FEATURE_MAP uses:\n",
        "    [chest acc(3), ankle acc(3), ankle gyro(3), arm acc(3), arm gyro(3)] = 15 ch\n",
        "    \"\"\"\n",
        "    x = x.astype(np.float32).copy()\n",
        "    T, C = x.shape\n",
        "    if C != 15:\n",
        "        print(f\"[Warn] Axis dropout assumes C=15, got C={C}. Skip masking.\")\n",
        "        return x\n",
        "\n",
        "    chest_acc = [0, 1, 2]\n",
        "    ankle_acc = [3, 4, 5]\n",
        "    ankle_gyro = [6, 7, 8]\n",
        "    arm_acc   = [9, 10, 11]\n",
        "    arm_gyro  = [12, 13, 14]\n",
        "\n",
        "    acc_all = chest_acc + ankle_acc + arm_acc\n",
        "    gyro_all = ankle_gyro + arm_gyro\n",
        "\n",
        "    if case_name == \"acc_z_only\":\n",
        "        # keep z of each acc triad: chest z=2, ankle z=5, arm z=11\n",
        "        keep = [2, 5, 11]\n",
        "        drop = [i for i in acc_all if i not in keep]\n",
        "        x[:, drop] = 0.0\n",
        "        return x\n",
        "\n",
        "    if case_name == \"gyro_drop\":\n",
        "        x[:, gyro_all] = 0.0\n",
        "        return x\n",
        "\n",
        "    if case_name == \"acc_only\":\n",
        "        x[:, gyro_all] = 0.0\n",
        "        return x\n",
        "\n",
        "    if case_name == \"gyro_only\":\n",
        "        x[:, acc_all] = 0.0\n",
        "        return x\n",
        "\n",
        "    raise ValueError(f\"Unknown axis dropout case: {case_name}\")\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 7) Robustness evaluation helpers\n",
        "# ---------------------------------------------------------------------\n",
        "def compute_errors(pred, gt, eps=1e-6):\n",
        "    abs_err = float(abs(pred - gt))\n",
        "    mape = abs_err / float(max(gt, eps))\n",
        "    return abs_err, float(mape)\n",
        "\n",
        "\n",
        "def eval_one_trial_windowing(model, x_np, gt_count, fs_eval, win_sec_eval, device, tau, batch_size):\n",
        "    stride_sec_eval = win_sec_eval / 2.0  # always 50% overlap\n",
        "    pred_count, _ = predict_count_by_windowing(\n",
        "        model,\n",
        "        x_np=x_np,\n",
        "        fs=fs_eval,\n",
        "        win_sec=win_sec_eval,\n",
        "        stride_sec=stride_sec_eval,\n",
        "        device=device,\n",
        "        tau=tau,\n",
        "        batch_size=batch_size\n",
        "    )\n",
        "    mae, mape = compute_errors(pred_count, gt_count)\n",
        "    return pred_count, mae, mape\n",
        "\n",
        "\n",
        "def save_degradation_plot(df_agg, out_path, title, x_col=\"level\", y_col=\"mae_mean\", yerr_col=\"mae_std\"):\n",
        "    x = df_agg[x_col].values\n",
        "    y = df_agg[y_col].values\n",
        "    yerr = df_agg[yerr_col].values if yerr_col in df_agg.columns else None\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    if yerr is not None:\n",
        "        plt.errorbar(x, y, yerr=yerr, marker='o', linestyle='-')\n",
        "    else:\n",
        "        plt.plot(x, y, marker='o', linestyle='-')\n",
        "    plt.title(title)\n",
        "    plt.xlabel(x_col)\n",
        "    plt.ylabel(\"MAE\")\n",
        "    plt.grid(True, linestyle=\"--\", alpha=0.5)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(out_path, dpi=200)\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 8) Main (LOSO + Robustness sweeps)\n",
        "# ---------------------------------------------------------------------\n",
        "def main():\n",
        "    CONFIG = {\n",
        "        \"seed\": 42,\n",
        "        \"data_dir\": \"/content/drive/MyDrive/Colab Notebooks/HAR_data/MHEALTHDATASET\",\n",
        "        \"out_dir\": \"./robustness_results\",\n",
        "        \"INCLUDE_BASELINE_LEVELS\": False,  # clean→clean baseline은 이미 있다 했으니 False 권장\n",
        "\n",
        "        \"COLUMN_NAMES\": [\n",
        "            'acc_chest_x', 'acc_chest_y', 'acc_chest_z',\n",
        "            'ecg_1', 'ecg_2',\n",
        "            'acc_ankle_x', 'acc_ankle_y', 'acc_ankle_z',\n",
        "            'gyro_ankle_x', 'gyro_ankle_y', 'gyro_ankle_z',\n",
        "            'mag_ankle_x', 'mag_ankle_y', 'mag_ankle_z',\n",
        "            'acc_arm_x', 'acc_arm_y', 'acc_arm_z',\n",
        "            'gyro_arm_x', 'gyro_arm_y', 'gyro_arm_z',\n",
        "            'mag_arm_x', 'mag_arm_y', 'mag_arm_z',\n",
        "            'activity_id'\n",
        "        ],\n",
        "\n",
        "        # ✅ You will extend/replace these with your 10 activities and GT counts\n",
        "        \"TARGET_ACTIVITIES_MAP\": {\n",
        "            6:  'Waist bends forward',\n",
        "            7:  'Frontal elevation of arms',\n",
        "            8:  'Knees bending',\n",
        "            10:  'Jogging',\n",
        "            11: 'Running',\n",
        "            12: 'Jump front & back',\n",
        "        },\n",
        "\n",
        "        # ✅ Using 15 channels (acc+gyro only) as in your current code\n",
        "        \"ACT_FEATURE_MAP\": {\n",
        "            6:  ['acc_chest_x','acc_chest_y','acc_chest_z','acc_ankle_x','acc_ankle_y','acc_ankle_z',\n",
        "                 'gyro_ankle_x','gyro_ankle_y','gyro_ankle_z','acc_arm_x','acc_arm_y','acc_arm_z',\n",
        "                 'gyro_arm_x','gyro_arm_y','gyro_arm_z'],\n",
        "            7:  ['acc_chest_x','acc_chest_y','acc_chest_z','acc_ankle_x','acc_ankle_y','acc_ankle_z',\n",
        "                 'gyro_ankle_x','gyro_ankle_y','gyro_ankle_z','acc_arm_x','acc_arm_y','acc_arm_z',\n",
        "                 'gyro_arm_x','gyro_arm_y','gyro_arm_z'],\n",
        "            8:  ['acc_chest_x','acc_chest_y','acc_chest_z','acc_ankle_x','acc_ankle_y','acc_ankle_z',\n",
        "                 'gyro_ankle_x','gyro_ankle_y','gyro_ankle_z','acc_arm_x','acc_arm_y','acc_arm_z',\n",
        "                 'gyro_arm_x','gyro_arm_y','gyro_arm_z'],\n",
        "            10:  ['acc_chest_x','acc_chest_y','acc_chest_z','acc_ankle_x','acc_ankle_y','acc_ankle_z',\n",
        "                 'gyro_ankle_x','gyro_ankle_y','gyro_ankle_z','acc_arm_x','acc_arm_y','acc_arm_z',\n",
        "                 'gyro_arm_x','gyro_arm_y','gyro_arm_z'],\n",
        "            11: ['acc_chest_x','acc_chest_y','acc_chest_z','acc_ankle_x','acc_ankle_y','acc_ankle_z',\n",
        "                 'gyro_ankle_x','gyro_ankle_y','gyro_ankle_z','acc_arm_x','acc_arm_y','acc_arm_z',\n",
        "                 'gyro_arm_x','gyro_arm_y','gyro_arm_z'],\n",
        "            12: ['acc_chest_x','acc_chest_y','acc_chest_z','acc_ankle_x','acc_ankle_y','acc_ankle_z',\n",
        "                 'gyro_ankle_x','gyro_ankle_y','gyro_ankle_z','acc_arm_x','acc_arm_y','acc_arm_z',\n",
        "                 'gyro_arm_x','gyro_arm_y','gyro_arm_z'],\n",
        "        },\n",
        "\n",
        "        # Training Params\n",
        "        \"epochs\": 100,\n",
        "        \"lr\": 5e-4,\n",
        "        \"batch_size\": 64,\n",
        "        \"fs\": 50,\n",
        "\n",
        "        # Windowing Params (TRAIN fixed 8s/4s)\n",
        "        \"win_sec_train\": 8.0,\n",
        "        \"stride_sec_train\": 4.0,\n",
        "        \"drop_last\": True,\n",
        "\n",
        "        # Model\n",
        "        \"hidden_dim\": 128,\n",
        "        \"latent_dim\": 16,\n",
        "        \"K_max\": 6,\n",
        "\n",
        "        # Loss Weights\n",
        "        \"lambda_recon\": 1.0,\n",
        "        \"lambda_smooth\": 0.05,\n",
        "        \"lambda_phase_ent\": 0.01,\n",
        "        \"lambda_effk\": 0.0075,\n",
        "\n",
        "        # temperature\n",
        "        \"tau\": 1.0,\n",
        "\n",
        "        # Count-only labels (your GT table)\n",
        "        \"COUNT_TABLE\": {\n",
        "            6: {\n",
        "                \"subject1\": 21, \"subject2\": 19, \"subject3\": 21, \"subject4\": 20, \"subject5\": 20,\n",
        "                \"subject6\": 20, \"subject7\": 20, \"subject8\": 21, \"subject9\": 21, \"subject10\": 20,\n",
        "            },\n",
        "            7: {\n",
        "                \"subject1\": 20, \"subject2\": 20, \"subject3\": 20, \"subject4\": 20, \"subject5\": 20,\n",
        "                \"subject6\": 20, \"subject7\": 20, \"subject8\": 19, \"subject9\": 19, \"subject10\": 20,\n",
        "            },\n",
        "            8: {\n",
        "                \"subject1\": 20, \"subject2\": 21, \"subject3\": 21, \"subject4\": 19, \"subject5\": 20,\n",
        "                \"subject6\": 20, \"subject7\": 21, \"subject8\": 21, \"subject9\": 21, \"subject10\": 21,\n",
        "            },\n",
        "            10: {\n",
        "                \"subject1\": 157, \"subject2\": 161, \"subject3\": 154, \"subject4\": 154, \"subject5\": 160,\n",
        "                \"subject6\": 156, \"subject7\": 153, \"subject8\": 160, \"subject9\": 166, \"subject10\": 156,\n",
        "            },\n",
        "            11: {\n",
        "                \"subject1\": 165, \"subject2\": 158, \"subject3\": 174, \"subject4\": 163, \"subject5\": 157,\n",
        "                \"subject6\": 172, \"subject7\": 149, \"subject8\": 166, \"subject9\": 174, \"subject10\": 172,\n",
        "            },\n",
        "            12: {\n",
        "                \"subject1\": 20, \"subject2\": 22, \"subject3\": 21, \"subject4\": 21, \"subject5\": 20,\n",
        "                \"subject6\": 21, \"subject7\": 19, \"subject8\": 20, \"subject9\": 20, \"subject10\": 20,\n",
        "            },\n",
        "        }\n",
        "    }\n",
        "\n",
        "    os.makedirs(CONFIG[\"out_dir\"], exist_ok=True)\n",
        "\n",
        "    set_strict_seed(CONFIG[\"seed\"])\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Device: {device}\")\n",
        "\n",
        "    full_data = load_mhealth_dataset(CONFIG[\"data_dir\"], CONFIG[\"TARGET_ACTIVITIES_MAP\"], CONFIG[\"COLUMN_NAMES\"])\n",
        "    if not full_data:\n",
        "        return\n",
        "\n",
        "    subjects = [f\"subject{i}\" for i in range(1, 11)]\n",
        "\n",
        "    # ✅ build ALL_LABELS from COUNT_TABLE (only mapped activities to avoid 10/11 unless you add maps)\n",
        "    CONFIG[\"ALL_LABELS\"] = build_all_labels_from_count_table(\n",
        "        CONFIG[\"COUNT_TABLE\"],\n",
        "        subjects,\n",
        "        target_map=CONFIG[\"TARGET_ACTIVITIES_MAP\"],\n",
        "        feature_map=CONFIG[\"ACT_FEATURE_MAP\"],\n",
        "        allow_only_mapped=True\n",
        "    )\n",
        "\n",
        "    # -------------------------\n",
        "    # Robustness sweep configs\n",
        "    # -------------------------\n",
        "    noise_gauss_levels = [0.02, 0.05, 0.1, 0.2]\n",
        "    noise_drift_levels = [0.05, 0.1, 0.2]\n",
        "    noise_spike_p_levels = [0.001, 0.005, 0.01]\n",
        "    spike_A = 3.0\n",
        "\n",
        "    fs_levels = [50.0, 25.0, 12.5]\n",
        "    if not CONFIG[\"INCLUDE_BASELINE_LEVELS\"]:\n",
        "        fs_levels = [25.0, 12.5]  # exclude 50Hz baseline\n",
        "\n",
        "    window_test_levels = [2.0, 3.0, 4.0, 6.0, 8.0]\n",
        "    if not CONFIG[\"INCLUDE_BASELINE_LEVELS\"]:\n",
        "        window_test_levels = [2.0, 3.0, 4.0, 6.0]  # exclude 8s baseline\n",
        "\n",
        "    axis_cases = [\"acc_z_only\", \"gyro_drop\", \"acc_only\", \"gyro_only\"]\n",
        "\n",
        "    # all activities present in labels\n",
        "    act_ids_in_labels = sorted(list(set([a for (_, a, _) in CONFIG[\"ALL_LABELS\"]])))\n",
        "\n",
        "    records = []  # row-wise results\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 90)\n",
        "    print(\">>> Robustness Evaluation: Clean-train → Corrupted-test (test-only)\")\n",
        "    print(\"=\" * 90)\n",
        "\n",
        "    # -----------------------------------------------------------------\n",
        "    # For each activity: run LOSO training once per fold, test under all stressors\n",
        "    # -----------------------------------------------------------------\n",
        "    for act_id in act_ids_in_labels:\n",
        "        act_name = CONFIG[\"TARGET_ACTIVITIES_MAP\"].get(act_id, str(act_id))\n",
        "        print(\"\\n\" + \"-\" * 90)\n",
        "        print(f\"[Activity] {act_id} | {act_name}\")\n",
        "        print(\"-\" * 90)\n",
        "\n",
        "        labels_act = [x for x in CONFIG[\"ALL_LABELS\"] if x[1] == act_id]\n",
        "        if len(labels_act) == 0:\n",
        "            continue\n",
        "\n",
        "        for fold_idx, test_subj in enumerate(subjects, start=1):\n",
        "            set_strict_seed(CONFIG[\"seed\"])\n",
        "\n",
        "            train_labels = [x for x in labels_act if x[0] != test_subj]\n",
        "            test_labels  = [x for x in labels_act if x[0] == test_subj]\n",
        "\n",
        "            train_trials = prepare_trial_list(train_labels, full_data, CONFIG[\"TARGET_ACTIVITIES_MAP\"], CONFIG[\"ACT_FEATURE_MAP\"])\n",
        "            test_trials  = prepare_trial_list(test_labels,  full_data, CONFIG[\"TARGET_ACTIVITIES_MAP\"], CONFIG[\"ACT_FEATURE_MAP\"])\n",
        "\n",
        "            if not test_trials or not train_trials:\n",
        "                print(f\"[Skip] Fold {fold_idx}: {test_subj} (missing train or test trials)\")\n",
        "                continue\n",
        "\n",
        "            # TRAIN: clean, fixed 8s window\n",
        "            train_data = trial_list_to_windows(\n",
        "                train_trials,\n",
        "                fs=CONFIG[\"fs\"],\n",
        "                win_sec=CONFIG[\"win_sec_train\"],\n",
        "                stride_sec=CONFIG[\"stride_sec_train\"],\n",
        "                drop_last=CONFIG[\"drop_last\"]\n",
        "            )\n",
        "\n",
        "            g = torch.Generator()\n",
        "            g.manual_seed(CONFIG[\"seed\"])\n",
        "\n",
        "            train_loader = DataLoader(\n",
        "                TrialDataset(train_data),\n",
        "                batch_size=CONFIG[\"batch_size\"],\n",
        "                shuffle=True,\n",
        "                collate_fn=collate_variable_length,\n",
        "                generator=g,\n",
        "                num_workers=0\n",
        "            )\n",
        "\n",
        "            input_ch = train_data[0]['data'].shape[1]\n",
        "            model = KAutoCountModel(\n",
        "                input_ch=input_ch,\n",
        "                hidden_dim=CONFIG[\"hidden_dim\"],\n",
        "                latent_dim=CONFIG[\"latent_dim\"],\n",
        "                K_max=CONFIG[\"K_max\"]\n",
        "            ).to(device)\n",
        "\n",
        "            optimizer = torch.optim.Adam(model.parameters(), lr=CONFIG[\"lr\"])\n",
        "            scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.5)\n",
        "\n",
        "            for epoch in range(CONFIG[\"epochs\"]):\n",
        "                _ = train_one_epoch(model, train_loader, optimizer, CONFIG, device)\n",
        "                scheduler.step()\n",
        "\n",
        "            model.eval()\n",
        "\n",
        "            # TEST trials (usually 1 per subject per activity)\n",
        "            for item in test_trials:\n",
        "                x_clean = item[\"data\"]  # (T,C) normalized\n",
        "                gt = float(item[\"count\"])\n",
        "                meta = item[\"meta\"]\n",
        "\n",
        "                # ---------- Noise: Gaussian ----------\n",
        "                for sigma in noise_gauss_levels:\n",
        "                    rng = _rng_from_meta(CONFIG[\"seed\"], f\"{meta}|gauss|{sigma}\")\n",
        "                    x_cor = add_gaussian_noise(x_clean, sigma=sigma, rng=rng)\n",
        "\n",
        "                    pred, mae, mape = eval_one_trial_windowing(\n",
        "                        model, x_cor, gt,\n",
        "                        fs_eval=CONFIG[\"fs\"],\n",
        "                        win_sec_eval=CONFIG[\"win_sec_train\"],  # test win same as train for noise\n",
        "                        device=device,\n",
        "                        tau=CONFIG[\"tau\"],\n",
        "                        batch_size=CONFIG[\"batch_size\"]\n",
        "                    )\n",
        "\n",
        "                    records.append({\n",
        "                        \"activity_id\": act_id,\n",
        "                        \"activity\": act_name,\n",
        "                        \"fold\": fold_idx,\n",
        "                        \"test_subj\": test_subj,\n",
        "                        \"meta\": meta,\n",
        "                        \"stressor\": \"noise_gaussian\",\n",
        "                        \"level\": float(sigma),\n",
        "                        \"fs_eval\": float(CONFIG[\"fs\"]),\n",
        "                        \"win_sec_eval\": float(CONFIG[\"win_sec_train\"]),\n",
        "                        \"gt\": gt,\n",
        "                        \"pred\": float(pred),\n",
        "                        \"mae\": float(mae),\n",
        "                        \"mape\": float(mape),\n",
        "                    })\n",
        "\n",
        "                # ---------- Noise: Bias drift ----------\n",
        "                for amp in noise_drift_levels:\n",
        "                    rng = _rng_from_meta(CONFIG[\"seed\"], f\"{meta}|drift|{amp}\")\n",
        "                    x_cor = add_bias_drift(x_clean, fs=CONFIG[\"fs\"], amplitude=amp, rng=rng, freq_hz=0.2)\n",
        "\n",
        "                    pred, mae, mape = eval_one_trial_windowing(\n",
        "                        model, x_cor, gt,\n",
        "                        fs_eval=CONFIG[\"fs\"],\n",
        "                        win_sec_eval=CONFIG[\"win_sec_train\"],\n",
        "                        device=device,\n",
        "                        tau=CONFIG[\"tau\"],\n",
        "                        batch_size=CONFIG[\"batch_size\"]\n",
        "                    )\n",
        "\n",
        "                    records.append({\n",
        "                        \"activity_id\": act_id,\n",
        "                        \"activity\": act_name,\n",
        "                        \"fold\": fold_idx,\n",
        "                        \"test_subj\": test_subj,\n",
        "                        \"meta\": meta,\n",
        "                        \"stressor\": \"noise_drift\",\n",
        "                        \"level\": float(amp),\n",
        "                        \"fs_eval\": float(CONFIG[\"fs\"]),\n",
        "                        \"win_sec_eval\": float(CONFIG[\"win_sec_train\"]),\n",
        "                        \"gt\": gt,\n",
        "                        \"pred\": float(pred),\n",
        "                        \"mae\": float(mae),\n",
        "                        \"mape\": float(mape),\n",
        "                    })\n",
        "\n",
        "                # ---------- Noise: Spikes ----------\n",
        "                for p in noise_spike_p_levels:\n",
        "                    rng = _rng_from_meta(CONFIG[\"seed\"], f\"{meta}|spike|{p}|A{spike_A}\")\n",
        "                    x_cor = add_spikes(x_clean, p=p, A=spike_A, rng=rng)\n",
        "\n",
        "                    pred, mae, mape = eval_one_trial_windowing(\n",
        "                        model, x_cor, gt,\n",
        "                        fs_eval=CONFIG[\"fs\"],\n",
        "                        win_sec_eval=CONFIG[\"win_sec_train\"],\n",
        "                        device=device,\n",
        "                        tau=CONFIG[\"tau\"],\n",
        "                        batch_size=CONFIG[\"batch_size\"]\n",
        "                    )\n",
        "\n",
        "                    records.append({\n",
        "                        \"activity_id\": act_id,\n",
        "                        \"activity\": act_name,\n",
        "                        \"fold\": fold_idx,\n",
        "                        \"test_subj\": test_subj,\n",
        "                        \"meta\": meta,\n",
        "                        \"stressor\": \"noise_spike\",\n",
        "                        \"level\": float(p),\n",
        "                        \"fs_eval\": float(CONFIG[\"fs\"]),\n",
        "                        \"win_sec_eval\": float(CONFIG[\"win_sec_train\"]),\n",
        "                        \"gt\": gt,\n",
        "                        \"pred\": float(pred),\n",
        "                        \"mae\": float(mae),\n",
        "                        \"mape\": float(mape),\n",
        "                    })\n",
        "\n",
        "                # ---------- Sampling rate (LPF → decimate) ----------\n",
        "                for fs_new in fs_levels:\n",
        "                    _ = _rng_from_meta(CONFIG[\"seed\"], f\"{meta}|fs|{fs_new}\")  # kept for symmetry\n",
        "                    x_ds = lowpass_then_decimate(x_clean, fs_orig=float(CONFIG[\"fs\"]), fs_new=float(fs_new))\n",
        "\n",
        "                    pred, mae, mape = eval_one_trial_windowing(\n",
        "                        model, x_ds, gt,\n",
        "                        fs_eval=float(fs_new),\n",
        "                        win_sec_eval=CONFIG[\"win_sec_train\"],  # 8s in seconds stays 8s\n",
        "                        device=device,\n",
        "                        tau=CONFIG[\"tau\"],\n",
        "                        batch_size=CONFIG[\"batch_size\"]\n",
        "                    )\n",
        "\n",
        "                    records.append({\n",
        "                        \"activity_id\": act_id,\n",
        "                        \"activity\": act_name,\n",
        "                        \"fold\": fold_idx,\n",
        "                        \"test_subj\": test_subj,\n",
        "                        \"meta\": meta,\n",
        "                        \"stressor\": \"sampling_rate\",\n",
        "                        \"level\": float(fs_new),\n",
        "                        \"fs_eval\": float(fs_new),\n",
        "                        \"win_sec_eval\": float(CONFIG[\"win_sec_train\"]),\n",
        "                        \"gt\": gt,\n",
        "                        \"pred\": float(pred),\n",
        "                        \"mae\": float(mae),\n",
        "                        \"mape\": float(mape),\n",
        "                    })\n",
        "\n",
        "                # ---------- Axis / Channel dropout cases (0-masking) ----------\n",
        "                for case in axis_cases:\n",
        "                    x_cor = apply_axis_dropout_case(x_clean, case_name=case)\n",
        "\n",
        "                    pred, mae, mape = eval_one_trial_windowing(\n",
        "                        model, x_cor, gt,\n",
        "                        fs_eval=CONFIG[\"fs\"],\n",
        "                        win_sec_eval=CONFIG[\"win_sec_train\"],\n",
        "                        device=device,\n",
        "                        tau=CONFIG[\"tau\"],\n",
        "                        batch_size=CONFIG[\"batch_size\"]\n",
        "                    )\n",
        "\n",
        "                    records.append({\n",
        "                        \"activity_id\": act_id,\n",
        "                        \"activity\": act_name,\n",
        "                        \"fold\": fold_idx,\n",
        "                        \"test_subj\": test_subj,\n",
        "                        \"meta\": meta,\n",
        "                        \"stressor\": \"axis_dropout\",\n",
        "                        \"level\": case,  # string level for cases\n",
        "                        \"fs_eval\": float(CONFIG[\"fs\"]),\n",
        "                        \"win_sec_eval\": float(CONFIG[\"win_sec_train\"]),\n",
        "                        \"gt\": gt,\n",
        "                        \"pred\": float(pred),\n",
        "                        \"mae\": float(mae),\n",
        "                        \"mape\": float(mape),\n",
        "                    })\n",
        "\n",
        "                # ---------- Window length mismatch (stride=win/2, 50% overlap) ----------\n",
        "                for win_test in window_test_levels:\n",
        "                    pred, mae, mape = eval_one_trial_windowing(\n",
        "                        model, x_clean, gt,\n",
        "                        fs_eval=CONFIG[\"fs\"],\n",
        "                        win_sec_eval=float(win_test),\n",
        "                        device=device,\n",
        "                        tau=CONFIG[\"tau\"],\n",
        "                        batch_size=CONFIG[\"batch_size\"]\n",
        "                    )\n",
        "\n",
        "                    records.append({\n",
        "                        \"activity_id\": act_id,\n",
        "                        \"activity\": act_name,\n",
        "                        \"fold\": fold_idx,\n",
        "                        \"test_subj\": test_subj,\n",
        "                        \"meta\": meta,\n",
        "                        \"stressor\": \"window_mismatch\",\n",
        "                        \"level\": float(win_test),\n",
        "                        \"fs_eval\": float(CONFIG[\"fs\"]),\n",
        "                        \"win_sec_eval\": float(win_test),\n",
        "                        \"gt\": gt,\n",
        "                        \"pred\": float(pred),\n",
        "                        \"mae\": float(mae),\n",
        "                        \"mape\": float(mape),\n",
        "                    })\n",
        "\n",
        "            print(f\"[Done] Fold {fold_idx:2d} | Test: {test_subj}\")\n",
        "\n",
        "    # -----------------------------------------------------------------\n",
        "    # Save results + aggregate summaries + plots\n",
        "    # -----------------------------------------------------------------\n",
        "    df = pd.DataFrame(records)\n",
        "    out_csv = os.path.join(CONFIG[\"out_dir\"], \"robustness_raw.csv\")\n",
        "    df.to_csv(out_csv, index=False)\n",
        "    print(\"\\nSaved:\", out_csv)\n",
        "\n",
        "    # Aggregate (numeric levels only)\n",
        "    # For axis_dropout (string level), handle separately.\n",
        "    df_num = df.copy()\n",
        "    df_num[\"level_num\"] = pd.to_numeric(df_num[\"level\"], errors=\"coerce\")\n",
        "\n",
        "    # Summary table (mean±std across folds/trials)\n",
        "    agg_rows = []\n",
        "\n",
        "    for (act_id, stressor), g in df_num.groupby([\"activity_id\", \"stressor\"]):\n",
        "        act_name = g[\"activity\"].iloc[0]\n",
        "\n",
        "        # numeric levels\n",
        "        g_num = g[~g[\"level_num\"].isna()].copy()\n",
        "        if len(g_num) > 0:\n",
        "            for lvl, gg in g_num.groupby(\"level_num\"):\n",
        "                agg_rows.append({\n",
        "                    \"activity_id\": act_id,\n",
        "                    \"activity\": act_name,\n",
        "                    \"stressor\": stressor,\n",
        "                    \"level\": float(lvl),\n",
        "                    \"mae_mean\": float(gg[\"mae\"].mean()),\n",
        "                    \"mae_std\": float(gg[\"mae\"].std(ddof=0)),\n",
        "                    \"mape_mean\": float(gg[\"mape\"].mean()),\n",
        "                    \"mape_std\": float(gg[\"mape\"].std(ddof=0)),\n",
        "                    \"worst_mae\": float(gg[\"mae\"].max()),\n",
        "                    \"n\": int(len(gg)),\n",
        "                })\n",
        "\n",
        "        # string levels (axis_dropout)\n",
        "        g_str = g[g[\"level_num\"].isna()].copy()\n",
        "        if len(g_str) > 0:\n",
        "            for lvl, gg in g_str.groupby(\"level\"):\n",
        "                agg_rows.append({\n",
        "                    \"activity_id\": act_id,\n",
        "                    \"activity\": act_name,\n",
        "                    \"stressor\": stressor,\n",
        "                    \"level\": str(lvl),\n",
        "                    \"mae_mean\": float(gg[\"mae\"].mean()),\n",
        "                    \"mae_std\": float(gg[\"mae\"].std(ddof=0)),\n",
        "                    \"mape_mean\": float(gg[\"mape\"].mean()),\n",
        "                    \"mape_std\": float(gg[\"mape\"].std(ddof=0)),\n",
        "                    \"worst_mae\": float(gg[\"mae\"].max()),\n",
        "                    \"n\": int(len(gg)),\n",
        "                })\n",
        "\n",
        "    df_agg = pd.DataFrame(agg_rows)\n",
        "    out_csv2 = os.path.join(CONFIG[\"out_dir\"], \"robustness_summary.csv\")\n",
        "    df_agg.to_csv(out_csv2, index=False)\n",
        "    print(\"Saved:\", out_csv2)\n",
        "\n",
        "    # Plots: one per (activity, stressor) for numeric levels\n",
        "    plot_dir = os.path.join(CONFIG[\"out_dir\"], \"plots\")\n",
        "    os.makedirs(plot_dir, exist_ok=True)\n",
        "\n",
        "    for (act_id, stressor), g in df_agg.groupby([\"activity_id\", \"stressor\"]):\n",
        "        g2 = g.copy()\n",
        "        g2[\"level_num\"] = pd.to_numeric(g2[\"level\"], errors=\"coerce\")\n",
        "        g2 = g2[~g2[\"level_num\"].isna()].sort_values(\"level_num\")\n",
        "        if len(g2) == 0:\n",
        "            continue\n",
        "\n",
        "        act_name = g2[\"activity\"].iloc[0]\n",
        "        title = f\"[{act_name}] {stressor} degradation (MAE)\"\n",
        "        out_png = os.path.join(plot_dir, f\"act{act_id}_{stressor}.png\")\n",
        "        g2_plot = g2.rename(columns={\"level_num\": \"level\"})\n",
        "        save_degradation_plot(\n",
        "            g2_plot,\n",
        "            out_path=out_png,\n",
        "            title=title,\n",
        "            x_col=\"level_num\",\n",
        "            y_col=\"mae_mean\",\n",
        "            yerr_col=\"mae_std\"\n",
        "        )\n",
        "\n",
        "    print(f\"Saved plots to: {plot_dir}\")\n",
        "    print(\"\\nDone.\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    }
  ]
}